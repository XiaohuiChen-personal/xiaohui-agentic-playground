[project]
name = "xiaohui-agentic-playground"
version = "0.1.0"
description = "A playground for agentic experiments"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "anthropic>=0.75.0",
    "datasets>=4.4.1",
    "ipykernel>=7.1.0",
    "matplotlib>=3.9.0",
    "openai>=2.14.0",
    "openai-agents>=0.6.3",
    "python-dotenv>=1.2.1",
    "scikit-learn>=1.8.0",
    "nest-asyncio>=1.6.0",
    # Performance monitoring
    "psutil>=6.0.0",
    "py-cpuinfo>=9.0.0",
    # LangGraph dependencies
    "langgraph>=0.2.0",
    "langchain>=0.3.0",
    "langchain-openai>=0.2.0",
    "langchain-anthropic>=0.2.0",
    # AutoGen dependencies (Microsoft)
    "autogen-agentchat>=0.7.5",
    "autogen-core>=0.7.5",
    "autogen-ext>=0.7.5",
]

# =============================================================================
# Fine-Tuning Dependencies
# =============================================================================
# All fine-tuning runs inside NVIDIA Docker containers because DGX Spark
# (aarch64 + Blackwell sm_121) requires native CUDA kernels.
#
# Three fine-tuning methods are supported:
#
# 1. FULL FINE-TUNING (100% of parameters)
#    - Container: nvcr.io/nvidia/pytorch:25.11-py3
#    - Command:   ./start_docker.sh start finetune
#    - Jupyter:   http://localhost:8888
#    - Memory:    ~70 GB (requires stopping inference containers)
#    - Time:      ~10 hours for Qwen2.5-7B
#    - Output:    New model weights (~14 GB)
#
# 2. LoRA (Low-Rank Adaptation, ~1% of parameters)
#    - Container: nvcr.io/nvidia/pytorch:25.11-py3 + Unsloth
#    - Command:   ./start_docker.sh start peft
#    - Jupyter:   http://localhost:8889
#    - Memory:    ~20-25 GB
#    - Time:      ~2-4 hours
#    - Output:    Small adapter (~200 MB)
#
# 3. QLoRA (Quantized LoRA, 4-bit base model)
#    - Container: Same as LoRA (peft container)
#    - Command:   ./start_docker.sh start peft
#    - Jupyter:   http://localhost:8889
#    - Memory:    ~8-12 GB (most memory-efficient)
#    - Time:      ~2-4 hours
#    - Output:    Small adapter (~200 MB)
#
# See 6-open-source/fine-tuning-dense/README.md for detailed instructions.
