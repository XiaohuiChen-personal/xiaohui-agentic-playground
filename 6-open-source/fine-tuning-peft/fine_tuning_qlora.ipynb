{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c665f09",
      "metadata": {},
      "source": [
        "# QLoRA Fine-Tuning: Qwen2.5-7B on AG News\n",
        "\n",
        "This notebook demonstrates **QLoRA (Quantized LoRA)** fine-tuning using a **4-bit quantized base model** for maximum memory efficiency and faster training on memory-bandwidth-limited hardware.\n",
        "\n",
        "## Overview\n",
        "\n",
        "| Aspect | Details |\n",
        "|--------|---------|\n",
        "| **Model** | Qwen/Qwen2.5-7B-Instruct (4-bit quantized) |\n",
        "| **Method** | QLoRA (4-bit base + LoRA adapters) |\n",
        "| **Framework** | HuggingFace + PEFT + TRL + bitsandbytes |\n",
        "| **Dataset** | AG News (120K train, 7.6K test) |\n",
        "| **Task** | 4-class text classification |\n",
        "| **Expected Time** | ~4-6 hours |\n",
        "| **Memory** | ~8-12 GB |\n",
        "\n",
        "## Why QLoRA is Faster on DGX Spark\n",
        "\n",
        "DGX Spark's bottleneck is **memory bandwidth** (~273 GB/s). QLoRA addresses this:\n",
        "\n",
        "| Aspect | LoRA (BF16) | QLoRA (4-bit) | Benefit |\n",
        "|--------|-------------|---------------|---------|\n",
        "| Model weights | 14 GB | **3.5 GB** | 4x smaller |\n",
        "| Memory bandwidth/iter | High | **4x lower** | Faster forward pass |\n",
        "| GPU memory | ~25 GB | **~10 GB** | More headroom |\n",
        "| Training time | ~12 hours | **~4-6 hours** | 2-3x faster |\n",
        "\n",
        "## Trade-offs\n",
        "\n",
        "| Aspect | Impact |\n",
        "|--------|--------|\n",
        "| Speed | **2-3x faster** (less memory to transfer) |\n",
        "| Memory | **60% less** GPU memory |\n",
        "| Quality | Slight degradation (~1-2% accuracy loss typical) |\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This notebook must run inside the PEFT Docker container:\n",
        "```bash\n",
        "./start_docker.sh start peft\n",
        "# Then open http://localhost:8889\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cabaf7",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8e87ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Environment Verification - QLoRA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check CUDA\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "    \n",
        "    # Memory info\n",
        "    try:\n",
        "        total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
        "    except:\n",
        "        print(\"GPU Memory: Unified memory system (DGX Spark)\")\n",
        "else:\n",
        "    raise RuntimeError(\"CUDA not available!\")\n",
        "\n",
        "# Check bitsandbytes (required for 4-bit quantization)\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(f\"\\nbitsandbytes version: {bnb.__version__}\")\n",
        "    print(\"✓ 4-bit quantization available\")\n",
        "except ImportError:\n",
        "    raise RuntimeError(\"bitsandbytes not installed! Required for QLoRA.\")\n",
        "\n",
        "# Check working directory\n",
        "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
        "print(f\"Dataset available: {os.path.exists('/fine-tuning-dense/datasets/train.jsonl')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c11b5e6",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "QLoRA uses the same LoRA configuration but with a **4-bit quantized base model**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6fe3a6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Model Configuration\n",
        "# =============================================================================\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "MAX_SEQ_LENGTH = 512  # AG News articles are short (~120 tokens avg)\n",
        "LOAD_IN_4BIT = True   # *** KEY DIFFERENCE: QLoRA uses 4-bit quantization ***\n",
        "\n",
        "# =============================================================================\n",
        "# LoRA Configuration (same as LoRA)\n",
        "# =============================================================================\n",
        "LORA_R = 16           # LoRA rank\n",
        "LORA_ALPHA = 32       # LoRA scaling factor\n",
        "LORA_DROPOUT = 0.05   # Small dropout for regularization (QLoRA benefits from this)\n",
        "\n",
        "# Target modules for Qwen2.5\n",
        "TARGET_MODULES = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP\n",
        "]\n",
        "\n",
        "# =============================================================================\n",
        "# Training Configuration\n",
        "# =============================================================================\n",
        "BATCH_SIZE = 16       # Can use larger batch with 4-bit (less memory)\n",
        "GRADIENT_ACCUMULATION_STEPS = 2  # Effective batch size = 32\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 1\n",
        "WARMUP_RATIO = 0.03\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# =============================================================================\n",
        "# Output Configuration\n",
        "# =============================================================================\n",
        "OUTPUT_DIR = \"./adapters/qwen7b-ag-news-qlora\"\n",
        "LOGGING_STEPS = 50\n",
        "SAVE_STEPS = 500\n",
        "\n",
        "# =============================================================================\n",
        "# Dataset Paths\n",
        "# =============================================================================\n",
        "TRAIN_DATA_PATH = \"/fine-tuning-dense/datasets/train.jsonl\"\n",
        "\n",
        "print(\"QLoRA Configuration loaded!\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  4-bit quantization: {LOAD_IN_4BIT}\")\n",
        "print(f\"  LoRA rank: {LORA_R}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13f3a60",
      "metadata": {},
      "source": [
        "## 3. Load Model with 4-bit Quantization\n",
        "\n",
        "The key to QLoRA is loading the base model in 4-bit precision using `BitsAndBytesConfig`.\n",
        "\n",
        "**Memory savings**: 14 GB (BF16) → 3.5 GB (4-bit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a11988",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "print(\"Loading model with 4-bit quantization (QLoRA)...\")\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",           # NormalFloat4 - best for LLMs\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in BF16\n",
        "    bnb_4bit_use_double_quant=True,      # Double quantization for more savings\n",
        ")\n",
        "\n",
        "print(\"  Quantization config:\")\n",
        "print(f\"    - 4-bit type: nf4 (NormalFloat4)\")\n",
        "print(f\"    - Compute dtype: bfloat16\")\n",
        "print(f\"    - Double quantization: enabled\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "# Check memory usage\n",
        "if torch.cuda.is_available():\n",
        "    mem_used = torch.cuda.memory_allocated() / 1e9\n",
        "    print(f\"\\n✓ Model loaded in 4-bit!\")\n",
        "    print(f\"  GPU memory used: {mem_used:.2f} GB\")\n",
        "    print(f\"  (vs ~14 GB for BF16 - {100*(1 - mem_used/14):.0f}% savings)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1084853",
      "metadata": {},
      "source": [
        "## 4. Apply LoRA Adapters\n",
        "\n",
        "With QLoRA, we need to prepare the 4-bit model for training before applying LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "677d7bbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "print(\"Preparing 4-bit model for training...\")\n",
        "\n",
        "# Prepare the quantized model for training\n",
        "# This enables gradient checkpointing and casts certain layers to float32\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(\"Applying LoRA adapters...\")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    return trainable, total\n",
        "\n",
        "trainable, total = count_parameters(model)\n",
        "print(f\"\\nQLoRA Configuration:\")\n",
        "print(f\"  Base model: 4-bit quantized\")\n",
        "print(f\"  LoRA rank: {LORA_R}\")\n",
        "print(f\"  LoRA alpha: {LORA_ALPHA}\")\n",
        "print(f\"\\nParameter Count:\")\n",
        "print(f\"  Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
        "print(f\"  Total: {total:,}\")\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57bc153f",
      "metadata": {},
      "source": [
        "## 5. Load and Format Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cae2886a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(f\"Loading dataset from: {TRAIN_DATA_PATH}\")\n",
        "\n",
        "# Load the JSONL dataset\n",
        "dataset = load_dataset(\"json\", data_files=TRAIN_DATA_PATH, split=\"train\")\n",
        "\n",
        "print(f\"  Total examples: {len(dataset):,}\")\n",
        "\n",
        "# Format with chat template\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"Applying chat template...\")\n",
        "formatted_dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    desc=\"Formatting\",\n",
        ")\n",
        "\n",
        "print(f\"✓ Dataset formatted: {len(formatted_dataset):,} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10bd9f22",
      "metadata": {},
      "source": [
        "## 6. Configure Training\n",
        "\n",
        "QLoRA can use **larger batch sizes** due to lower memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc7425f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Enable cuDNN benchmark\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Calculate steps\n",
        "total_steps = (len(formatted_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_EPOCHS\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Estimated steps: {total_steps:,}\")\n",
        "\n",
        "# SFT Configuration\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Training\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    \n",
        "    # Optimizer - use paged adamw for 4-bit training\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    optim=\"paged_adamw_8bit\",  # Better for QLoRA\n",
        "    \n",
        "    # Precision\n",
        "    bf16=True,\n",
        "    fp16=False,\n",
        "    \n",
        "    # Sequence handling\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    packing=True,\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    logging_first_step=True,\n",
        "    \n",
        "    # Checkpointing\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=2,\n",
        "    \n",
        "    # Performance\n",
        "    dataloader_num_workers=4,\n",
        "    \n",
        "    # Misc\n",
        "    seed=42,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"✓ SFTConfig created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f57f3b65",
      "metadata": {},
      "source": [
        "## 7. Train\n",
        "\n",
        "Expected time: **~4-6 hours** (2-3x faster than LoRA due to 4-bit quantization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b3df055",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    args=sft_config,\n",
        ")\n",
        "\n",
        "print(\"Trainer created!\")\n",
        "print(f\"\\nStarting QLoRA training...\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339c6a14",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Train!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "hours, remainder = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"QLoRA Training Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTraining time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
        "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"Total steps: {trainer_stats.global_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf61e796",
      "metadata": {},
      "source": [
        "## 8. Save the QLoRA Adapter\n",
        "\n",
        "The adapter weights are the same size as LoRA (~200 MB) - only the LoRA weights are saved, not the quantized base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f781d7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the adapter\n",
        "adapter_path = f\"{OUTPUT_DIR}/final\"\n",
        "\n",
        "print(f\"Saving QLoRA adapter to: {adapter_path}\")\n",
        "\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "# Check saved files\n",
        "saved_files = os.listdir(adapter_path)\n",
        "total_size = sum(os.path.getsize(os.path.join(adapter_path, f)) for f in saved_files)\n",
        "\n",
        "print(f\"\\nSaved files:\")\n",
        "for f in sorted(saved_files):\n",
        "    size = os.path.getsize(os.path.join(adapter_path, f))\n",
        "    print(f\"  {f}: {size / 1e6:.2f} MB\")\n",
        "\n",
        "print(f\"\\nTotal adapter size: {total_size / 1e6:.2f} MB\")\n",
        "print(f\"✓ QLoRA adapter saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac64cbf",
      "metadata": {},
      "source": [
        "## 9. Quick Evaluation\n",
        "\n",
        "Test the QLoRA fine-tuned model on sample articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13522f4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set to eval mode\n",
        "model.eval()\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a news article classifier. Categorize articles into one of four categories:\n",
        "- World: Politics, government, international affairs\n",
        "- Sports: Athletic events, games, teams\n",
        "- Business: Companies, markets, finance\n",
        "- Sci/Tech: Technology, scientific research\n",
        "\n",
        "Respond with a JSON object containing only the category field.\"\"\"\n",
        "\n",
        "test_articles = [\n",
        "    (\"The Federal Reserve announced a quarter-point interest rate cut.\", \"Business\"),\n",
        "    (\"Scientists at CERN discovered a new subatomic particle.\", \"Sci/Tech\"),\n",
        "    (\"The Lakers defeated the Celtics 112-108 in overtime.\", \"Sports\"),\n",
        "    (\"The UN Security Council voted to impose new sanctions.\", \"World\"),\n",
        "]\n",
        "\n",
        "print(\"Testing QLoRA fine-tuned model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "correct = 0\n",
        "for article, expected in test_articles:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"Classify: {article}\"},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True,\n",
        "        return_dict=False,\n",
        "    ).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.0,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    is_correct = expected.lower() in response.lower()\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    \n",
        "    print(f\"\\n{expected}: {'✓' if is_correct else '✗'}\")\n",
        "    print(f\"  Response: {response.strip()[:80]}\")\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"Quick test accuracy: {correct}/{len(test_articles)} ({100*correct/len(test_articles):.0f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08bc43a0",
      "metadata": {},
      "source": [
        "## 10. Conclusions\n",
        "\n",
        "*To be filled after running*\n",
        "\n",
        "### Training Results\n",
        "\n",
        "| Metric | QLoRA | LoRA | Full Fine-Tuning |\n",
        "|--------|-------|------|------------------|\n",
        "| Training Time | TBD | ~12h | ~10h |\n",
        "| Final Loss | TBD | TBD | ~0.45 |\n",
        "| GPU Memory | ~10 GB | ~25 GB | ~70 GB |\n",
        "| Adapter Size | ~200 MB | ~200 MB | ~14 GB |\n",
        "\n",
        "### Performance Comparison\n",
        "\n",
        "| Method | Expected Accuracy | Training Time |\n",
        "|--------|------------------|---------------|\n",
        "| Full Fine-Tuning | 88.33% | ~10 hours |\n",
        "| LoRA | ~85-88% | ~12 hours |\n",
        "| **QLoRA** | ~83-86% | **~4-6 hours** |\n",
        "\n",
        "### Key Observations\n",
        "\n",
        "1. **Training speed**: TBD\n",
        "2. **Memory efficiency**: TBD  \n",
        "3. **Quality vs LoRA**: TBD"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
