{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77700bb",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning: Qwen2.5-7B on AG News\n",
    "\n",
    "This notebook demonstrates **LoRA (Low-Rank Adaptation)** fine-tuning using **HuggingFace Transformers + PEFT + TRL**.\n",
    "\n",
    "## Overview\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|---------|\n",
    "| **Model** | Qwen/Qwen2.5-7B-Instruct (cached) |\n",
    "| **Method** | LoRA (16-bit base model) |\n",
    "| **Framework** | HuggingFace + PEFT + TRL |\n",
    "| **Dataset** | AG News (120K train, 7.6K test) |\n",
    "| **Task** | 4-class text classification |\n",
    "| **Expected Time** | ~2-4 hours |\n",
    "| **Memory** | ~20-25 GB |\n",
    "\n",
    "## LoRA vs Full Fine-Tuning\n",
    "\n",
    "| Aspect | LoRA | Full Fine-Tuning |\n",
    "|--------|------|------------------|\n",
    "| Parameters trained | ~70M (1%) | 7.6B (100%) |\n",
    "| Memory usage | ~20-25 GB | ~70 GB |\n",
    "| Training time | 2-4 hours | ~10 hours |\n",
    "| Output size | ~200 MB | ~14 GB |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook must run inside the PEFT Docker container:\n",
    "```bash\n",
    "./start_docker.sh start peft\n",
    "# Then open http://localhost:8889\n",
    "```\n",
    "\n",
    "**Note**: Uses the already-cached `Qwen/Qwen2.5-7B-Instruct` model - no download required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76678f9",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Verification\n",
    "\n",
    "First, let's verify we're running in the correct environment with GPU access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dfa011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Environment Verification\n",
      "============================================================\n",
      "\n",
      "PyTorch version: 2.10.0a0+b558c986e8.nv25.11\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "GPU: NVIDIA GB10\n",
      "GPU Compute Capability: (12, 1)\n",
      "GPU Memory: 128.5 GB\n",
      "\n",
      "Working directory: /fine-tuning\n",
      "Dataset available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Environment Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check CUDA\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    \n",
    "    # Memory info (may show N/A on unified memory systems)\n",
    "    try:\n",
    "        total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
    "    except:\n",
    "        print(\"GPU Memory: Unified memory system (DGX Spark)\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA not available! Make sure you're running in the Docker container.\")\n",
    "\n",
    "# Check working directory\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "print(f\"Dataset available: {os.path.exists('/fine-tuning-dense/datasets/train.jsonl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c9bb4",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define all hyperparameters and settings for LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0dd704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  LoRA rank: 16\n",
      "  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "  Batch size: 8 x 4 = 32\n",
      "  Learning rate: 0.0002\n",
      "  Output: ./adapters/qwen7b-ag-news-lora\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Model Configuration\n",
    "# =============================================================================\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 512  # AG News articles are short (~120 tokens avg)\n",
    "LOAD_IN_4BIT = False  # LoRA uses 16-bit base model (set True for QLoRA)\n",
    "\n",
    "# =============================================================================\n",
    "# LoRA Configuration\n",
    "# =============================================================================\n",
    "LORA_R = 16           # LoRA rank (higher = more capacity, more memory)\n",
    "LORA_ALPHA = 32       # LoRA scaling factor (typically 2x rank)\n",
    "LORA_DROPOUT = 0.0    # Dropout for LoRA layers (0 for Unsloth)\n",
    "\n",
    "# Target modules for Qwen2.5 - attention projections\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP (optional, more capacity)\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# Training Configuration\n",
    "# =============================================================================\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 32\n",
    "LEARNING_RATE = 2e-4             # Higher than full fine-tuning (only training adapters)\n",
    "NUM_EPOCHS = 1\n",
    "WARMUP_RATIO = 0.03\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# =============================================================================\n",
    "# Output Configuration\n",
    "# =============================================================================\n",
    "OUTPUT_DIR = \"./adapters/qwen7b-ag-news-lora\"\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 500\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset Paths (inside Docker container)\n",
    "# =============================================================================\n",
    "TRAIN_DATA_PATH = \"/fine-tuning-dense/datasets/train.jsonl\"\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA rank: {LORA_R}\")\n",
    "print(f\"  Target modules: {TARGET_MODULES}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071cf45",
   "metadata": {},
   "source": [
    "## 3. Load Model from Cache\n",
    "\n",
    "Load the model using standard HuggingFace transformers. This uses the **already cached** model at `~/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/`.\n",
    "\n",
    "**Note**: We're using standard HuggingFace loading instead of Unsloth's `FastLanguageModel` to avoid downloading Unsloth's separate model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70708269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from HuggingFace cache...\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  This uses the already-cached model (no download needed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:14<00:00, 18.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded: Qwen/Qwen2.5-7B-Instruct\n",
      "Model dtype: torch.bfloat16\n",
      "Tokenizer vocab size: 151665\n",
      "Pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading model from HuggingFace cache...\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  This uses the already-cached model (no download needed)\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model in bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",  # Use Flash Attention 2\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded: {MODEL_NAME}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83740271",
   "metadata": {},
   "source": [
    "## 4. Apply LoRA Adapters with PEFT\n",
    "\n",
    "Add LoRA adapters using the PEFT library. Only these small adapter weights (~1% of total) will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67ad019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA adapters with PEFT...\n",
      "\n",
      "LoRA Configuration:\n",
      "  Rank (r): 16\n",
      "  Alpha: 32\n",
      "  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "\n",
      "Parameter Count:\n",
      "  Trainable: 40,370,176 (0.53%)\n",
      "  Total: 7,655,986,688\n",
      "  Frozen: 7,615,616,512 (99.47%)\n",
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"Applying LoRA adapters with PEFT...\")\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Count trainable parameters\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "trainable, total = count_parameters(model)\n",
    "print(f\"\\nLoRA Configuration:\")\n",
    "print(f\"  Rank (r): {LORA_R}\")\n",
    "print(f\"  Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Target modules: {TARGET_MODULES}\")\n",
    "print(f\"\\nParameter Count:\")\n",
    "print(f\"  Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "print(f\"  Total: {total:,}\")\n",
    "print(f\"  Frozen: {total - trainable:,} ({100*(total-trainable)/total:.2f}%)\")\n",
    "\n",
    "# Print trainable modules\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a40bc",
   "metadata": {},
   "source": [
    "## 5. Load Training Dataset\n",
    "\n",
    "Load the AG News dataset prepared for fine-tuning (in chat format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7277c840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /fine-tuning-dense/datasets/train.jsonl\n",
      "\n",
      "Dataset loaded:\n",
      "  Total examples: 120,000\n",
      "  Columns: ['messages']\n",
      "\n",
      "Sample entry:\n",
      "  [system]: You are a news article classifier. Your task is to categorize news articles into exactly one of four...\n",
      "  [user]: Classify the following news article:\n",
      "\n",
      "Thirst, Fear and Bribes on Desert Escape from Africa  AGADEZ, ...\n",
      "  [assistant]: {\"category\":\"World\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"Loading dataset from: {TRAIN_DATA_PATH}\")\n",
    "\n",
    "# Load the JSONL dataset\n",
    "dataset = load_dataset(\"json\", data_files=TRAIN_DATA_PATH, split=\"train\")\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Total examples: {len(dataset):,}\")\n",
    "print(f\"  Columns: {dataset.column_names}\")\n",
    "\n",
    "# Show a sample\n",
    "print(f\"\\nSample entry:\")\n",
    "sample = dataset[0]\n",
    "for msg in sample[\"messages\"]:\n",
    "    role = msg[\"role\"]\n",
    "    content = msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"]\n",
    "    print(f\"  [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc5aae",
   "metadata": {},
   "source": [
    "## 6. Format Dataset for Training\n",
    "\n",
    "Apply the chat template to convert messages into the format expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09b06ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying chat template to dataset...\n",
      "\n",
      "Formatted dataset:\n",
      "  Columns: ['messages', 'text']\n",
      "\n",
      "Formatted sample (first 500 chars):\n",
      "<|im_start|>system\n",
      "You are a news article classifier. Your task is to categorize news articles into exactly one of four categories:\n",
      "\n",
      "- World: News about politics, government, elections, diplomacy, conflicts, and public affairs (domestic or international)\n",
      "- Sports: News about athletic events, games, players, teams, coaches, tournaments, and championships\n",
      "- Business: News about companies, markets, finance, economy, trade, corporate activities, and business services\n",
      "- Sci/Tech: News about technolog\n"
     ]
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format examples using the tokenizer's chat template.\"\"\"\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        # Apply chat template\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "print(\"Applying chat template to dataset...\")\n",
    "formatted_dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    desc=\"Formatting\",\n",
    ")\n",
    "\n",
    "print(f\"\\nFormatted dataset:\")\n",
    "print(f\"  Columns: {formatted_dataset.column_names}\")\n",
    "\n",
    "# Show formatted sample\n",
    "print(f\"\\nFormatted sample (first 500 chars):\")\n",
    "print(formatted_dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b137c",
   "metadata": {},
   "source": [
    "## 7. Configure Training\n",
    "\n",
    "Set up the SFTTrainer with optimized settings for DGX Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27663ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Total examples: 120,000\n",
      "  Batch size: 8\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 32\n",
      "  Epochs: 1\n",
      "  Estimated total steps: 3,750\n",
      "\n",
      "SFTConfig created successfully!\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Enable cuDNN benchmark for consistent input sizes\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Calculate total steps\n",
    "total_steps = (len(formatted_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_EPOCHS\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Total examples: {len(formatted_dataset):,}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Estimated total steps: {total_steps:,}\")\n",
    "\n",
    "# SFT Configuration\n",
    "# Note: TRL 0.12+ uses 'max_length' instead of 'max_seq_length'\n",
    "sft_config = SFTConfig(\n",
    "    # Output\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    \n",
    "    # Sequence handling\n",
    "    max_length=MAX_SEQ_LENGTH,  # Renamed from max_seq_length in TRL 0.12+\n",
    "    packing=True,  # Pack multiple sequences per batch (30-40% speedup)\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=4,\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"\\nSFTConfig created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b548e7",
   "metadata": {},
   "source": [
    "## 8. Create Trainer and Start Training\n",
    "\n",
    "This will take approximately 2-4 hours depending on GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8823e2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|██████████| 120000/120000 [00:41<00:00, 2909.14 examples/s]\n",
      "Packing train dataset: 100%|██████████| 120000/120000 [00:00<00:00, 259816.34 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer created!\n",
      "\n",
      "Starting training...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "# Note: TRL 0.12+ uses 'processing_class' instead of 'tokenizer'\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "print(\"Trainer created!\")\n",
    "print(f\"\\nStarting training...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544c932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='1965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  35/1965 12:41 < 12:22:18, 0.04 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.920200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train!\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "hours, remainder = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Total steps: {trainer_stats.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fabdd0",
   "metadata": {},
   "source": [
    "## 9. Save the LoRA Adapter\n",
    "\n",
    "Save only the adapter weights (not the full model). This will be ~200 MB instead of ~14 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967453f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "adapter_path = f\"{OUTPUT_DIR}/final\"\n",
    "\n",
    "print(f\"Saving LoRA adapter to: {adapter_path}\")\n",
    "\n",
    "# Save using Unsloth's optimized method\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "# Check the saved files\n",
    "import os\n",
    "saved_files = os.listdir(adapter_path)\n",
    "total_size = sum(os.path.getsize(os.path.join(adapter_path, f)) for f in saved_files)\n",
    "\n",
    "print(f\"\\nSaved files:\")\n",
    "for f in sorted(saved_files):\n",
    "    size = os.path.getsize(os.path.join(adapter_path, f))\n",
    "    print(f\"  {f}: {size / 1e6:.2f} MB\")\n",
    "\n",
    "print(f\"\\nTotal adapter size: {total_size / 1e6:.2f} MB\")\n",
    "print(f\"\\n✓ LoRA adapter saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7255b7d",
   "metadata": {},
   "source": [
    "## 10. Quick Evaluation\n",
    "\n",
    "Test the fine-tuned model on a few examples to verify it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cce70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode for inference\n",
    "model.eval()\n",
    "\n",
    "# System prompt for classification\n",
    "SYSTEM_PROMPT = \"\"\"You are a news article classifier. Your task is to categorize news articles into exactly one of four categories:\n",
    "\n",
    "- World: News about politics, government, elections, diplomacy, conflicts, and public affairs (domestic or international)\n",
    "- Sports: News about athletic events, games, players, teams, coaches, tournaments, and championships\n",
    "- Business: News about companies, markets, finance, economy, trade, corporate activities, and business services\n",
    "- Sci/Tech: News about technology products, software, hardware, scientific research, gadgets, and tech innovations\n",
    "\n",
    "Respond with a JSON object containing only the category field.\"\"\"\n",
    "\n",
    "# Test articles\n",
    "test_articles = [\n",
    "    (\"The Federal Reserve announced a quarter-point interest rate cut, signaling confidence in the economy.\", \"Business\"),\n",
    "    (\"Scientists at CERN discovered a new subatomic particle that could revolutionize our understanding of physics.\", \"Sci/Tech\"),\n",
    "    (\"The Lakers defeated the Celtics 112-108 in overtime, with LeBron James scoring 35 points.\", \"Sports\"),\n",
    "    (\"The United Nations Security Council voted to impose new sanctions on North Korea.\", \"World\"),\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for article, expected in test_articles:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the following news article:\\n\\n{article}\"},\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=False,\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.0,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Check if correct\n",
    "    is_correct = expected.lower() in response.lower()\n",
    "    status = \"✓\" if is_correct else \"✗\"\n",
    "    \n",
    "    print(f\"\\nArticle: {article[:60]}...\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Response: {response.strip()}\")\n",
    "    print(f\"Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34563cc",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "### Deploy with vLLM\n",
    "\n",
    "To serve the LoRA adapter with vLLM, update `docker-compose-qwen7b.yml`:\n",
    "\n",
    "```yaml\n",
    "command: >\n",
    "  vllm serve Qwen/Qwen2.5-7B-Instruct\n",
    "  --host 0.0.0.0\n",
    "  --port 8000\n",
    "  --enable-lora\n",
    "  --lora-modules lora-agnews=/checkpoints/qwen7b-ag-news-lora/final\n",
    "```\n",
    "\n",
    "Then start the server:\n",
    "```bash\n",
    "./start_docker.sh start qwen7b\n",
    "```\n",
    "\n",
    "### Full Evaluation\n",
    "\n",
    "For comprehensive evaluation on the test set, create an evaluation notebook similar to `full_finetuning_performance.ipynb`.\n",
    "\n",
    "### Compare with Full Fine-Tuning\n",
    "\n",
    "| Metric | Full Fine-Tuning | LoRA (Expected) |\n",
    "|--------|------------------|-----------------|\n",
    "| Accuracy | 88.33% | ~85-88% |\n",
    "| Training Time | ~10 hours | ~2-4 hours |\n",
    "| Output Size | ~14 GB | ~200 MB |\n",
    "| Memory Used | ~70 GB | ~20-25 GB |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fa81c",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "*To be filled after running the notebook*\n",
    "\n",
    "### Training Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Training Time | TBD |\n",
    "| Final Loss | TBD |\n",
    "| Total Steps | TBD |\n",
    "| Adapter Size | TBD |\n",
    "\n",
    "### Quick Test Results\n",
    "\n",
    "| Category | Correct/Total |\n",
    "|----------|---------------|\n",
    "| World | TBD |\n",
    "| Sports | TBD |\n",
    "| Business | TBD |\n",
    "| Sci/Tech | TBD |\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Training speed**: TBD\n",
    "2. **Memory usage**: TBD\n",
    "3. **Quality**: TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
