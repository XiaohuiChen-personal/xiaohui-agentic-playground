{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1b2c3d4",
      "metadata": {},
      "source": [
        "# Full Fine-Tuning with Unsloth: Qwen2.5-7B on AG News\n",
        "\n",
        "This notebook demonstrates **Full Fine-Tuning** using **Unsloth's FastLanguageModel** for optimized performance on DGX Spark.\n",
        "\n",
        "## Overview\n",
        "\n",
        "| Aspect | Details |\n",
        "|--------|---------|\n",
        "| **Model** | unsloth/Qwen2.5-7B-Instruct |\n",
        "| **Method** | Full Fine-Tuning (100% parameters) |\n",
        "| **Framework** | Unsloth + TRL |\n",
        "| **Dataset** | AG News (120K train, 7.6K test) |\n",
        "| **Task** | 4-class text classification |\n",
        "| **Expected Time** | ~6-8 hours (DGX Spark optimized) |\n",
        "| **Memory** | ~60-70 GB |\n",
        "| **Output** | Full model (~14 GB) |\n",
        "\n",
        "## Base Model Performance (Target to Beat)\n",
        "\n",
        "| Metric | Base Model | Target |\n",
        "|--------|------------|--------|\n",
        "| **Accuracy** | 78.76% | >88% |\n",
        "| **F1 (macro)** | 77.97% | >85% |\n",
        "| **Sci/Tech F1** | 62.06% | >80% |\n",
        "| **Business Precision** | 63.66% | >80% |\n",
        "\n",
        "## Full Fine-Tuning vs LoRA\n",
        "\n",
        "| Aspect | Full Fine-Tuning | LoRA |\n",
        "|--------|------------------|------|\n",
        "| Parameters trained | 7.6B (100%) | ~40M (0.5%) |\n",
        "| Memory usage | ~60-70 GB | ~25 GB |\n",
        "| Output size | ~14 GB | ~180 MB |\n",
        "| Quality | Maximum | Slightly lower |\n",
        "\n",
        "## DGX Spark Optimizations Applied\n",
        "\n",
        "| Optimization | Setting | Reason |\n",
        "|--------------|---------|--------|\n",
        "| `gradient_checkpointing` | False | Faster training (128GB memory available) |\n",
        "| `BATCH_SIZE` | 8 | Larger batches for better GPU utilization |\n",
        "| `GRADIENT_ACCUMULATION` | 2 | Keeps effective batch size = 16 |\n",
        "| `optim` | adamw_8bit | Reduces optimizer memory by 50% |\n",
        "| `packing` | True | Efficient sequence packing |\n",
        "| `dataloader_num_workers` | 8 | Faster parallel data loading |\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "```bash\n",
        "./start_docker.sh start finetune\n",
        "# Then open http://localhost:8888\n",
        "```\n",
        "\n",
        "**Note**: Full fine-tuning requires significant memory. DGX Spark's 128 GB unified memory is sufficient."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c3d4e5",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c3d4e5f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Environment Verification - Full Fine-Tuning\n",
            "============================================================\n",
            "\n",
            "PyTorch version: 2.10.0a0+b558c986e8.nv25.11\n",
            "CUDA available: True\n",
            "CUDA version: 13.0\n",
            "GPU: NVIDIA GB10\n",
            "GPU Compute Capability: (12, 1)\n",
            "GPU Memory: 128.5 GB\n",
            "\n",
            "Working directory: /fine-tuning\n",
            "Dataset available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Environment Verification - Full Fine-Tuning\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "    try:\n",
        "        total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
        "    except:\n",
        "        print(\"GPU Memory: Unified memory system (DGX Spark)\")\n",
        "else:\n",
        "    raise RuntimeError(\"CUDA not available!\")\n",
        "\n",
        "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
        "print(f\"Dataset available: {os.path.exists('/fine-tuning/datasets/train.jsonl')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e5f6a7",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e5f6a7b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full Fine-Tuning Configuration loaded!\n",
            "  Model: unsloth/Qwen2.5-7B-Instruct\n",
            "  Method: Full Fine-Tuning (100% parameters)\n",
            "  Batch size: 8 x 2 = 16\n",
            "  Learning rate: 2e-05\n",
            "  Output: ./checkpoints/qwen7b-ag-news-full-unsloth\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Model Configuration\n",
        "# =============================================================================\n",
        "MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# =============================================================================\n",
        "# Training Configuration (Optimized for DGX Spark 128GB)\n",
        "# =============================================================================\n",
        "BATCH_SIZE = 8  # Increased from 4 (DGX Spark has 128GB memory)\n",
        "GRADIENT_ACCUMULATION_STEPS = 2  # Reduced to keep effective batch size = 16\n",
        "LEARNING_RATE = 2e-5  # Lower than LoRA for full fine-tuning\n",
        "NUM_EPOCHS = 1\n",
        "WARMUP_RATIO = 0.03\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# =============================================================================\n",
        "# Output Configuration\n",
        "# =============================================================================\n",
        "OUTPUT_DIR = \"./checkpoints/qwen7b-ag-news-full-unsloth\"\n",
        "LOGGING_STEPS = 50\n",
        "SAVE_STEPS = 500\n",
        "\n",
        "TRAIN_DATA_PATH = \"/fine-tuning/datasets/train.jsonl\"\n",
        "\n",
        "print(\"Full Fine-Tuning Configuration loaded!\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Method: Full Fine-Tuning (100% parameters)\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a7b8c9",
      "metadata": {},
      "source": [
        "## 3. Load Model with Full Fine-Tuning Enabled\n",
        "\n",
        "Using `full_finetuning=True` to train all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a7b8c9d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Unsloth: Could not import trl.trainer.nash_md_trainer: Failed to import trl.trainer.nash_md_trainer because of the following error (look up to see its traceback):\n",
            "cannot import name 'amp' from 'apex' (/usr/local/lib/python3.12/dist-packages/apex/__init__.py)\n",
            "Unsloth: Could not import trl.trainer.online_dpo_trainer: Failed to import trl.trainer.online_dpo_trainer because of the following error (look up to see its traceback):\n",
            "cannot import name 'amp' from 'apex' (/usr/local/lib/python3.12/dist-packages/apex/__init__.py)\n",
            "Unsloth: Could not import trl.trainer.xpo_trainer: Failed to import trl.trainer.xpo_trainer because of the following error (look up to see its traceback):\n",
            "cannot import name 'amp' from 'apex' (/usr/local/lib/python3.12/dist-packages/apex/__init__.py)\n",
            "Loading model for Full Fine-Tuning...\n",
            "  Model: unsloth/Qwen2.5-7B-Instruct\n",
            "  This will train ALL 7.6B parameters!\n",
            "==((====))==  Unsloth 2026.1.4: Fast Qwen2 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA GB10. Num GPUs = 1. Max memory: 119.697 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0a0+b558c986e8.nv25.11. CUDA: 12.1. CUDA Toolkit: 13.0. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33+aa7bc36.d20260203. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using bfloat16 full finetuning which cuts memory usage by 50%.\n",
            "To enable float32 training, use `float32_mixed_precision = True` during FastLanguageModel.from_pretrained\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:17<00:00, 19.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Model loaded for full fine-tuning!\n",
            "  GPU memory used: 15.23 GB\n",
            "  Tokenizer vocab size: 151665\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "print(\"Loading model for Full Fine-Tuning...\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  This will train ALL 7.6B parameters!\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect (will use BF16)\n",
        "    load_in_4bit=False,  # Full fine-tuning uses full precision\n",
        "    full_finetuning=True,  # â† KEY: Enable full fine-tuning\n",
        "    use_exact_model_name=True,  # Use cached model, don't look for alternatives\n",
        ")\n",
        "\n",
        "mem_used = torch.cuda.memory_allocated() / 1e9\n",
        "\n",
        "print(f\"\\nâœ“ Model loaded for full fine-tuning!\")\n",
        "print(f\"  GPU memory used: {mem_used:.2f} GB\")\n",
        "print(f\"  Tokenizer vocab size: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c9d0e1",
      "metadata": {},
      "source": [
        "## 4. Verify All Parameters Are Trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c9d0e1f2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter Count:\n",
            "  Trainable: 7,615,616,512 (100.00%)\n",
            "  Total: 7,615,616,512\n",
            "\n",
            "âœ“ All parameters are trainable (Full Fine-Tuning mode)\n"
          ]
        }
      ],
      "source": [
        "# Count parameters\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"Parameter Count:\")\n",
        "print(f\"  Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
        "print(f\"  Total: {total:,}\")\n",
        "\n",
        "if trainable == total:\n",
        "    print(f\"\\nâœ“ All parameters are trainable (Full Fine-Tuning mode)\")\n",
        "else:\n",
        "    print(f\"\\nâš  Warning: Only {100*trainable/total:.2f}% of parameters are trainable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e1f2a3",
      "metadata": {},
      "source": [
        "## 5. Load Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e1f2a3b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from: /fine-tuning/datasets/train.jsonl\n",
            "\n",
            "Dataset loaded:\n",
            "  Total examples: 120,000\n",
            "  Columns: ['messages']\n",
            "\n",
            "Sample entry:\n",
            "  [system]: You are a news article classifier. Your task is to categorize news articles into...\n",
            "  [user]: Classify the following news article:\n",
            "\n",
            "Thirst, Fear and Bribes on Desert Escape f...\n",
            "  [assistant]: {\"category\":\"World\"}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(f\"Loading dataset from: {TRAIN_DATA_PATH}\")\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=TRAIN_DATA_PATH, split=\"train\")\n",
        "\n",
        "print(f\"\\nDataset loaded:\")\n",
        "print(f\"  Total examples: {len(dataset):,}\")\n",
        "print(f\"  Columns: {dataset.column_names}\")\n",
        "\n",
        "print(f\"\\nSample entry:\")\n",
        "sample = dataset[0]\n",
        "for msg in sample[\"messages\"]:\n",
        "    role = msg[\"role\"]\n",
        "    content = msg[\"content\"][:80] + \"...\" if len(msg[\"content\"]) > 80 else msg[\"content\"]\n",
        "    print(f\"  [{role}]: {content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a3b4c5",
      "metadata": {},
      "source": [
        "## 6. Format Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a3b4c5d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying chat template to dataset...\n",
            "\n",
            "Formatted dataset columns: ['messages', 'text']\n",
            "\n",
            "Sample (first 400 chars):\n",
            "<|im_start|>system\n",
            "You are a news article classifier. Your task is to categorize news articles into exactly one of four categories:\n",
            "\n",
            "- World: News about politics, government, elections, diplomacy, conflicts, and public affairs (domestic or international)\n",
            "- Sports: News about athletic events, games, players, teams, coaches, tournaments, and championships\n",
            "- Business: News about companies, markets, f\n"
          ]
        }
      ],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"Format examples using the tokenizer's chat template.\"\"\"\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"Applying chat template to dataset...\")\n",
        "formatted_dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    desc=\"Formatting\",\n",
        ")\n",
        "\n",
        "print(f\"\\nFormatted dataset columns: {formatted_dataset.column_names}\")\n",
        "print(f\"\\nSample (first 400 chars):\")\n",
        "print(formatted_dataset[0][\"text\"][:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c5d6e7",
      "metadata": {},
      "source": [
        "## 7. Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c5d6e7f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training configuration:\n",
            "  Total examples: 120,000\n",
            "  Batch size: 8 x 2 = 16\n",
            "  Estimated total steps: 7,500\n",
            "\n",
            "âœ“ SFTConfig created!\n",
            "  Note: gradient_checkpointing=False for faster training (requires ~60-70GB memory)\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "total_steps = (len(formatted_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_EPOCHS\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Total examples: {len(formatted_dataset):,}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Estimated total steps: {total_steps:,}\")\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    optim=\"adamw_8bit\",\n",
        "    bf16=True,\n",
        "    fp16=False,\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    packing=True,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    logging_first_step=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=2,\n",
        "    dataloader_num_workers=8,  # Increased for faster data loading\n",
        "    gradient_checkpointing=False,  # Disabled for speed (DGX Spark has 128GB memory)\n",
        "    seed=42,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"\\nâœ“ SFTConfig created!\")\n",
        "print(\"  Note: gradient_checkpointing=False for faster training (requires ~60-70GB memory)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e7f8a9",
      "metadata": {},
      "source": [
        "## 8. Create Trainer and Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e7f8a9b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Sample packing skipped (custom data collator detected).\n",
            "âœ“ Trainer created!\n",
            "\n",
            "Starting Full Fine-Tuning...\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    args=sft_config,\n",
        ")\n",
        "\n",
        "print(\"âœ“ Trainer created!\")\n",
        "print(f\"\\nStarting Full Fine-Tuning...\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f8a9b0c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 120,000 | Num Epochs = 1 | Total steps = 7,500\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 7,615,616,512 of 7,615,616,512 (100.00% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7500/7500 8:35:49, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.950600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.957100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.506500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.484100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.476700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.476300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.481500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.473300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.473500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.475700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.467900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.470300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.470700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.476100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.468700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.473400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.470400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.473500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.466800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.465500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.456500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.461700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.458500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.456100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.456600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.456800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.447400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.450500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.465500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.466200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.450600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.455700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.452100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.457400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.449700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.460800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.452900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.452100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.455100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.452000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.460100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.446500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.452800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.453800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.454200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.446800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.443000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.448100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.446700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.445300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.448600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.438100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.451800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.445600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.444900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.433900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.438800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.445600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.439000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.442700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>0.442100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.449600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.442600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.447600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>0.434400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.432700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>0.445600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.436800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>0.429400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.433700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>0.435500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.431800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.434100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.439500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>0.441400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.439000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3950</td>\n",
              "      <td>0.434500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.433900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>0.432300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.429900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4150</td>\n",
              "      <td>0.428700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.429400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>0.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.424800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>0.443700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.440700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4450</td>\n",
              "      <td>0.426100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.431200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4550</td>\n",
              "      <td>0.430100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.425800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4650</td>\n",
              "      <td>0.429500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.435900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>0.425200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.425700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4850</td>\n",
              "      <td>0.434100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.438800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4950</td>\n",
              "      <td>0.433400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.432900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5050</td>\n",
              "      <td>0.435000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.432300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5150</td>\n",
              "      <td>0.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.437000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>0.432300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>0.435500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5350</td>\n",
              "      <td>0.425400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>0.434000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5450</td>\n",
              "      <td>0.430400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.425500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5550</td>\n",
              "      <td>0.431700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.426900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5650</td>\n",
              "      <td>0.425000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>0.433400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5750</td>\n",
              "      <td>0.427400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>0.427700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5850</td>\n",
              "      <td>0.432400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>0.434600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5950</td>\n",
              "      <td>0.430000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.420700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6050</td>\n",
              "      <td>0.426700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>0.432800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6150</td>\n",
              "      <td>0.431600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>0.431200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6250</td>\n",
              "      <td>0.433300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>0.425000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6350</td>\n",
              "      <td>0.435700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>0.436500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6450</td>\n",
              "      <td>0.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.432400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6550</td>\n",
              "      <td>0.427300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>0.433300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6650</td>\n",
              "      <td>0.437800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>0.433500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6750</td>\n",
              "      <td>0.433600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>0.428400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6850</td>\n",
              "      <td>0.428600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>0.428500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6950</td>\n",
              "      <td>0.435900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.433400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7050</td>\n",
              "      <td>0.423400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>0.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7150</td>\n",
              "      <td>0.427400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>0.430200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7250</td>\n",
              "      <td>0.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>0.431200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7350</td>\n",
              "      <td>0.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>0.438000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7450</td>\n",
              "      <td>0.428700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.428500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Full Fine-Tuning Complete!\n",
            "============================================================\n",
            "\n",
            "Training time: 8h 35m 59s\n",
            "Final loss: 0.4536\n",
            "Total steps: 7500\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "hours, remainder = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Full Fine-Tuning Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTraining time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
        "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"Total steps: {trainer_stats.global_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9b0c1d2",
      "metadata": {},
      "source": [
        "## 9. Save the Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b0c1d2e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving full fine-tuned model to: ./checkpoints/qwen7b-ag-news-full-unsloth/final\n",
            "\n",
            "Saved files:\n",
            "  added_tokens.json: 0.00 MB\n",
            "  chat_template.jinja: 0.00 MB\n",
            "  config.json: 0.00 MB\n",
            "  generation_config.json: 0.00 MB\n",
            "  merges.txt: 1.67 MB\n",
            "  model-00001-of-00004.safetensors: 4.88 GB\n",
            "  model-00002-of-00004.safetensors: 4.93 GB\n",
            "  model-00003-of-00004.safetensors: 4.33 GB\n",
            "  model-00004-of-00004.safetensors: 1.09 GB\n",
            "  model.safetensors.index.json: 0.03 MB\n",
            "\n",
            "Total model size: 15.25 GB\n",
            "\n",
            "âœ“ Full model saved!\n"
          ]
        }
      ],
      "source": [
        "model_path = f\"{OUTPUT_DIR}/final\"\n",
        "\n",
        "print(f\"Saving full fine-tuned model to: {model_path}\")\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "# Check saved files\n",
        "import os\n",
        "saved_files = os.listdir(model_path)\n",
        "total_size = sum(os.path.getsize(os.path.join(model_path, f)) for f in saved_files if os.path.isfile(os.path.join(model_path, f)))\n",
        "\n",
        "print(f\"\\nSaved files:\")\n",
        "for f in sorted(saved_files)[:10]:  # Show first 10 files\n",
        "    path = os.path.join(model_path, f)\n",
        "    if os.path.isfile(path):\n",
        "        size = os.path.getsize(path)\n",
        "        print(f\"  {f}: {size / 1e9:.2f} GB\" if size > 1e9 else f\"  {f}: {size / 1e6:.2f} MB\")\n",
        "\n",
        "print(f\"\\nTotal model size: {total_size / 1e9:.2f} GB\")\n",
        "print(f\"\\nâœ“ Full model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1d2e3f4",
      "metadata": {},
      "source": [
        "## 10. Quick Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d2e3f4a5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing full fine-tuned model:\n",
            "============================================================\n",
            "\n",
            "Article: The Federal Reserve announced a quarter-point inte...\n",
            "Expected: Business\n",
            "Response: {\"category\": \"Business\"}\n",
            "Status: âœ“\n",
            "\n",
            "Article: Scientists at CERN discovered a new subatomic part...\n",
            "Expected: Sci/Tech\n",
            "Response: {\"category\": \"Sci/Tech\"}\n",
            "Status: âœ“\n",
            "\n",
            "Article: The Lakers defeated the Celtics 112-108 in overtim...\n",
            "Expected: Sports\n",
            "Response: {\"category\": \"Sports\"}\n",
            "Status: âœ“\n",
            "\n",
            "Article: The UN Security Council voted to impose new sancti...\n",
            "Expected: World\n",
            "Response: {\"category\": \"World\"}\n",
            "Status: âœ“\n",
            "\n",
            "============================================================\n",
            "Quick test accuracy: 4/4 (100%)\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a news article classifier. Categorize into: World, Sports, Business, or Sci/Tech.\n",
        "Respond with JSON: {\"category\": \"<category>\"}\"\"\"\n",
        "\n",
        "test_articles = [\n",
        "    (\"The Federal Reserve announced a quarter-point interest rate cut.\", \"Business\"),\n",
        "    (\"Scientists at CERN discovered a new subatomic particle.\", \"Sci/Tech\"),\n",
        "    (\"The Lakers defeated the Celtics 112-108 in overtime.\", \"Sports\"),\n",
        "    (\"The UN Security Council voted to impose new sanctions.\", \"World\"),\n",
        "]\n",
        "\n",
        "print(\"Testing full fine-tuned model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "correct = 0\n",
        "for article, expected in test_articles:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"Classify: {article}\"},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True,\n",
        "    ).to(model.device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    is_correct = expected.lower() in response.lower()\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    \n",
        "    print(f\"\\nArticle: {article[:50]}...\")\n",
        "    print(f\"Expected: {expected}\")\n",
        "    print(f\"Response: {response.strip()}\")\n",
        "    print(f\"Status: {'âœ“' if is_correct else 'âœ—'}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"Quick test accuracy: {correct}/{len(test_articles)} ({100*correct/len(test_articles):.0f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3f4a5b6",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "### Training Results\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| **Training Time** | 8h 35m 59s |\n",
        "| **Avg Training Loss** | 0.4536 |\n",
        "| **Total Steps** | 7,500 |\n",
        "| **Trainable Parameters** | 7.6B (100%) |\n",
        "| **Model Size** | 15.25 GB |\n",
        "| **Training Speed** | ~0.24 it/s |\n",
        "| **GPU Memory** | ~70 GB |\n",
        "\n",
        "### Quick Validation Results\n",
        "\n",
        "| Test Article | Expected | Predicted | Status |\n",
        "|--------------|----------|-----------|--------|\n",
        "| Federal Reserve interest rate cut | Business | Business | âœ“ |\n",
        "| CERN subatomic particle discovery | Sci/Tech | Sci/Tech | âœ“ |\n",
        "| Lakers vs Celtics game | Sports | Sports | âœ“ |\n",
        "| UN Security Council sanctions | World | World | âœ“ |\n",
        "\n",
        "**Quick test accuracy: 4/4 (100%)**\n",
        "\n",
        "### Comparison: Full Fine-Tuning vs LoRA vs QLoRA\n",
        "\n",
        "| Aspect | QLoRA | LoRA | Full Fine-Tuning |\n",
        "|--------|-------|------|------------------|\n",
        "| **Parameters Trained** | 40M (0.5%) | 40M (0.5%) | 7.6B (100%) |\n",
        "| **Training Speed** | ~0.35 it/s | ~0.36 it/s | ~0.24 it/s |\n",
        "| **Training Time** | 5h 58m | 5h 51m | 8h 36m |\n",
        "| **Avg Training Loss** | 0.4625 | 0.4600 | 0.4536 |\n",
        "| **Memory Usage** | ~15 GB | ~25 GB | ~70 GB |\n",
        "| **Output Size** | 177 MB | 177 MB | 15.25 GB |\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "1. **Full fine-tuning achieves the lowest loss** (0.4536 vs 0.4600/0.4625):\n",
        "   - Training all 7.6B parameters allows more complete model adaptation\n",
        "   - ~1.4% lower average loss than LoRA, ~1.9% lower than QLoRA\n",
        "   - Difference is modest but may translate to slightly better accuracy\n",
        "\n",
        "2. **Training is slower but not drastically** (0.24 it/s vs 0.35 it/s):\n",
        "   - Full fine-tuning is ~33% slower than LoRA/QLoRA\n",
        "   - Memory bandwidth is the bottleneck for all methods on DGX Spark\n",
        "   - 8.5h is reasonable for training all 7.6B parameters\n",
        "\n",
        "3. **Memory usage is significant but manageable**:\n",
        "   - ~70 GB fits comfortably in DGX Spark's 128 GB\n",
        "   - `adamw_8bit` optimizer saves ~15 GB vs standard AdamW\n",
        "   - `gradient_checkpointing=False` trades memory for speed\n",
        "\n",
        "4. **Output is a full model, not an adapter**:\n",
        "   - 15.25 GB model can be deployed standalone\n",
        "   - No need to load base model + adapter separately\n",
        "   - Can be further quantized (GGUF, AWQ, etc.) for deployment\n",
        "\n",
        "### Memory Breakdown\n",
        "\n",
        "| Component | Size |\n",
        "|-----------|------|\n",
        "| Model weights (BF16) | ~15 GB |\n",
        "| Gradients (BF16) | ~15 GB |\n",
        "| Optimizer states (8-bit) | ~15 GB |\n",
        "| Activations | ~15-25 GB |\n",
        "| **Total** | **~60-70 GB** |\n",
        "\n",
        "### DGX Spark Optimizations Impact\n",
        "\n",
        "| Optimization | Impact |\n",
        "|--------------|--------|\n",
        "| `gradient_checkpointing=False` | +50% speed (0.16 â†’ 0.24 it/s) |\n",
        "| `BATCH_SIZE=8` | Better GPU utilization |\n",
        "| `adamw_8bit` | -15 GB memory |\n",
        "| `dataloader_num_workers=8` | Faster data loading |\n",
        "\n",
        "### Recommendation\n",
        "\n",
        "**When to use Full Fine-Tuning:**\n",
        "- Maximum quality is required\n",
        "- You have sufficient memory (60+ GB)\n",
        "- You want a standalone model (no adapter dependencies)\n",
        "- Training time is not critical\n",
        "\n",
        "**When to use LoRA/QLoRA instead:**\n",
        "- Rapid iteration needed (5-6h vs 8.5h)\n",
        "- Memory constrained environments\n",
        "- You want small adapter files for easy switching\n",
        "- Loss difference (~1-2%) is acceptable\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Full Evaluation**: Run comprehensive test on AG News test set (7,600 samples)\n",
        "2. **Compare Accuracy**: Verify if 1.4% lower loss translates to better accuracy than QLoRA (95.14%)\n",
        "3. **Quantization**: Export model to GGUF/AWQ for efficient deployment"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
