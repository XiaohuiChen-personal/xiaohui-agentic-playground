{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1b2c3d4",
      "metadata": {},
      "source": [
        "# Full Fine-Tuning with Unsloth: Qwen2.5-7B on AG News\n",
        "\n",
        "This notebook demonstrates **Full Fine-Tuning** using **Unsloth's FastLanguageModel** for optimized performance on DGX Spark.\n",
        "\n",
        "## Overview\n",
        "\n",
        "| Aspect | Details |\n",
        "|--------|---------|\n",
        "| **Model** | unsloth/Qwen2.5-7B-Instruct |\n",
        "| **Method** | Full Fine-Tuning (100% parameters) |\n",
        "| **Framework** | Unsloth + TRL |\n",
        "| **Dataset** | AG News (120K train, 7.6K test) |\n",
        "| **Task** | 4-class text classification |\n",
        "| **Expected Time** | ~4-5 hours (with Unsloth optimizations) |\n",
        "| **Memory** | ~60-70 GB |\n",
        "| **Output** | Full model (~14 GB) |\n",
        "\n",
        "## Base Model Performance (Target to Beat)\n",
        "\n",
        "| Metric | Base Model | Target |\n",
        "|--------|------------|--------|\n",
        "| **Accuracy** | 78.76% | >88% |\n",
        "| **F1 (macro)** | 77.97% | >85% |\n",
        "| **Sci/Tech F1** | 62.06% | >80% |\n",
        "| **Business Precision** | 63.66% | >80% |\n",
        "\n",
        "## Full Fine-Tuning vs LoRA\n",
        "\n",
        "| Aspect | Full Fine-Tuning | LoRA |\n",
        "|--------|------------------|------|\n",
        "| Parameters trained | 7.6B (100%) | ~70M (1%) |\n",
        "| Memory usage | ~60-70 GB | ~25 GB |\n",
        "| Output size | ~14 GB | ~200 MB |\n",
        "| Quality | Maximum | Slightly lower |\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "```bash\n",
        "./start_docker.sh start finetune\n",
        "# Then open http://localhost:8888\n",
        "```\n",
        "\n",
        "**Note**: Full fine-tuning requires significant memory. DGX Spark's 128 GB unified memory is sufficient."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c3d4e5",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d4e5f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Environment Verification - Full Fine-Tuning\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "    try:\n",
        "        total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
        "    except:\n",
        "        print(\"GPU Memory: Unified memory system (DGX Spark)\")\n",
        "else:\n",
        "    raise RuntimeError(\"CUDA not available!\")\n",
        "\n",
        "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
        "print(f\"Dataset available: {os.path.exists('/fine-tuning-dense/datasets/train.jsonl')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e5f6a7",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f6a7b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Model Configuration\n",
        "# =============================================================================\n",
        "MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# =============================================================================\n",
        "# Training Configuration\n",
        "# =============================================================================\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 2e-5  # Lower than LoRA for full fine-tuning\n",
        "NUM_EPOCHS = 1\n",
        "WARMUP_RATIO = 0.03\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# =============================================================================\n",
        "# Output Configuration\n",
        "# =============================================================================\n",
        "OUTPUT_DIR = \"./checkpoints/qwen7b-ag-news-full-unsloth\"\n",
        "LOGGING_STEPS = 50\n",
        "SAVE_STEPS = 500\n",
        "\n",
        "TRAIN_DATA_PATH = \"/fine-tuning-dense/datasets/train.jsonl\"\n",
        "\n",
        "print(\"Full Fine-Tuning Configuration loaded!\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Method: Full Fine-Tuning (100% parameters)\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a7b8c9",
      "metadata": {},
      "source": [
        "## 3. Load Model with Full Fine-Tuning Enabled\n",
        "\n",
        "Using `full_finetuning=True` to train all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b8c9d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "print(\"Loading model for Full Fine-Tuning...\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  This will train ALL 7.6B parameters!\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect (will use BF16)\n",
        "    load_in_4bit=False,  # Full fine-tuning uses full precision\n",
        "    full_finetuning=True,  # ← KEY: Enable full fine-tuning\n",
        ")\n",
        "\n",
        "mem_used = torch.cuda.memory_allocated() / 1e9\n",
        "\n",
        "print(f\"\\n✓ Model loaded for full fine-tuning!\")\n",
        "print(f\"  GPU memory used: {mem_used:.2f} GB\")\n",
        "print(f\"  Tokenizer vocab size: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c9d0e1",
      "metadata": {},
      "source": [
        "## 4. Verify All Parameters Are Trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d0e1f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count parameters\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"Parameter Count:\")\n",
        "print(f\"  Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
        "print(f\"  Total: {total:,}\")\n",
        "\n",
        "if trainable == total:\n",
        "    print(f\"\\n✓ All parameters are trainable (Full Fine-Tuning mode)\")\n",
        "else:\n",
        "    print(f\"\\n⚠ Warning: Only {100*trainable/total:.2f}% of parameters are trainable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e1f2a3",
      "metadata": {},
      "source": [
        "## 5. Load Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f2a3b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(f\"Loading dataset from: {TRAIN_DATA_PATH}\")\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=TRAIN_DATA_PATH, split=\"train\")\n",
        "\n",
        "print(f\"\\nDataset loaded:\")\n",
        "print(f\"  Total examples: {len(dataset):,}\")\n",
        "print(f\"  Columns: {dataset.column_names}\")\n",
        "\n",
        "print(f\"\\nSample entry:\")\n",
        "sample = dataset[0]\n",
        "for msg in sample[\"messages\"]:\n",
        "    role = msg[\"role\"]\n",
        "    content = msg[\"content\"][:80] + \"...\" if len(msg[\"content\"]) > 80 else msg[\"content\"]\n",
        "    print(f\"  [{role}]: {content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a3b4c5",
      "metadata": {},
      "source": [
        "## 6. Format Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b4c5d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"Format examples using the tokenizer's chat template.\"\"\"\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"Applying chat template to dataset...\")\n",
        "formatted_dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    desc=\"Formatting\",\n",
        ")\n",
        "\n",
        "print(f\"\\nFormatted dataset columns: {formatted_dataset.column_names}\")\n",
        "print(f\"\\nSample (first 400 chars):\")\n",
        "print(formatted_dataset[0][\"text\"][:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c5d6e7",
      "metadata": {},
      "source": [
        "## 7. Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d6e7f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "total_steps = (len(formatted_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_EPOCHS\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Total examples: {len(formatted_dataset):,}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Estimated total steps: {total_steps:,}\")\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    optim=\"adamw_8bit\",\n",
        "    bf16=True,\n",
        "    fp16=False,\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    packing=True,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    logging_first_step=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=2,\n",
        "    dataloader_num_workers=4,\n",
        "    gradient_checkpointing=True,  # Essential for full fine-tuning\n",
        "    seed=42,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"\\n✓ SFTConfig created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e7f8a9",
      "metadata": {},
      "source": [
        "## 8. Create Trainer and Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f8a9b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    args=sft_config,\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer created!\")\n",
        "print(f\"\\nStarting Full Fine-Tuning...\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a9b0c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "hours, remainder = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Full Fine-Tuning Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTraining time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
        "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"Total steps: {trainer_stats.global_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9b0c1d2",
      "metadata": {},
      "source": [
        "## 9. Save the Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0c1d2e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = f\"{OUTPUT_DIR}/final\"\n",
        "\n",
        "print(f\"Saving full fine-tuned model to: {model_path}\")\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "# Check saved files\n",
        "import os\n",
        "saved_files = os.listdir(model_path)\n",
        "total_size = sum(os.path.getsize(os.path.join(model_path, f)) for f in saved_files if os.path.isfile(os.path.join(model_path, f)))\n",
        "\n",
        "print(f\"\\nSaved files:\")\n",
        "for f in sorted(saved_files)[:10]:  # Show first 10 files\n",
        "    path = os.path.join(model_path, f)\n",
        "    if os.path.isfile(path):\n",
        "        size = os.path.getsize(path)\n",
        "        print(f\"  {f}: {size / 1e9:.2f} GB\" if size > 1e9 else f\"  {f}: {size / 1e6:.2f} MB\")\n",
        "\n",
        "print(f\"\\nTotal model size: {total_size / 1e9:.2f} GB\")\n",
        "print(f\"\\n✓ Full model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1d2e3f4",
      "metadata": {},
      "source": [
        "## 10. Quick Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e3f4a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a news article classifier. Categorize into: World, Sports, Business, or Sci/Tech.\n",
        "Respond with JSON: {\"category\": \"<category>\"}\"\"\"\n",
        "\n",
        "test_articles = [\n",
        "    (\"The Federal Reserve announced a quarter-point interest rate cut.\", \"Business\"),\n",
        "    (\"Scientists at CERN discovered a new subatomic particle.\", \"Sci/Tech\"),\n",
        "    (\"The Lakers defeated the Celtics 112-108 in overtime.\", \"Sports\"),\n",
        "    (\"The UN Security Council voted to impose new sanctions.\", \"World\"),\n",
        "]\n",
        "\n",
        "print(\"Testing full fine-tuned model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "correct = 0\n",
        "for article, expected in test_articles:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"Classify: {article}\"},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True,\n",
        "    ).to(model.device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    is_correct = expected.lower() in response.lower()\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    \n",
        "    print(f\"\\nArticle: {article[:50]}...\")\n",
        "    print(f\"Expected: {expected}\")\n",
        "    print(f\"Response: {response.strip()}\")\n",
        "    print(f\"Status: {'✓' if is_correct else '✗'}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"Quick test accuracy: {correct}/{len(test_articles)} ({100*correct/len(test_articles):.0f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3f4a5b6",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "*To be filled after running the notebook*\n",
        "\n",
        "### Training Results\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Training Time | TBD |\n",
        "| Final Loss | TBD |\n",
        "| Total Steps | TBD |\n",
        "| Model Size | TBD |\n",
        "| GPU Memory | TBD |\n",
        "\n",
        "### Performance Comparison\n",
        "\n",
        "| Metric | HuggingFace Full | Unsloth Full |\n",
        "|--------|------------------|---------------|\n",
        "| Tokens/sec | ~900 | TBD |\n",
        "| Training time | ~10h | TBD |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
