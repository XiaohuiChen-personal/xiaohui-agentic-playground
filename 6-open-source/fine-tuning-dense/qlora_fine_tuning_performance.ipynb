{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "overview",
      "metadata": {},
      "source": [
        "# QLoRA Fine-Tuned Model Performance on AG News Dataset\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook evaluates the **QLoRA fine-tuned Qwen2.5-7B model** on the AG News test set and compares its performance against the base model.\n",
        "\n",
        "| Aspect | Details |\n",
        "|--------|--------|\n",
        "| **Base Model** | unsloth/Qwen2.5-7B-Instruct |\n",
        "| **Fine-Tuning Method** | QLoRA (4-bit quantized base + LoRA adapters) |\n",
        "| **Adapter Name** | qlora-ag-news |\n",
        "| **Task** | 4-class text classification (AG News) |\n",
        "| **Test Set Size** | 7,600 samples |\n",
        "| **Max Concurrency** | 64 workers |\n",
        "\n",
        "## Base Model Performance (Baseline)\n",
        "\n",
        "| Metric | Base Model |\n",
        "|--------|------------|\n",
        "| **Accuracy** | 78.76% |\n",
        "| **F1 (macro)** | 77.97% |\n",
        "| **F1 (weighted)** | 77.97% |\n",
        "| **Sci/Tech F1** | 62.06% |\n",
        "| **Business Precision** | 63.66% |\n",
        "\n",
        "## Target Performance\n",
        "\n",
        "| Metric | Target |\n",
        "|--------|---------|\n",
        "| **Accuracy** | >85% |\n",
        "| **Sci/Tech F1** | >75% |\n",
        "| **Business Precision** | >75% |\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This notebook uses **vLLM with QLoRA adapter support** for fast parallel inference.\n",
        "\n",
        "Start the server with QLoRA adapter enabled:\n",
        "\n",
        "```bash\n",
        "cd 6-open-source\n",
        "./start_docker.sh start qwen7b-qlora\n",
        "```\n",
        "\n",
        "Wait for the server to be ready, then run this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-header",
      "metadata": {},
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import time\n",
        "import json\n",
        "import httpx\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "from enum import Enum\n",
        "\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "from pydantic import BaseModel\n",
        "from datasets import load_dataset\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# Fix for Jupyter's event loop - allows nested async calls\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config-header",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# vLLM Server Configuration\n",
        "VLLM_BASE_URL = \"http://localhost:8000/v1\"\n",
        "\n",
        "# Model names - use the LoRA adapter name for fine-tuned inference\n",
        "BASE_MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct\"  # Base model\n",
        "QLORA_MODEL_NAME = \"qlora-ag-news\"  # LoRA adapter name (registered with vLLM)\n",
        "\n",
        "# Use the QLoRA adapter for this evaluation\n",
        "MODEL_NAME = QLORA_MODEL_NAME\n",
        "\n",
        "# Inference configuration\n",
        "MAX_WORKERS = 64  # Maximum parallel requests (matches vLLM max_num_seqs)\n",
        "\n",
        "# Label mapping for AG News\n",
        "LABEL_NAMES = {\n",
        "    0: \"World\",\n",
        "    1: \"Sports\", \n",
        "    2: \"Business\",\n",
        "    3: \"Sci/Tech\"\n",
        "}\n",
        "\n",
        "NAME_TO_LABEL = {v: k for k, v in LABEL_NAMES.items()}\n",
        "\n",
        "print(f\"vLLM Server: {VLLM_BASE_URL}\")\n",
        "print(f\"Model (QLoRA adapter): {MODEL_NAME}\")\n",
        "print(f\"Max parallel workers: {MAX_WORKERS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verify-header",
      "metadata": {},
      "source": [
        "## Verify vLLM Server Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify-server",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create client with extended timeout for batch processing\n",
        "client = OpenAI(\n",
        "    base_url=VLLM_BASE_URL,\n",
        "    api_key=\"not-needed\",  # vLLM doesn't require auth\n",
        "    timeout=httpx.Timeout(120.0, connect=10.0)\n",
        ")\n",
        "\n",
        "# Verify connection\n",
        "try:\n",
        "    models = client.models.list()\n",
        "    print(\"Connected to vLLM server!\")\n",
        "    print(\"\\nAvailable models:\")\n",
        "    for model in models.data:\n",
        "        print(f\"  - {model.id}\")\n",
        "    \n",
        "    # Check if our LoRA adapter is available\n",
        "    model_ids = [m.id for m in models.data]\n",
        "    if QLORA_MODEL_NAME in model_ids:\n",
        "        print(f\"\\n✓ QLoRA adapter '{QLORA_MODEL_NAME}' is available!\")\n",
        "    else:\n",
        "        print(f\"\\n✗ QLoRA adapter '{QLORA_MODEL_NAME}' not found!\")\n",
        "        print(\"Make sure to start vLLM with: ./start_docker.sh start qwen7b-qlora\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to connect to vLLM server: {e}\")\n",
        "    print(f\"\\nMake sure the server is running:\")\n",
        "    print(f\"  cd 6-open-source && ./start_docker.sh start qwen7b-qlora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-data-header",
      "metadata": {},
      "source": [
        "## Load Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load AG News dataset from Hugging Face\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "test_data = dataset[\"test\"]\n",
        "\n",
        "print(f\"Test dataset loaded!\")\n",
        "print(f\"  Total test samples: {len(test_data):,}\")\n",
        "\n",
        "# Show distribution\n",
        "from collections import Counter\n",
        "test_counts = Counter(test_data[\"label\"])\n",
        "print(f\"\\nCategory distribution:\")\n",
        "for label in sorted(LABEL_NAMES.keys()):\n",
        "    print(f\"  {LABEL_NAMES[label]}: {test_counts[label]:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prompts-header",
      "metadata": {},
      "source": [
        "## Define Prompts and Structured Output Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prompts",
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt (same as used for training and base model evaluation)\n",
        "SYSTEM_PROMPT = \"\"\"You are a news article classifier. Your task is to categorize news articles into exactly one of four categories:\n",
        "\n",
        "- World: News about politics, government, elections, diplomacy, conflicts, and public affairs (domestic or international)\n",
        "- Sports: News about athletic events, games, players, teams, coaches, tournaments, and championships\n",
        "- Business: News about companies, markets, finance, economy, trade, corporate activities, and business services\n",
        "- Sci/Tech: News about technology products, software, hardware, scientific research, gadgets, and tech innovations\n",
        "\n",
        "Rules:\n",
        "- Focus on the PRIMARY topic of the article\n",
        "- Ignore HTML artifacts (like #39; or &lt;b&gt;) - they are formatting errors\n",
        "- If an article is truncated, classify based on the available content\n",
        "- When a topic spans multiple categories, choose the one that best represents the main focus\"\"\"\n",
        "\n",
        "# Pydantic schema for structured output\n",
        "class NewsCategory(str, Enum):\n",
        "    WORLD = \"World\"\n",
        "    SPORTS = \"Sports\"\n",
        "    BUSINESS = \"Business\"\n",
        "    SCI_TECH = \"Sci/Tech\"\n",
        "\n",
        "class ClassificationResponse(BaseModel):\n",
        "    category: NewsCategory\n",
        "\n",
        "print(\"Prompts and schema defined.\")\n",
        "print(f\"\\nValid categories: {[c.value for c in NewsCategory]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "classify-header",
      "metadata": {},
      "source": [
        "## Classification Function with Structured Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classify-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create async client\n",
        "async_client = AsyncOpenAI(\n",
        "    base_url=VLLM_BASE_URL,\n",
        "    api_key=\"not-needed\",\n",
        "    timeout=httpx.Timeout(120.0, connect=10.0)\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class ClassificationOutput:\n",
        "    \"\"\"Result of a single classification\"\"\"\n",
        "    index: int\n",
        "    ground_truth: int\n",
        "    predicted: Optional[int] = None\n",
        "    predicted_name: Optional[str] = None\n",
        "    ground_truth_name: str = \"\"\n",
        "    raw_output: str = \"\"\n",
        "    error: Optional[str] = None\n",
        "    latency_ms: float = 0.0\n",
        "\n",
        "async def classify_article(index: int, article_text: str, ground_truth: int) -> ClassificationOutput:\n",
        "    \"\"\"Classify a single article using structured output\"\"\"\n",
        "    start_time = time.perf_counter()\n",
        "    \n",
        "    try:\n",
        "        response = await async_client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"Classify the following news article:\\n\\n{article_text}\"}\n",
        "            ],\n",
        "            max_tokens=20,\n",
        "            temperature=0.1,\n",
        "            extra_body={\n",
        "                \"guided_json\": ClassificationResponse.model_json_schema()\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        latency = (time.perf_counter() - start_time) * 1000\n",
        "        raw_output = response.choices[0].message.content\n",
        "        \n",
        "        # Parse structured output\n",
        "        parsed = ClassificationResponse.model_validate_json(raw_output)\n",
        "        predicted_name = parsed.category.value\n",
        "        predicted_label = NAME_TO_LABEL.get(predicted_name)\n",
        "        \n",
        "        return ClassificationOutput(\n",
        "            index=index,\n",
        "            ground_truth=ground_truth,\n",
        "            predicted=predicted_label,\n",
        "            predicted_name=predicted_name,\n",
        "            ground_truth_name=LABEL_NAMES[ground_truth],\n",
        "            raw_output=raw_output,\n",
        "            latency_ms=latency\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        latency = (time.perf_counter() - start_time) * 1000\n",
        "        return ClassificationOutput(\n",
        "            index=index,\n",
        "            ground_truth=ground_truth,\n",
        "            ground_truth_name=LABEL_NAMES[ground_truth],\n",
        "            error=str(e),\n",
        "            latency_ms=latency\n",
        "        )\n",
        "\n",
        "print(\"Classification function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test-header",
      "metadata": {},
      "source": [
        "## Quick Test: Single Article Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-single",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a single article\n",
        "test_article = test_data[0]\n",
        "print(f\"Test article:\")\n",
        "print(f\"Text: {test_article['text'][:200]}...\")\n",
        "print(f\"Ground Truth: {LABEL_NAMES[test_article['label']]}\")\n",
        "\n",
        "# Run single test\n",
        "result = asyncio.get_event_loop().run_until_complete(\n",
        "    classify_article(0, test_article[\"text\"], test_article[\"label\"])\n",
        ")\n",
        "\n",
        "print(f\"\\n--- Result ---\")\n",
        "print(f\"Predicted: {result.predicted_name}\")\n",
        "print(f\"Ground Truth: {result.ground_truth_name}\")\n",
        "print(f\"Correct: {'✓' if result.predicted == result.ground_truth else '✗'}\")\n",
        "print(f\"Latency: {result.latency_ms:.0f}ms\")\n",
        "if result.error:\n",
        "    print(f\"Error: {result.error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "batch-header",
      "metadata": {},
      "source": [
        "## Batch Classification with Parallel Workers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "batch-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def classify_batch(\n",
        "    data,\n",
        "    max_workers: int = MAX_WORKERS,\n",
        "    show_progress: bool = True\n",
        ") -> list[ClassificationOutput]:\n",
        "    \"\"\"\n",
        "    Classify a batch of articles with parallel workers.\n",
        "    \n",
        "    Args:\n",
        "        data: Dataset with 'text' and 'label' fields\n",
        "        max_workers: Maximum concurrent requests\n",
        "        show_progress: Show progress bar\n",
        "    \n",
        "    Returns:\n",
        "        List of ClassificationOutput results\n",
        "    \"\"\"\n",
        "    # Create semaphore to limit concurrency\n",
        "    semaphore = asyncio.Semaphore(max_workers)\n",
        "    \n",
        "    async def classify_with_semaphore(index: int, article_text: str, ground_truth: int):\n",
        "        async with semaphore:\n",
        "            return await classify_article(index, article_text, ground_truth)\n",
        "    \n",
        "    # Create all tasks\n",
        "    tasks = [\n",
        "        classify_with_semaphore(i, example[\"text\"], example[\"label\"])\n",
        "        for i, example in enumerate(data)\n",
        "    ]\n",
        "    \n",
        "    # Run with progress bar\n",
        "    if show_progress:\n",
        "        results = await tqdm_asyncio.gather(*tasks, desc=\"Classifying\")\n",
        "    else:\n",
        "        results = await asyncio.gather(*tasks)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def run_classification(data, max_workers: int = MAX_WORKERS) -> list[ClassificationOutput]:\n",
        "    \"\"\"Synchronous wrapper for batch classification\"\"\"\n",
        "    return asyncio.get_event_loop().run_until_complete(\n",
        "        classify_batch(data, max_workers)\n",
        "    )\n",
        "\n",
        "print(\"Batch classification function ready!\")\n",
        "print(f\"  - Max parallel workers: {MAX_WORKERS}\")\n",
        "print(f\"  - Test set size: {len(test_data):,} articles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fulltest-header",
      "metadata": {},
      "source": [
        "## Run Full Test Set Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fulltest",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"RUNNING CLASSIFICATION ON FULL TEST SET (QLoRA Fine-Tuned)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nTest set size: {len(test_data):,} articles\")\n",
        "print(f\"Max parallel workers: {MAX_WORKERS}\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(\"\\nStarting classification...\\n\")\n",
        "\n",
        "# Run classification\n",
        "start_time = time.time()\n",
        "\n",
        "# Run classification\n",
        "results = run_classification(test_data, max_workers=MAX_WORKERS)\n",
        "\n",
        "# Calculate total time\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "# Summary\n",
        "successful = [r for r in results if r.error is None]\n",
        "failed = [r for r in results if r.error is not None]\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(f\"CLASSIFICATION COMPLETE\")\n",
        "print(f\"=\" * 70)\n",
        "print(f\"Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
        "print(f\"Throughput: {len(test_data)/total_time:.1f} articles/second\")\n",
        "print(f\"Successful: {len(successful):,} ({len(successful)/len(results)*100:.1f}%)\")\n",
        "print(f\"Failed: {len(failed):,} ({len(failed)/len(results)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval-header",
      "metadata": {},
      "source": [
        "## Model Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eval-metrics",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, \n",
        "    precision_score, \n",
        "    recall_score, \n",
        "    f1_score, \n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "# Extract predictions (only from successful results)\n",
        "y_true = [r.ground_truth for r in successful]\n",
        "y_pred = [r.predicted for r in successful]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
        "precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
        "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
        "recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n{'Metric':<25} {'Value':>15}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"{'Accuracy':<25} {accuracy:>14.2%}\")\n",
        "print(f\"{'Precision (macro)':<25} {precision_macro:>14.2%}\")\n",
        "print(f\"{'Precision (weighted)':<25} {precision_weighted:>14.2%}\")\n",
        "print(f\"{'Recall (macro)':<25} {recall_macro:>14.2%}\")\n",
        "print(f\"{'Recall (weighted)':<25} {recall_weighted:>14.2%}\")\n",
        "print(f\"{'F1 Score (macro)':<25} {f1_macro:>14.2%}\")\n",
        "print(f\"{'F1 Score (weighted)':<25} {f1_weighted:>14.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classification-report",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed classification report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT BY CATEGORY\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "target_names = [LABEL_NAMES[i] for i in range(4)]\n",
        "print(classification_report(y_true, y_pred, target_names=target_names, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confusion-header",
      "metadata": {},
      "source": [
        "## Confusion Matrix Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confusion-matrix",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# Labels\n",
        "ax.set(\n",
        "    xticks=np.arange(4),\n",
        "    yticks=np.arange(4),\n",
        "    xticklabels=target_names,\n",
        "    yticklabels=target_names,\n",
        "    ylabel='True Label',\n",
        "    xlabel='Predicted Label',\n",
        "    title='QLoRA Fine-Tuned Model - Confusion Matrix'\n",
        ")\n",
        "\n",
        "# Rotate x labels\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# Add text annotations\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('qlora_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Confusion matrix saved to 'qlora_confusion_matrix.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison-header",
      "metadata": {},
      "source": [
        "## Performance Comparison: Base Model vs QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base model results (from base_model_performance.ipynb)\n",
        "base_model_metrics = {\n",
        "    \"accuracy\": 0.7876,\n",
        "    \"precision_macro\": 0.8198,\n",
        "    \"recall_macro\": 0.7876,\n",
        "    \"f1_macro\": 0.7797,\n",
        "    \"f1_weighted\": 0.7797,\n",
        "}\n",
        "\n",
        "# QLoRA fine-tuned results\n",
        "qlora_metrics = {\n",
        "    \"accuracy\": accuracy,\n",
        "    \"precision_macro\": precision_macro,\n",
        "    \"recall_macro\": recall_macro,\n",
        "    \"f1_macro\": f1_macro,\n",
        "    \"f1_weighted\": f1_weighted,\n",
        "}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PERFORMANCE COMPARISON: BASE MODEL vs QLoRA FINE-TUNED\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Metric':<25} {'Base Model':>15} {'QLoRA':>15} {'Δ Change':>15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for metric in [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"f1_weighted\"]:\n",
        "    base_val = base_model_metrics[metric]\n",
        "    qlora_val = qlora_metrics[metric]\n",
        "    delta = qlora_val - base_val\n",
        "    delta_str = f\"+{delta:.2%}\" if delta >= 0 else f\"{delta:.2%}\"\n",
        "    \n",
        "    metric_name = metric.replace(\"_\", \" \").title()\n",
        "    print(f\"{metric_name:<25} {base_val:>14.2%} {qlora_val:>14.2%} {delta_str:>15}\")\n",
        "\n",
        "# Calculate overall improvement\n",
        "accuracy_improvement = (qlora_metrics[\"accuracy\"] - base_model_metrics[\"accuracy\"]) / base_model_metrics[\"accuracy\"] * 100\n",
        "f1_improvement = (qlora_metrics[\"f1_macro\"] - base_model_metrics[\"f1_macro\"]) / base_model_metrics[\"f1_macro\"] * 100\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"RELATIVE IMPROVEMENT\")\n",
        "print(f\"  Accuracy:  {accuracy_improvement:+.1f}%\")\n",
        "print(f\"  F1 (macro): {f1_improvement:+.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-header",
      "metadata": {},
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-results",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive results dictionary\n",
        "qlora_results = {\n",
        "    \"model_name\": BASE_MODEL_NAME,\n",
        "    \"adapter_name\": QLORA_MODEL_NAME,\n",
        "    \"model_type\": \"qlora_fine_tuned\",\n",
        "    \"test_set_size\": len(test_data),\n",
        "    \"successful_predictions\": len(successful),\n",
        "    \"failed_predictions\": len(failed),\n",
        "    \"total_time_seconds\": total_time,\n",
        "    \"throughput_per_second\": len(test_data) / total_time,\n",
        "    \"avg_latency_ms\": sum(r.latency_ms for r in successful) / len(successful) if successful else 0,\n",
        "    \"metrics\": {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision_macro\": precision_macro,\n",
        "        \"precision_weighted\": precision_weighted,\n",
        "        \"recall_macro\": recall_macro,\n",
        "        \"recall_weighted\": recall_weighted,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"f1_weighted\": f1_weighted,\n",
        "    },\n",
        "    \"comparison_vs_base\": {\n",
        "        \"accuracy_delta\": accuracy - base_model_metrics[\"accuracy\"],\n",
        "        \"f1_macro_delta\": f1_macro - base_model_metrics[\"f1_macro\"],\n",
        "        \"accuracy_improvement_pct\": accuracy_improvement,\n",
        "        \"f1_improvement_pct\": f1_improvement,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open(\"qlora_fine_tuned_results.json\", \"w\") as f:\n",
        "    json.dump(qlora_results, f, indent=2)\n",
        "\n",
        "print(\"Results saved to 'qlora_fine_tuned_results.json'\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "Model: {BASE_MODEL_NAME}\n",
        "Adapter: {QLORA_MODEL_NAME}\n",
        "\n",
        "PERFORMANCE METRICS:\n",
        "--------------------\n",
        "Accuracy:           {accuracy:.2%}\n",
        "F1 Score (macro):   {f1_macro:.2%}\n",
        "F1 Score (weighted):{f1_weighted:.2%}\n",
        "\n",
        "IMPROVEMENT vs BASE MODEL:\n",
        "--------------------------\n",
        "Accuracy:   {accuracy_improvement:+.1f}%\n",
        "F1 (macro): {f1_improvement:+.1f}%\n",
        "\n",
        "INFERENCE STATS:\n",
        "----------------\n",
        "Test samples:       {len(test_data):,}\n",
        "Total time:         {total_time:.1f}s\n",
        "Throughput:         {len(test_data)/total_time:.1f} articles/sec\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusions",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "### QLoRA Fine-Tuning Results\n",
        "\n",
        "| Metric | Base Model | QLoRA Fine-Tuned | Target | Status |\n",
        "|--------|------------|------------------|--------|--------|\n",
        "| **Accuracy** | 78.76% | **95.14%** | >85% | **Exceeded** |\n",
        "| **F1 (macro)** | 77.97% | **95.13%** | - | **+17.16%** |\n",
        "| **Sci/Tech F1** | 62.06% | **~93.2%** | >75% | **Exceeded** |\n",
        "| **Business Precision** | 63.66% | **~95.8%** | >75% | **Exceeded** |\n",
        "\n",
        "### Performance Improvement Summary\n",
        "\n",
        "| Metric | Absolute Improvement | Relative Improvement |\n",
        "|--------|---------------------|---------------------|\n",
        "| Accuracy | +16.38% | **+20.8%** |\n",
        "| F1 (macro) | +17.16% | **+22.0%** |\n",
        "| Precision (macro) | +13.25% | +16.2% |\n",
        "| Recall (macro) | +16.38% | +20.8% |\n",
        "\n",
        "### Per-Category Performance (from Confusion Matrix)\n",
        "\n",
        "| Category | Correct | Total | Accuracy | Key Observations |\n",
        "|----------|---------|-------|----------|------------------|\n",
        "| **World** | 1,818 | 1,900 | 95.7% | Excellent classification |\n",
        "| **Sports** | 1,894 | 1,900 | **99.7%** | Near-perfect performance |\n",
        "| **Business** | 1,690 | 1,900 | 88.9% | Most challenging category |\n",
        "| **Sci/Tech** | 1,829 | 1,900 | 96.3% | Massive improvement from 46.8% recall |\n",
        "\n",
        "### Key Observations\n",
        "\n",
        "1. **All Targets Exceeded**: QLoRA fine-tuning achieved 95.14% accuracy, far exceeding the 85% target.\n",
        "\n",
        "2. **Sci/Tech Category Transformed**: \n",
        "   - Base model: 62.06% F1 (major weakness with only 46.84% recall)\n",
        "   - QLoRA: ~93.2% F1 with 96.3% recall\n",
        "   - This was the biggest improvement area\n",
        "\n",
        "3. **Business-Sci/Tech Confusion Reduced but Persists**:\n",
        "   - 155 Business articles still misclassified as Sci/Tech\n",
        "   - This represents tech-business overlap (e.g., tech company news)\n",
        "   - Still significantly better than base model\n",
        "\n",
        "4. **Sports Nearly Perfect**: 99.7% accuracy (1,894/1,900) - the clearest category\n",
        "\n",
        "5. **Inference Speed**: \n",
        "   - 33.6 articles/second with vLLM parallel processing\n",
        "   - Total evaluation: 226 seconds (~3.8 minutes) for 7,600 articles\n",
        "   - 100% success rate\n",
        "\n",
        "### QLoRA Training Summary\n",
        "\n",
        "| Aspect | Value |\n",
        "|--------|-------|\n",
        "| Training Time | ~6 hours |\n",
        "| Final Loss | 0.4625 |\n",
        "| Adapter Size | 177.42 MB |\n",
        "| Trainable Parameters | 40.4M (0.53% of model) |\n",
        "| Memory Efficiency | 4-bit quantized base model |\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "QLoRA fine-tuning on Qwen2.5-7B-Instruct achieved **exceptional results** for AG News classification:\n",
        "- **95.14% accuracy** (vs 78.76% base) - a **20.8% relative improvement**\n",
        "- All target metrics exceeded\n",
        "- Training required only **0.53%** of parameters (40.4M vs 7.6B)\n",
        "- Adapter size is only **177 MB** vs ~15 GB for the full model\n",
        "\n",
        "This demonstrates that QLoRA is highly effective for domain-specific fine-tuning, achieving near state-of-the-art performance with minimal computational resources."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
