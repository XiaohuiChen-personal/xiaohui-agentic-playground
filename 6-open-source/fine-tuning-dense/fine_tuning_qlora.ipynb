{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c665f09",
      "metadata": {},
      "source": [
        "# QLoRA Fine-Tuning with Unsloth: Qwen2.5-7B on AG News\n",
        "\n",
        "This notebook demonstrates **QLoRA (Quantized LoRA)** fine-tuning using **Unsloth's FastLanguageModel** with a 4-bit quantized base model.\n",
        "\n",
        "## Overview\n",
        "\n",
        "| Aspect | Details |\n",
        "|--------|---------|\n",
        "| **Model** | unsloth/Qwen2.5-7B-Instruct (4-bit) |\n",
        "| **Method** | QLoRA (4-bit base + LoRA adapters) |\n",
        "| **Framework** | Unsloth + TRL + bitsandbytes |\n",
        "| **Dataset** | AG News (120K train, 7.6K test) |\n",
        "| **Task** | 4-class text classification |\n",
        "| **Expected Time** | ~6-8 hours |\n",
        "| **Memory** | ~8-12 GB |\n",
        "\n",
        "## Base Model Performance (Target to Beat)\n",
        "\n",
        "| Metric | Base Model | Target |\n",
        "|--------|------------|--------|\n",
        "| **Accuracy** | 78.76% | >85% |\n",
        "| **F1 (macro)** | 77.97% | >82% |\n",
        "| **Sci/Tech F1** | 62.06% | >75% |\n",
        "| **Business Precision** | 63.66% | >75% |\n",
        "\n",
        "## QLoRA vs LoRA\n",
        "\n",
        "| Aspect | LoRA (16-bit) | QLoRA (4-bit) |\n",
        "|--------|---------------|---------------|\n",
        "| Model weights | 14 GB | 3.5 GB |\n",
        "| Memory usage | ~25 GB | ~10 GB |\n",
        "| Speed | Faster | Slower (dequantization) |\n",
        "| Quality | Baseline | ~1-2% accuracy loss |\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "```bash\n",
        "./start_docker.sh start finetune\n",
        "# Then open http://localhost:8888\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cabaf7",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8e87ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Environment Verification - QLoRA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "    try:\n",
        "        total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
        "    except:\n",
        "        print(\"GPU Memory: Unified memory system (DGX Spark)\")\n",
        "else:\n",
        "    raise RuntimeError(\"CUDA not available!\")\n",
        "\n",
        "# Check bitsandbytes\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(f\"\\nbitsandbytes version: {bnb.__version__}\")\n",
        "    print(\"✓ 4-bit quantization available\")\n",
        "except ImportError:\n",
        "    raise RuntimeError(\"bitsandbytes not installed!\")\n",
        "\n",
        "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
        "print(f\"Dataset available: {os.path.exists('/fine-tuning-dense/datasets/train.jsonl')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c11b5e6",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6fe3a6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Model Configuration\n",
        "# =============================================================================\n",
        "MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct\"  # Unsloth optimized version\n",
        "MAX_SEQ_LENGTH = 512\n",
        "LOAD_IN_4BIT = True  # QLoRA uses 4-bit quantization\n",
        "\n",
        "# =============================================================================\n",
        "# LoRA Configuration\n",
        "# =============================================================================\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0  # Must be 0 for Unsloth optimization!\n",
        "\n",
        "TARGET_MODULES = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "]\n",
        "\n",
        "# =============================================================================\n",
        "# Training Configuration\n",
        "# =============================================================================\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 1\n",
        "WARMUP_RATIO = 0.03\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# =============================================================================\n",
        "# Output Configuration\n",
        "# =============================================================================\n",
        "OUTPUT_DIR = \"./adapters/qwen7b-ag-news-qlora\"\n",
        "LOGGING_STEPS = 50\n",
        "SAVE_STEPS = 500\n",
        "\n",
        "TRAIN_DATA_PATH = \"/fine-tuning-dense/datasets/train.jsonl\"\n",
        "\n",
        "print(\"QLoRA Configuration loaded!\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  4-bit quantization: {LOAD_IN_4BIT}\")\n",
        "print(f\"  LoRA rank: {LORA_R}, alpha: {LORA_ALPHA}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13f3a60",
      "metadata": {},
      "source": [
        "## 3. Load Model with 4-bit Quantization\n",
        "\n",
        "Using `FastLanguageModel` with `load_in_4bit=True` for QLoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a11988",
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "print(\"Loading model with 4-bit quantization (QLoRA)...\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,  # QLoRA: 4-bit quantized base model\n",
        "    full_finetuning=False,\n",
        ")\n",
        "\n",
        "# Check memory\n",
        "mem_used = torch.cuda.memory_allocated() / 1e9\n",
        "\n",
        "print(f\"\\n✓ Model loaded in 4-bit!\")\n",
        "print(f\"  GPU memory used: {mem_used:.2f} GB\")\n",
        "print(f\"  (vs ~14 GB for BF16 - 60% savings)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b5c6d7",
      "metadata": {},
      "source": [
        "## 4. Apply LoRA with Unsloth Optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5c6d7e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Applying LoRA with Unsloth optimizations...\")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_R,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,  # Must be 0 for Unsloth optimization\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # 30% less VRAM\n",
        "    random_state=42,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ LoRA applied!\")\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d7e8f9",
      "metadata": {},
      "source": [
        "## 5. Load Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e8f9a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(f\"Loading dataset from: {TRAIN_DATA_PATH}\")\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=TRAIN_DATA_PATH, split=\"train\")\n",
        "\n",
        "print(f\"\\nDataset loaded:\")\n",
        "print(f\"  Total examples: {len(dataset):,}\")\n",
        "print(f\"  Columns: {dataset.column_names}\")\n",
        "\n",
        "print(f\"\\nSample entry:\")\n",
        "sample = dataset[0]\n",
        "for msg in sample[\"messages\"]:\n",
        "    role = msg[\"role\"]\n",
        "    content = msg[\"content\"][:80] + \"...\" if len(msg[\"content\"]) > 80 else msg[\"content\"]\n",
        "    print(f\"  [{role}]: {content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8f9a0b1",
      "metadata": {},
      "source": [
        "## 6. Format Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a0b1c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"Format examples using the tokenizer's chat template.\"\"\"\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"Applying chat template to dataset...\")\n",
        "formatted_dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    desc=\"Formatting\",\n",
        ")\n",
        "\n",
        "print(f\"\\nFormatted dataset columns: {formatted_dataset.column_names}\")\n",
        "print(f\"\\nSample (first 400 chars):\")\n",
        "print(formatted_dataset[0][\"text\"][:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0b1c2d3",
      "metadata": {},
      "source": [
        "## 7. Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c2d3e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "total_steps = (len(formatted_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_EPOCHS\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Total examples: {len(formatted_dataset):,}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Estimated total steps: {total_steps:,}\")\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    optim=\"adamw_8bit\",\n",
        "    bf16=True,\n",
        "    fp16=False,\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    packing=True,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    logging_first_step=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=2,\n",
        "    dataloader_num_workers=4,\n",
        "    seed=42,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"\\n✓ SFTConfig created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d3e4f5",
      "metadata": {},
      "source": [
        "## 8. Create Trainer and Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e4f5a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    args=sft_config,\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer created!\")\n",
        "print(f\"\\nStarting QLoRA training...\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4f5a6b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "hours, remainder = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"QLoRA Training Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTraining time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
        "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"Total steps: {trainer_stats.global_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5a6b7c8",
      "metadata": {},
      "source": [
        "## 9. Save the QLoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b7c8d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "adapter_path = f\"{OUTPUT_DIR}/final\"\n",
        "\n",
        "print(f\"Saving QLoRA adapter to: {adapter_path}\")\n",
        "\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "import os\n",
        "saved_files = os.listdir(adapter_path)\n",
        "total_size = sum(os.path.getsize(os.path.join(adapter_path, f)) for f in saved_files)\n",
        "\n",
        "print(f\"\\nSaved files:\")\n",
        "for f in sorted(saved_files):\n",
        "    size = os.path.getsize(os.path.join(adapter_path, f))\n",
        "    print(f\"  {f}: {size / 1e6:.2f} MB\")\n",
        "\n",
        "print(f\"\\nTotal adapter size: {total_size / 1e6:.2f} MB\")\n",
        "print(f\"\\n✓ QLoRA adapter saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c8d9e0",
      "metadata": {},
      "source": [
        "## 10. Quick Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d9e0f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a news article classifier. Categorize into: World, Sports, Business, or Sci/Tech.\n",
        "Respond with JSON: {\"category\": \"<category>\"}\"\"\"\n",
        "\n",
        "test_articles = [\n",
        "    (\"The Federal Reserve announced a quarter-point interest rate cut.\", \"Business\"),\n",
        "    (\"Scientists at CERN discovered a new subatomic particle.\", \"Sci/Tech\"),\n",
        "    (\"The Lakers defeated the Celtics 112-108 in overtime.\", \"Sports\"),\n",
        "    (\"The UN Security Council voted to impose new sanctions.\", \"World\"),\n",
        "]\n",
        "\n",
        "print(\"Testing QLoRA fine-tuned model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "correct = 0\n",
        "for article, expected in test_articles:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"Classify: {article}\"},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True,\n",
        "    ).to(model.device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    is_correct = expected.lower() in response.lower()\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    \n",
        "    print(f\"\\nArticle: {article[:50]}...\")\n",
        "    print(f\"Expected: {expected}\")\n",
        "    print(f\"Response: {response.strip()}\")\n",
        "    print(f\"Status: {'✓' if is_correct else '✗'}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"Quick test accuracy: {correct}/{len(test_articles)} ({100*correct/len(test_articles):.0f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e0f1a2",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "*To be filled after running the notebook*\n",
        "\n",
        "### Training Results\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Training Time | TBD |\n",
        "| Final Loss | TBD |\n",
        "| Total Steps | TBD |\n",
        "| Adapter Size | TBD |\n",
        "| GPU Memory | TBD |\n",
        "\n",
        "### Performance Comparison\n",
        "\n",
        "| Metric | HuggingFace QLoRA | Unsloth QLoRA |\n",
        "|--------|-------------------|----------------|\n",
        "| Tokens/sec | ~527 | TBD |\n",
        "| Training time | ~18h | TBD |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
