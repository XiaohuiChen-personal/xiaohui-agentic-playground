{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Fine-Tuning: Qwen2.5-7B on AG News\n",
    "\n",
    "This notebook performs **full fine-tuning** (updating all 7B parameters) of the Qwen2.5-7B-Instruct model on the AG News classification dataset.\n",
    "\n",
    "## What is Full Fine-Tuning?\n",
    "\n",
    "Unlike LoRA/QLoRA which only trains small adapter layers, **full fine-tuning** updates ALL model parameters:\n",
    "\n",
    "| Aspect | Full Fine-Tuning | LoRA |\n",
    "|--------|------------------|------|\n",
    "| Parameters updated | 7,000,000,000 (100%) | ~70,000,000 (1%) |\n",
    "| Memory required | ~60-70 GB | ~16-24 GB |\n",
    "| Training time | 8-15 hours | 2-4 hours |\n",
    "| Output size | ~14 GB | ~100-500 MB |\n",
    "| Risk of overfitting | Higher | Lower |\n",
    "| Potential improvement | Higher | Moderate |\n",
    "\n",
    "## Environment\n",
    "\n",
    "**This notebook is designed to run inside the NVIDIA PyTorch Docker container** (`nvcr.io/nvidia/pytorch:25.11-py3`) which provides:\n",
    "- Native sm_120/121 CUDA kernels (optimized for Blackwell GB10)\n",
    "- Transformer Engine 2.9+ with FP8 support\n",
    "- Flash Attention 2 compiled for Blackwell\n",
    "- Triton with optimized kernels\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. **Start the training container**:\n",
    "   ```bash\n",
    "   cd ~/Projects/xiaohui-agentic-playground/6-open-source\n",
    "   ./start_docker.sh start finetune\n",
    "   ```\n",
    "\n",
    "2. **Open Jupyter** at http://localhost:8888\n",
    "\n",
    "3. **Training data prepared** at `datasets/train.jsonl` (120K samples)\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- **Base model accuracy**: 78.63%\n",
    "- **Target accuracy**: 85-92% (with fine-tuning)\n",
    "- **Training time**: ~7-9 hours for 1 epoch (with optimizations on DGX Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Pre-flight Checks\n",
    "\n",
    "First, let's verify that:\n",
    "- GPU is available and has sufficient memory\n",
    "- Required libraries are installed\n",
    "- Training data exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-flight checks\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZATION 1: Enable cuDNN benchmark mode for consistent input sizes\n",
    "# =============================================================================\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"✓ cuDNN benchmark mode enabled (optimization for consistent input sizes)\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-FLIGHT CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check Docker environment\n",
    "print(f\"\\n[1] Docker Environment:\")\n",
    "if os.path.exists(\"/usr/local/cuda-13.0\") or os.path.exists(\"/usr/local/cuda\"):\n",
    "    print(\"    ✓ Running in NVIDIA PyTorch container\")\n",
    "    # Check for Transformer Engine\n",
    "    try:\n",
    "        import transformer_engine as te\n",
    "        print(f\"    ✓ Transformer Engine: {te.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"    ⚠ Transformer Engine not found (optional)\")\n",
    "    # Check for Triton\n",
    "    try:\n",
    "        import triton\n",
    "        print(f\"    ✓ Triton: {triton.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"    ⚠ Triton not found\")\n",
    "else:\n",
    "    print(\"    ⚠ Not in NVIDIA container - may have suboptimal performance\")\n",
    "    print(\"    ⚠ Start with: ./start_docker.sh start finetune\")\n",
    "\n",
    "# Check GPU\n",
    "print(f\"\\n[2] GPU Availability:\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    compute_cap = torch.cuda.get_device_capability(0)\n",
    "    print(f\"    ✓ CUDA available: {gpu_name}\")\n",
    "    print(f\"    ✓ Compute capability: sm_{compute_cap[0]}{compute_cap[1]}\")\n",
    "    print(f\"    ✓ Total memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Check current memory usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"    ✓ Currently allocated: {allocated:.1f} GB\")\n",
    "    print(f\"    ✓ Currently reserved: {reserved:.1f} GB\")\n",
    "else:\n",
    "    print(\"    ✗ CUDA NOT available - cannot proceed!\")\n",
    "\n",
    "# Check training data\n",
    "print(f\"\\n[3] Training Data:\")\n",
    "train_file = Path(\"datasets/train.jsonl\")\n",
    "if train_file.exists():\n",
    "    size_mb = train_file.stat().st_size / 1e6\n",
    "    with open(train_file) as f:\n",
    "        num_lines = sum(1 for _ in f)\n",
    "    print(f\"    ✓ Found: {train_file}\")\n",
    "    print(f\"    ✓ Size: {size_mb:.1f} MB\")\n",
    "    print(f\"    ✓ Examples: {num_lines:,}\")\n",
    "else:\n",
    "    print(f\"    ✗ Training data not found at {train_file}\")\n",
    "\n",
    "# Check output directory\n",
    "print(f\"\\n[4] Output Directory:\")\n",
    "output_dir = Path(\"checkpoints/qwen7b-ag-news-full\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"    ✓ Will save to: {output_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PRE-FLIGHT CHECKS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Library versions - Docker container already has these installed\n",
    "import transformers\n",
    "import trl\n",
    "import accelerate\n",
    "\n",
    "print(\"Library versions:\")\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  trl: {trl.__version__}\")\n",
    "print(f\"  accelerate: {accelerate.__version__}\")\n",
    "\n",
    "# Check for Transformer Engine (optional, for FP8)\n",
    "try:\n",
    "    import transformer_engine as te\n",
    "    print(f\"  transformer_engine: {te.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"  transformer_engine: not installed\")\n",
    "\n",
    "# Check CUDA version\n",
    "print(f\"\\nCUDA info:\")\n",
    "print(f\"  CUDA version (torch): {torch.version.cuda}\")\n",
    "print(f\"  cuDNN version: {torch.backends.cudnn.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define all hyperparameters and settings for training.\n",
    "\n",
    "### Key Hyperparameters Explained:\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|-----------|-------|-------------|\n",
    "| `learning_rate` | 2e-5 | Lower than LoRA (1e-4) because we're updating all params |\n",
    "| `num_train_epochs` | 1 | Full fine-tuning often needs fewer epochs |\n",
    "| `per_device_train_batch_size` | 8 | Larger batch = better GPU utilization |\n",
    "| `gradient_accumulation_steps` | 4 | Effective batch size = 8 × 4 = 32 |\n",
    "| `gradient_checkpointing` | True | Trades compute for memory (essential!) |\n",
    "| `bf16` | True | BFloat16 precision for speed + stability |\n",
    "| `packing` | True | Pack multiple short sequences into one batch (big speedup!) |\n",
    "\n",
    "### Performance Optimizations Applied\n",
    "\n",
    "1. **cuDNN benchmark mode** - Auto-tunes kernels for consistent 512-token inputs\n",
    "2. **Sequence packing** - Avg sequence ~120 tokens, packing eliminates ~75% wasted compute\n",
    "3. **Flash Attention 2** - Memory-efficient attention optimized for Blackwell\n",
    "4. **Larger batch size** - Better GPU utilization, fewer sync points\n",
    "5. **DataLoader workers** - Parallel CPU preprocessing\n",
    "\n",
    "### Expected Training Time\n",
    "\n",
    "On DGX Spark with NVIDIA Docker container + optimizations: **~7-9 hours** for 1 epoch on 120K examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "OUTPUT_DIR = \"checkpoints/qwen7b-ag-news-full\"\n",
    "\n",
    "# Data\n",
    "TRAIN_FILE = \"datasets/train.jsonl\"\n",
    "MAX_SEQ_LENGTH = 512  # Our data is short (~120 tokens avg)\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 8  # Per device (OPTIMIZATION 4: increased from 4 for better GPU utilization)\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch = 32 (reduced from 8, fewer sync points)\n",
    "WARMUP_RATIO = 0.03\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Checkpointing\n",
    "SAVE_STEPS = 500\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_TOTAL_LIMIT = 2  # Keep only 2 checkpoints to save disk space\n",
    "\n",
    "# Memory optimization\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "USE_BF16 = True\n",
    "\n",
    "# Performance optimizations\n",
    "ENABLE_TORCH_COMPILE = False  # Set True for potential 10-20% speedup (experimental)\n",
    "USE_PACKING = True  # OPTIMIZATION 2: Pack multiple short sequences into one batch\n",
    "DATALOADER_NUM_WORKERS = 4  # OPTIMIZATION 5: Parallel data loading\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} × {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} effective\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  Sequence packing: {USE_PACKING}\")\n",
    "print(f\"  DataLoader workers: {DATALOADER_NUM_WORKERS}\")\n",
    "print(f\"  torch.compile: {ENABLE_TORCH_COMPILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset\n",
    "\n",
    "Load the prepared training data and format it for the SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading training dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=TRAIN_FILE, split=\"train\")\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Examples: {len(dataset):,}\")\n",
    "print(f\"  Features: {dataset.features}\")\n",
    "\n",
    "# Show a sample\n",
    "print(f\"\\nSample example:\")\n",
    "sample = dataset[0]\n",
    "for msg in sample[\"messages\"]:\n",
    "    role = msg[\"role\"]\n",
    "    content = msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"]\n",
    "    print(f\"  [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Base Model\n",
    "\n",
    "Load the Qwen2.5-7B-Instruct model in BF16 precision.\n",
    "\n",
    "**Important**: We enable `gradient_checkpointing` to reduce memory usage. This trades compute for memory by recomputing activations during backward pass instead of storing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"  Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "\n",
    "print(f\"\\nTokenizer loaded:\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  Model max length: {tokenizer.model_max_length:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model (this may take a few minutes)...\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Precision: BF16\")\n",
    "print(f\"  Gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "\n",
    "# Determine best attention implementation\n",
    "# Docker container has Flash Attention 2 compiled for Blackwell\n",
    "try:\n",
    "    import flash_attn\n",
    "    attn_impl = \"flash_attention_2\"\n",
    "    print(f\"  Attention: Flash Attention 2 (optimized for Blackwell)\")\n",
    "except ImportError:\n",
    "    attn_impl = \"sdpa\"\n",
    "    print(f\"  Attention: SDPA (Flash Attention 2 not available)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=attn_impl,\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "if USE_GRADIENT_CHECKPOINTING:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"  ✓ Gradient checkpointing enabled\")\n",
    "\n",
    "# Optional: torch.compile for potential speedup\n",
    "if ENABLE_TORCH_COMPILE:\n",
    "    print(\"  Compiling model with torch.compile (this may take a few minutes)...\")\n",
    "    model = torch.compile(model)\n",
    "    print(\"  ✓ Model compiled\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel loaded:\")\n",
    "print(f\"  Total parameters: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/1e9:.2f}B)\")\n",
    "print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    print(f\"\\nGPU memory after model load: {allocated:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration Preview\n",
    "\n",
    "Preview the training configuration. The actual configuration will be set via `SFTConfig` in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate expected training steps for reference\n",
    "# Note: We'll use SFTConfig in the next cell which combines TrainingArguments + SFT options\n",
    "num_training_steps = (len(dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_EPOCHS\n",
    "warmup_steps = int(0.03 * num_training_steps)  # 3% warmup\n",
    "\n",
    "print(\"Training Configuration Preview:\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} × {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} effective\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Total training steps: ~{num_training_steps:,}\")\n",
    "print(f\"  Checkpoint every: {SAVE_STEPS} steps\")\n",
    "print(f\"\\nOptimizations enabled:\")\n",
    "print(f\"  1. cuDNN benchmark mode: ✓\")\n",
    "print(f\"  2. Sequence packing: {USE_PACKING}\")\n",
    "print(f\"  3. Flash Attention 2: ✓ (set in model loading)\")\n",
    "print(f\"  4. Batch size 8 / Accum 4: ✓\")\n",
    "print(f\"  5. DataLoader workers: {DATALOADER_NUM_WORKERS}\")\n",
    "print(f\"\\n  Expected training time: ~7-9 hours (with optimizations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize SFTTrainer\n",
    "\n",
    "We use TRL's `SFTTrainer` (Supervised Fine-Tuning Trainer) with `SFTConfig` which handles:\n",
    "- Chat template formatting\n",
    "- **Assistant-only loss**: Only compute loss on assistant responses (via `assistant_only_loss=True`)\n",
    "- Efficient data collation\n",
    "- All training hyperparameters in one config object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# In TRL 0.27+, use SFTConfig with assistant_only_loss instead of DataCollatorForCompletionOnlyLM\n",
    "# This automatically masks the loss so we only train on assistant responses\n",
    "\n",
    "print(\"Initializing SFTTrainer with performance optimizations...\")\n",
    "\n",
    "# Create SFTConfig with all training parameters\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_steps=int(0.03 * (len(dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS))),  # 3% warmup\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Precision\n",
    "    bf16=USE_BF16,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "    \n",
    "    # Note: assistant_only_loss=True requires tokenizer chat template with {% generation %} keyword\n",
    "    # Qwen2.5's template doesn't support this, so we train on full sequences\n",
    "    # This is acceptable since our assistant responses are very short (just category JSON)\n",
    "    assistant_only_loss=False,\n",
    "    \n",
    "    # Sequence length (max_length in TRL 0.27+)\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    \n",
    "    # =============================================================================\n",
    "    # PERFORMANCE OPTIMIZATIONS\n",
    "    # =============================================================================\n",
    "    # OPTIMIZATION 2: Sequence packing - pack multiple short sequences into one batch\n",
    "    # Our avg sequence is ~120 tokens, max is 512. Packing eliminates ~75% wasted compute!\n",
    "    packing=USE_PACKING,\n",
    "    \n",
    "    # OPTIMIZATION 5: Parallel data loading\n",
    "    dataloader_num_workers=DATALOADER_NUM_WORKERS,\n",
    "    \n",
    "    # Other\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ SFTTrainer initialized with optimizations:\")\n",
    "print(f\"  Training examples: {len(trainer.train_dataset):,}\")\n",
    "print(f\"  Loss computed on: {'assistant only' if sft_config.assistant_only_loss else 'full sequence'}\")\n",
    "print(f\"  Sequence packing: {sft_config.packing}\")\n",
    "print(f\"  DataLoader workers: {sft_config.dataloader_num_workers}\")\n",
    "\n",
    "# Final memory check\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    print(f\"  GPU memory before training: {allocated:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train!\n",
    "\n",
    "Now we start the training process. With optimizations enabled, this should take **~7-9 hours** for 1 epoch on 120K examples on DGX Spark.\n",
    "\n",
    "**Optimizations Applied:**\n",
    "1. cuDNN benchmark mode (5-10% speedup)\n",
    "2. Sequence packing (30-40% speedup - biggest impact!)\n",
    "3. Flash Attention 2 (10-15% speedup)\n",
    "4. Larger batch size with less gradient accumulation (5-8% speedup)\n",
    "5. Parallel data loading (3-5% speedup)\n",
    "\n",
    "**What to monitor:**\n",
    "- `loss` should decrease over time\n",
    "- GPU memory usage (watch for OOM errors)\n",
    "- Training speed (it/s) - expect ~0.12-0.14 it/s with optimizations\n",
    "\n",
    "**Checkpoints are saved automatically** every 500 steps, so if training is interrupted, you can resume from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING (WITH OPTIMIZATIONS)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {len(dataset):,} examples\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch: {BATCH_SIZE} × {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS} effective\")\n",
    "print(f\"Packing: {USE_PACKING}\")\n",
    "print(f\"Expected time: ~7-9 hours (with optimizations)\")\n",
    "print(\"\\nCheckpoints will be saved to:\", OUTPUT_DIR)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = (end_time - start_time) / 3600  # Convert to hours\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal training time: {training_time:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Fine-Tuned Model\n",
    "\n",
    "Save the complete fine-tuned model (all weights, ~14 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving fine-tuned model...\")\n",
    "\n",
    "# Save the model\n",
    "final_model_path = f\"{OUTPUT_DIR}/final\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"\\n✓ Model saved to: {final_model_path}\")\n",
    "\n",
    "# Check size\n",
    "import subprocess\n",
    "result = subprocess.run([\"du\", \"-sh\", final_model_path], capture_output=True, text=True)\n",
    "print(f\"✓ Model size: {result.stdout.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Evaluation\n",
    "\n",
    "Let's do a quick sanity check by classifying a few examples with the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUICK EVALUATION\n",
      "============================================================\n",
      "\n",
      "Article: President Biden announces new climate policy at UN summit...\n",
      "Expected: World\n",
      "Predicted: {\"category\":\"World\"}\n",
      "----------------------------------------\n",
      "\n",
      "Article: Lakers defeat Celtics 115-108 in overtime thriller...\n",
      "Expected: Sports\n",
      "Predicted: {\"category\":\"Sports\"}\n",
      "----------------------------------------\n",
      "\n",
      "Article: Apple stock rises 5% after strong quarterly earnings report...\n",
      "Expected: Business\n",
      "Predicted: {\"category\":\"Sci/Tech\"}\n",
      "----------------------------------------\n",
      "\n",
      "Article: Google releases new AI model that can generate realistic ima...\n",
      "Expected: Sci/Tech\n",
      "Predicted: {\"category\":\"Sci/Tech\"}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Quick evaluation on a few examples\n",
    "print(\"=\" * 60)\n",
    "print(\"QUICK EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test examples (one from each category)\n",
    "test_examples = [\n",
    "    {\"text\": \"President Biden announces new climate policy at UN summit\", \"expected\": \"World\"},\n",
    "    {\"text\": \"Lakers defeat Celtics 115-108 in overtime thriller\", \"expected\": \"Sports\"},\n",
    "    {\"text\": \"Apple stock rises 5% after strong quarterly earnings report\", \"expected\": \"Business\"},\n",
    "    {\"text\": \"Google releases new AI model that can generate realistic images\", \"expected\": \"Sci/Tech\"},\n",
    "]\n",
    "\n",
    "# System prompt (same as training)\n",
    "system_prompt = \"\"\"You are a news article classifier. Your task is to categorize news articles into exactly one of four categories:\n",
    "\n",
    "- World: News about politics, government, elections, diplomacy, conflicts, and public affairs (domestic or international)\n",
    "- Sports: News about athletic events, games, players, teams, coaches, tournaments, and championships\n",
    "- Business: News about companies, markets, finance, economy, trade, corporate activities, and business services\n",
    "- Sci/Tech: News about technology products, software, hardware, scientific research, gadgets, and tech innovations\n",
    "\n",
    "Rules:\n",
    "- Focus on the PRIMARY topic of the article\n",
    "- Ignore HTML artifacts (like #39; or &lt;b&gt;) - they are formatting errors\n",
    "- If an article is truncated, classify based on the available content\n",
    "- When a topic spans multiple categories, choose the one that best represents the main focus\"\"\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for example in test_examples:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the following news article:\\n\\n{example['text']}\"}\n",
    "    ]\n",
    "    \n",
    "    # Note: return_dict=False returns just the tensor, not a BatchEncoding object\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=False,\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.0,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # inputs is now a tensor, so .shape works correctly\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nArticle: {example['text'][:60]}...\")\n",
    "    print(f\"Expected: {example['expected']}\")\n",
    "    print(f\"Predicted: {response.strip()}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Now that training is complete:\n",
    "\n",
    "### 1. Full Evaluation\n",
    "Run the fine-tuned model through the same evaluation as the base model:\n",
    "- Use `base_model_performance.ipynb` as a template\n",
    "- Load the fine-tuned model from `checkpoints/qwen7b-ag-news-full/final`\n",
    "- Compare accuracy, F1, confusion matrix\n",
    "\n",
    "### 2. Serve with vLLM\n",
    "To serve the fine-tuned model:\n",
    "```bash\n",
    "# Update docker-compose-qwen7b.yml to point to fine-tuned model:\n",
    "# command: vllm serve /checkpoints/qwen7b-ag-news-full/final ...\n",
    "\n",
    "# Then start the server:\n",
    "./start_docker.sh start qwen7b\n",
    "```\n",
    "\n",
    "### 3. Compare Results\n",
    "\n",
    "| Metric | Base Model | Fine-Tuned | Improvement |\n",
    "|--------|------------|------------|-------------|\n",
    "| Accuracy | 78.63% | TBD | TBD |\n",
    "| F1 (macro) | 77.80% | TBD | TBD |\n",
    "| Sci/Tech Recall | 46.37% | TBD | TBD |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Model:              Qwen/Qwen2.5-7B-Instruct\n",
      "Dataset:            120,000 examples\n",
      "Training time:      9.72 hours\n",
      "Output directory:   checkpoints/qwen7b-ag-news-full\n",
      "\n",
      "Next steps:\n",
      "1. Run full evaluation on test set\n",
      "2. Compare with base model results\n",
      "3. Serve with vLLM if results are good\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Model:              {MODEL_NAME}\n",
    "Dataset:            {len(dataset):,} examples\n",
    "Training time:      {training_time:.2f} hours\n",
    "Output directory:   {OUTPUT_DIR}\n",
    "\n",
    "Next steps:\n",
    "1. Run full evaluation on test set\n",
    "2. Compare with base model results\n",
    "3. Serve with vLLM if results are good\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
