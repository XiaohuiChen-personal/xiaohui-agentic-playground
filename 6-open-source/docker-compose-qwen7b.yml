# Qwen2.5-7B Configuration for DGX Spark
# Dense 7B model for fine-tuning practice and inference
#
# Model: unsloth/Qwen2.5-7B-Instruct (optimized version)
# Architecture: Dense Transformer (7B parameters)
# Context: 128K tokens (can reduce for faster inference)
#
# Use Cases:
# - Pre/post fine-tuning inference comparison
# - Learning fine-tuning with full parameter updates
# - Fast inference baseline
#
# Memory Usage:
# - Model weights: ~14 GB (BF16)
# - KV cache: Scales with context length
# - Total: ~20-30 GB with 32K context
#
# Performance Expectations:
# - TPS: 25-40 tokens/second (dense 7B is fast)
# - TTFT: ~0.1s

services:
  qwen7b:
    image: nvcr.io/nvidia/vllm:25.12-py3
    container_name: vllm-qwen7b
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_HOME=/root/.cache/huggingface
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000"
    command: >
      vllm serve unsloth/Qwen2.5-7B-Instruct
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization 0.85
      --max-num-seqs 64
      --max-model-len 32768
      --dtype auto
      --trust-remote-code
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
