# Qwen2.5-7B Configuration for DGX Spark
# Dense 7B model for fine-tuning practice and inference
#
# Model: unsloth/Qwen2.5-7B-Instruct (optimized version)
# Architecture: Dense Transformer (7B parameters)
# Context: 128K tokens (can reduce for faster inference)
#
# Use Cases:
# - Pre/post fine-tuning inference comparison
# - Learning fine-tuning with full parameter updates
# - Fast inference baseline
# - LoRA/QLoRA adapter inference (with --enable-lora)
#
# Memory Usage:
# - Model weights: ~14 GB (BF16)
# - KV cache: Scales with context length
# - Total: ~20-30 GB with 32K context
#
# Performance Expectations:
# - TPS: 25-40 tokens/second (dense 7B is fast)
# - TTFT: ~0.1s
#
# LoRA/Adapter Support:
# - Set ENABLE_LORA=true to enable adapter support
# - Set LORA_ADAPTER to specify which adapter(s) to load:
#   - "qlora" - Load QLoRA adapter only (qlora-ag-news)
#   - "lora"  - Load LoRA adapter only (lora-ag-news)
#   - "both"  - Load both adapters
# - Adapters are mounted at /adapters inside the container
# - Use model="adapter-name" in API calls to use adapters

services:
  qwen7b:
    image: nvcr.io/nvidia/vllm:25.12-py3
    container_name: vllm-qwen7b
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_HOME=/root/.cache/huggingface
      - ENABLE_LORA=${ENABLE_LORA:-false}
      - LORA_ADAPTER=${LORA_ADAPTER:-qlora}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./fine-tuning-dense/adapters:/adapters
    ports:
      - "8000:8000"
    command: >
      bash -c '
      if [ "$$ENABLE_LORA" = "true" ]; then
        # Build lora-modules argument based on LORA_ADAPTER setting
        LORA_MODULES=""
        if [ "$$LORA_ADAPTER" = "qlora" ]; then
          echo "Starting vLLM with QLoRA adapter...";
          LORA_MODULES="qlora-ag-news=/adapters/qwen7b-ag-news-qlora/final";
        elif [ "$$LORA_ADAPTER" = "lora" ]; then
          echo "Starting vLLM with LoRA adapter...";
          LORA_MODULES="lora-ag-news=/adapters/qwen7b-ag-news-lora/final";
        elif [ "$$LORA_ADAPTER" = "both" ]; then
          echo "Starting vLLM with both LoRA and QLoRA adapters...";
          LORA_MODULES="qlora-ag-news=/adapters/qwen7b-ag-news-qlora/final,lora-ag-news=/adapters/qwen7b-ag-news-lora/final";
        else
          echo "Unknown LORA_ADAPTER: $$LORA_ADAPTER, defaulting to QLoRA";
          LORA_MODULES="qlora-ag-news=/adapters/qwen7b-ag-news-qlora/final";
        fi
        vllm serve unsloth/Qwen2.5-7B-Instruct \
          --host 0.0.0.0 \
          --port 8000 \
          --gpu-memory-utilization 0.85 \
          --max-num-seqs 64 \
          --max-model-len 32768 \
          --dtype auto \
          --trust-remote-code \
          --enable-lora \
          --lora-modules $$LORA_MODULES \
          --max-lora-rank 64;
      else
        echo "Starting vLLM (base model only)...";
        vllm serve unsloth/Qwen2.5-7B-Instruct \
          --host 0.0.0.0 \
          --port 8000 \
          --gpu-memory-utilization 0.85 \
          --max-num-seqs 64 \
          --max-model-len 32768 \
          --dtype auto \
          --trust-remote-code;
      fi
      '
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
