# =============================================================================
# Docker Compose for PEFT (LoRA/QLoRA) Fine-Tuning with Unsloth on DGX Spark
# =============================================================================
# Uses NVIDIA's PyTorch container with Unsloth for 2x faster LoRA/QLoRA training
# with 60% less memory on Blackwell GB10 (sm_121).
#
# Supported Methods:
# - LoRA: Low-Rank Adaptation (16-bit base model)
# - QLoRA: Quantized LoRA (4-bit base model, even less memory)
#
# Features:
# - Unsloth: 2x faster training, 60% less memory
# - Native sm_120/121 CUDA kernels (no fallback mode)
# - Transformer Engine 2.9+ with NVFP4/FP8 support
# - Flash Attention 2 compiled for Blackwell
# - 4-bit quantization via bitsandbytes
# - Jupyter Lab for interactive development
#
# Usage:
#   ./start_docker.sh start peft          # Start container with Jupyter
#   ./start_docker.sh stop                # Stop container
#   ./start_docker.sh logs peft           # View logs
#   ./start_docker.sh status              # Check status
#
# Access Jupyter at: http://localhost:8889 (note: different port from full finetune)
# =============================================================================

services:
  peft:
    container_name: pytorch-peft
    image: nvcr.io/nvidia/pytorch:25.11-py3
    
    # GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Network - use port 8889 to allow running alongside full finetune container
    ports:
      - "8889:8888"    # Jupyter Lab (mapped to 8889 on host)
      - "6007:6006"    # TensorBoard (mapped to 6007 on host)
    
    # Volumes
    volumes:
      # Mount the workspace
      - ../:/workspace
      # Mount HuggingFace cache for model weights
      - ~/.cache/huggingface:/root/.cache/huggingface
      # Mount the PEFT fine-tuning directory
      - ./fine-tuning-peft:/fine-tuning
      # Also mount dense fine-tuning for access to datasets
      - ./fine-tuning-dense:/fine-tuning-dense
    
    # Working directory
    working_dir: /fine-tuning
    
    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      # Optimize for DGX Spark unified memory
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      # Enable TF32 for better performance
      - NVIDIA_TF32_OVERRIDE=1
      # Disable tokenizers parallelism warning
      - TOKENIZERS_PARALLELISM=false
      # Unsloth optimizations
      - UNSLOTH_RETURN_LOGITS=1
    
    # Resource limits
    shm_size: '32gb'
    ulimits:
      memlock: -1
      stack: 67108864
    
    # Keep container running and start Jupyter
    command: >
      bash -c "
        echo '============================================'
        echo '  PEFT (LoRA/QLoRA) Fine-Tuning with Unsloth'
        echo '  NVIDIA PyTorch Container'
        echo '============================================'
        echo ''
        echo 'Container: nvcr.io/nvidia/pytorch:25.11-py3'
        echo 'GPU: '$(nvidia-smi --query-gpu=name --format=csv,noheader)
        echo 'CUDA: '$(nvcc --version | grep release | awk '{print $6}')
        echo ''
        echo 'Installing Unsloth and dependencies...'
        echo '(This may take a few minutes on first run)'
        echo ''
        
        # Install unsloth_zoo first (required by unsloth)
        pip install -q --no-cache-dir 'unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git'
        
        # Install Unsloth from git (includes all optimized kernels)
        pip install -q --no-cache-dir 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'
        
        # Install additional packages
        pip install -q trl>=0.12.0 peft>=0.13.0 datasets>=3.4.0 scikit-learn matplotlib bitsandbytes>=0.44.0
        
        echo ''
        echo 'Installation complete!'
        echo ''
        echo 'Supported Methods:'
        echo '  - LoRA:  load_in_4bit=False (16-bit base model)'
        echo '  - QLoRA: load_in_4bit=True  (4-bit quantized, less memory)'
        echo ''
        echo 'Unsloth Features:'
        echo '  - 2x faster LoRA/QLoRA training'
        echo '  - 60% less memory usage'
        echo '  - Gradient checkpointing optimizations'
        echo ''
        echo 'Starting Jupyter Lab...'
        echo 'Access at: http://localhost:8889'
        echo ''
        jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password=''
      "
    
    # Restart policy
    restart: unless-stopped
    
    # Labels for identification
    labels:
      - "com.nvidia.dgx-spark.purpose=peft-fine-tuning"
      - "com.nvidia.dgx-spark.model=qwen2.5-7b"
      - "com.nvidia.dgx-spark.framework=unsloth"
