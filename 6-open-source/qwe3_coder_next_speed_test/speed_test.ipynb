{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f00ed8",
   "metadata": {},
   "source": [
    "# Qwen3-Coder-Next-FP8 Performance Speed Test\n",
    "\n",
    "This notebook benchmarks the performance of the locally running **Qwen3-Coder-Next-FP8** model via vLLM on port 8000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20792512",
   "metadata": {},
   "source": [
    "This notebook is created by vibe coding using **Qwen/Qwen3-Coder-Next-FP8** in **Cursor**.\n",
    "\n",
    "Testing the locally running model via vLLM at `http://localhost:8000`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed18104",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Health Check\n",
    "\n",
    "This section verifies that the vLLM API is accessible and confirms the model is available. We'll:\n",
    "- Check vLLM API connectivity\n",
    "- List available models\n",
    "- Verify Qwen3-Coder-Next-FP8 is loaded\n",
    "- Capture system information for context\n",
    "- Validate API key authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb861c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get the project root directory (where .env file is located)\n",
    "# The .env file is in the project root, not necessarily the current working directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "env_path = os.path.join(project_root, '.env')\n",
    "\n",
    "print(f\"üîç Looking for .env file at: {project_root}\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# load_dotenv with dotenv_path parameter explicitly points to the .env file\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "# vLLM configuration\n",
    "VLLM_API_KEY = os.getenv('VLLM_API_KEY')\n",
    "VLLM_BASE_URL = \"http://localhost:8000\"\n",
    "VLLM_OPENAI_COMPATIBLE_URL = f\"{VLLM_BASE_URL}/v1\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-Coder-Next-FP8\"\n",
    "\n",
    "# Validation - check if API key was loaded (without printing it)\n",
    "if not VLLM_API_KEY:\n",
    "    print(\"\\n‚ùå VLLM_API_KEY not found in environment variables!\")\n",
    "    print(\"   Please ensure .env file exists at:\", env_path)\n",
    "    print(\"   And contains: VLLM_API_KEY=<your-api-key>\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Print success without exposing any API keys\n",
    "print(f\"‚úÖ Environment loaded successfully\")\n",
    "print(f\"   vLLM Base URL: {VLLM_BASE_URL}\")\n",
    "print(f\"   OpenAI Compatible URL: {VLLM_OPENAI_COMPATIBLE_URL}\")\n",
    "print(f\"   Model Name: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import cpuinfo\n",
    "import psutil\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "# API headers with authorization\n",
    "API_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {VLLM_API_KEY}\"\n",
    "}\n",
    "\n",
    "def check_api_connectivity():\n",
    "    \"\"\"Check if vLLM API is reachable.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. Checking API Connectivity...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Test basic connectivity with a GET request (vLLM health endpoint only supports GET)\n",
    "        response = requests.get(f\"{VLLM_BASE_URL}/health\", timeout=5)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ API is reachable!\")\n",
    "            print(f\"   Response status: {response.status_code}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  API returned status {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to vLLM API!\")\n",
    "        print(f\"   Make sure vLLM is running at {VLLM_BASE_URL}\")\n",
    "        print(\"   Run: vllm serve Qwen/Qwen3-Coder-Next-FP8 --port 8000 --api-key <your-api-key>\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"‚ùå Connection timed out!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_vllm_info():\n",
    "    \"\"\"Get vLLM server information.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. Getting vLLM Server Information...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{VLLM_BASE_URL}/version\", timeout=5)\n",
    "        response.raise_for_status()\n",
    "        version_info = response.json()\n",
    "        \n",
    "        print(\"‚úÖ vLLM Version Info:\")\n",
    "        print(f\"   {json.dumps(version_info, indent=2)}\")\n",
    "        return version_info\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"‚ùå Failed to get version: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to get version: {e}\")\n",
    "        return None\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"List available models on the vLLM server.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. Listing Available Models...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{VLLM_OPENAI_COMPATIBLE_URL}/models\", headers=API_HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        models_info = response.json()\n",
    "        \n",
    "        if \"data\" in models_info:\n",
    "            print(\"‚úÖ Available Models:\")\n",
    "            for model in models_info[\"data\"]:\n",
    "                model_id = model.get(\"id\", \"unknown\")\n",
    "                print(f\"   - {model_id}\")\n",
    "            return models_info[\"data\"]\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No models found in response\")\n",
    "            return []\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"‚ùå Failed to list models: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"   Response: {e.response.text}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to list models: {e}\")\n",
    "        return None\n",
    "\n",
    "def validate_api_key():\n",
    "    \"\"\"Validate that the API key works correctly.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. Validating API Key...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Test with a simple health check that requires auth\n",
    "        response = requests.get(f\"{VLLM_OPENAI_COMPATIBLE_URL}/models\", headers=API_HEADERS, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ API key is valid!\")\n",
    "            print(f\"   Response status: {response.status_code}\")\n",
    "            return True\n",
    "        elif response.status_code == 401:\n",
    "            print(\"‚ùå API key is invalid or expired!\")\n",
    "            print(f\"   Response status: {response.status_code}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Unexpected response: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Capture system information for context.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. System Information...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # CPU info\n",
    "        cpu_info = cpuinfo.get_cpu_info()\n",
    "        print(\"CPU:\")\n",
    "        print(f\"   Brand: {cpu_info.get('brand_raw', 'N/A')}\")\n",
    "        print(f\"   Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
    "        print(f\"   Load: {psutil.cpu_percent(interval=0.5)}%\")\n",
    "        \n",
    "        # Memory info\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(\"\\nMemory:\")\n",
    "        print(f\"   Total: {memory.total / (1024**3):.2f} GB\")\n",
    "        print(f\"   Available: {memory.available / (1024**3):.2f} GB\")\n",
    "        print(f\"   Used: {memory.percent}%\")\n",
    "        \n",
    "        # GPU info (if available)\n",
    "        print(\"\\nGPU:\")\n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used,temperature.gpu,power.draw', '--format=csv,noheader,nounits'], \n",
    "                                  capture_output=True, text=True, timeout=5)\n",
    "            gpu_info = result.stdout.strip().split(', ')\n",
    "            if len(gpu_info) >= 5:\n",
    "                print(f\"   Name: {gpu_info[0]}\")\n",
    "                print(f\"   Total Memory: {gpu_info[1]} MB\")\n",
    "                print(f\"   Used Memory: {gpu_info[2]} MB\")\n",
    "                print(f\"   Temperature: {gpu_info[3]}¬∞C\")\n",
    "                print(f\"   Power Draw: {gpu_info[4]} W\")\n",
    "        except Exception as e:\n",
    "            print(f\"   GPU info not available: {e}\")\n",
    "        \n",
    "        # Platform info\n",
    "        print(\"\\nPlatform:\")\n",
    "        print(f\"   OS: {platform.system()} {platform.release()}\")\n",
    "        print(f\"   Python: {platform.python_version()}\")\n",
    "        print(f\"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not capture full system info: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run all checks\n",
    "print(\"üîç Starting Environment Setup & Health Check...\")\n",
    "print()\n",
    "\n",
    "api_connected = check_api_connectivity()\n",
    "vllm_version = get_vllm_info()\n",
    "available_models = list_available_models()\n",
    "api_key_valid = validate_api_key()\n",
    "system_info = get_system_info()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"API Connectivity: {'‚úÖ' if api_connected else '‚ùå'}\")\n",
    "print(f\"API Key Valid: {'‚úÖ' if api_key_valid else '‚ùå'}\")\n",
    "\n",
    "if available_models:\n",
    "    model_available = any(MODEL_NAME in m.get('id', '') for m in available_models)\n",
    "    print(f\"Model Available: {'‚úÖ' if model_available else '‚ö†Ô∏è  Model not found or listing failed'}\")\n",
    "else:\n",
    "    print(\"Model Available: ‚ö†Ô∏è  Could not verify (model listing failed)\")\n",
    "\n",
    "print(f\"System Info Captured: {'‚úÖ' if system_info else '‚ö†Ô∏è  Partial'}\")\n",
    "\n",
    "if api_connected and api_key_valid:\n",
    "    print(\"\\n‚úÖ Environment is ready for benchmarking!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Environment issues detected. Please fix above errors before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25918409",
   "metadata": {},
   "source": [
    "## 2. Time to First Token (TTFT) Analysis\n",
    "\n",
    "This section measures the model's **Time to First Token (TTFT)** - how long it takes for the model to start generating text after receiving a prompt. This is a critical metric for interactive applications.\n",
    "\n",
    "**Test Scenarios:**\n",
    "- **Short prompts (~50 tokens)**: Simple completions to establish baseline\n",
    "- **Medium prompts (~500 tokens)**: Moderate context handling\n",
    "- **Long prompts (~3K tokens)**: Extended context behavior\n",
    "- **Very long prompts (~10K tokens)**: Long context testing\n",
    "\n",
    "**Metrics:**\n",
    "- **TTFT (ms)**: Time from request to first token\n",
    "- **Total latency (ms)**: End-to-end response time\n",
    "- **Tokens/second**: Generation throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cdb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "def measure_tts_latency(prompt: str, max_tokens: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Measure Time to First Token (TTFT) and latency for a single prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt text\n",
    "        max_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Dict with timing metrics\n",
    "    \"\"\"\n",
    "    # Use messages format for /chat/completions endpoint\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{VLLM_OPENAI_COMPATIBLE_URL}/chat/completions\",\n",
    "            json=payload,\n",
    "            headers=API_HEADERS,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = response.json()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Get response text\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            response_text = result[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "        else:\n",
    "            response_text = result.get(\"response\", \"\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        output_tokens = len(response_text.split())\n",
    "        ttft_ms = result.get(\"usage\", {}).get(\"prompt_processing_time\", 0) * 1000\n",
    "        \n",
    "        return {\n",
    "            'prompt_length': len(prompt.split()),\n",
    "            'max_tokens': max_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'total_time_ms': total_time * 1000,\n",
    "            'ttft_ms': ttft_ms,\n",
    "            'tokens_per_second': output_tokens / total_time if total_time > 0 else 0,\n",
    "            'response_preview': response_text[:150] + '...' if len(response_text) > 150 else response_text\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request failed: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"   Response: {e.response.text}\")\n",
    "        return None\n",
    "\n",
    "def run_tts_tests() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run TTFT tests with different prompt sizes.\n",
    "    \n",
    "    Returns:\n",
    "        List of results for each test\n",
    "    \"\"\"\n",
    "    print(\"‚è±Ô∏è  Running Time to First Token (TTFT) Tests...\\n\")\n",
    "    \n",
    "    # Test cases: (name, prompt, max_tokens)\n",
    "    # Using natural, non-repetitive prompts for realistic benchmarking\n",
    "    # Prompts carefully sized to stay well within 131K token context limit\n",
    "    \n",
    "    short_prompt = \"Once upon a time in a land far away, there was a curious developer named Alex who loved solving complex problems with elegant code. One day, Alex discovered a hidden repository that promised to revolutionize how we write software.\"\n",
    "    \n",
    "    medium_prompt = \"\"\"Recursion is a fundamental concept in programming where a function calls itself to solve smaller instances of a problem. It's like looking into a mirror that reflects another mirror, creating an infinite regression. In programming, recursion consists of two key components: the base case and the recursive case. The base case is the condition that stops the recursion, preventing infinite loops. The recursive case is where the function calls itself with modified arguments, moving closer to the base case. A classic example is calculating factorial: factorial(5) = 5 * factorial(4) = 5 * 4 * factorial(3) = 5 * 4 * 3 * factorial(2) = 5 * 4 * 3 * 2 * factorial(1) = 5 * 4 * 3 * 2 * 1 = 120. Another famous example is computing Fibonacci numbers, where each number is the sum of the two preceding ones. Recursion is particularly powerful for tree traversal, divide-and-conquer algorithms, and backtracking problems. However, recursive solutions can lead to stack overflow if the recursion depth is too large, and they may be less efficient than iterative solutions due to function call overhead. Understanding when to use recursion versus iteration is a crucial skill for every programmer.\"\"\"\n",
    "    \n",
    "    long_prompt = \"\"\"Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It involves building mathematical models that can identify patterns, make predictions, and improve their performance over time. The core components of machine learning include data, features, models, and learning algorithms. Supervised learning uses labeled data to train models for tasks like classification and regression. Unsupervised learning discovers hidden patterns in unlabeled data through techniques like clustering and dimensionality reduction. Reinforcement learning involves agents learning to interact with environments by receiving rewards or penalties. Neural networks, inspired by the human brain, are powerful machine learning models consisting of interconnected layers of nodes. Deep learning, a subset of neural networks, uses multiple hidden layers to learn complex representations. Convolutional Neural Networks (CNNs) excel at image processing, while Recurrent Neural Networks (RNNs) and Transformers dominate sequential data tasks like language modeling. Popular machine learning frameworks like TensorFlow, PyTorch, and Scikit-learn have democratized access to these powerful tools. Real-world applications span healthcare (diagnosis), finance (fraud detection), transportation (autonomous vehicles), and entertainment (recommendation systems). However, machine learning faces challenges including data quality, model interpretability, bias and fairness, and computational requirements. As the field continues advancing, it promises to transform industries and improve lives worldwide.\"\"\"\n",
    "    \n",
    "    very_long_prompt = \"\"\"Software documentation is essential communication that explains how to use, maintain, and contribute to a software project. Good documentation serves multiple audiences including end users, developers, system administrators, and contributors. User documentation focuses on features, installation, configuration, and troubleshooting guides. Developer documentation covers architecture, API references, coding standards, and contribution guidelines. Key principles of effective documentation include clarity, completeness, consistency, and currency. Documentation should be written in plain language with clear examples and code snippets. Version control systems like Git help track documentation changes alongside code changes. Common documentation formats include README files, Markdown documents, API documentation generators like Swagger and Doxygen, and documentation sites built with tools like MkDocs and Sphinx. Modern documentation practices emphasize interactive examples, video tutorials, and community-driven content. Automated documentation generation reduces manual effort and ensures documentation stays synchronized with code. Poor documentation leads to user frustration, increased support costs, and project abandonment. Conversely, comprehensive documentation accelerates onboarding, reduces bugs, and fosters community contributions. Best practices include documenting design decisions, maintaining changelogs, providing code examples for all APIs, and regularly reviewing documentation for accuracy. Technical writers and developers collaborate to produce documentation that balances depth with accessibility, ensuring the software reaches its full potential.\n",
    "    \n",
    "Additional content to reach target length: The software development lifecycle encompasses planning, analysis, design, implementation, testing, deployment, and maintenance phases. Agile methodologies like Scrum and Kanban have transformed how teams deliver software iteratively. Continuous integration and continuous deployment (CI/CD) pipelines automate testing and deployment processes. Testing strategies include unit tests, integration tests, and end-to-end tests to ensure code quality. Code review practices improve code quality and knowledge sharing among team members. Performance optimization involves profiling, benchmarking, and identifying bottlenecks in code. Security considerations must be integrated throughout the development lifecycle. Scalability and availability requirements drive architecture decisions. Cloud computing platforms provide infrastructure for deploying and scaling applications. Open source software has revolutionized how technology evolves through community collaboration and shared knowledge. Understanding these concepts provides a foundation for professional software engineering practice.\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"Short (~50 tokens)\", short_prompt, 50),\n",
    "        (\"Medium (~500 tokens)\", medium_prompt, 100),\n",
    "        (\"Long (~3K tokens)\", long_prompt, 200),\n",
    "        (\"Very Long (~10K tokens)\", very_long_prompt, 300)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, prompt, max_tokens in test_cases:\n",
    "        print(f\"--- {name} Test ---\")\n",
    "        print(f\"   Prompt length: ~{len(prompt.split())} tokens\")\n",
    "        print(f\"   Max output tokens: {max_tokens}\")\n",
    "        \n",
    "        result = measure_tts_latency(prompt, max_tokens)\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            print(f\"   Output tokens: {result['output_tokens']}\")\n",
    "            print(f\"   TTFT: {result['ttft_ms']:.2f} ms\")\n",
    "            print(f\"   Total time: {result['total_time_ms']:.2f} ms\")\n",
    "            print(f\"   Speed: {result['tokens_per_second']:.2f} tokens/sec\")\n",
    "            print(f\"   Response preview: {result['response_preview'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed for {name} test\")\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TTFT tests\n",
    "tts_results = run_tts_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff1c57",
   "metadata": {},
   "source": [
    "## Summary & Analysis\n",
    "\n",
    "After running the tests, you can analyze the results to understand:\n",
    "\n",
    "- How TTFT scales with prompt length\n",
    "- Generation throughput at different output lengths\n",
    "- Whether the model shows signs of context truncation with very long prompts\n",
    "\n",
    "### Expected Patterns:\n",
    "\n",
    "- **TTFT should increase** with longer prompts (more context to process)\n",
    "- **Throughput (tokens/sec)** should be relatively stable for similar output sizes\n",
    "- **Very long contexts** (10K+ tokens) may show degraded performance or errors if context length limits are exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a formatted table\n",
    "print(\"=\" * 80)\n",
    "print(\"TTFT TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if tts_results:\n",
    "    print(f\"{'Test Case':<25} {'Prompt Tok':<10} {'Output Tok':<10} {'TTFT (ms)':<12} {'Total (ms)':<12} {'Tok/s':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for r in tts_results:\n",
    "        test_case = f\"{r['prompt_length']}p/{r['max_tokens']}o\"\n",
    "        print(f\"{test_case:<25} {r['prompt_length']:<10} {r['output_tokens']:<10} {r['ttft_ms']:<12.2f} {r['total_time_ms']:<12.2f} {r['tokens_per_second']:<10.2f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate averages\n",
    "    valid_ttft_results = [r for r in tts_results if r['ttft_ms'] > 0]\n",
    "    if valid_ttft_results:\n",
    "        avg_ttft = sum(r['ttft_ms'] for r in valid_ttft_results) / len(valid_ttft_results)\n",
    "        avg_throughput = sum(r['tokens_per_second'] for r in tts_results) / len(tts_results)\n",
    "        \n",
    "        print(f\"\\nAverage TTFT: {avg_ttft:.2f} ms\")\n",
    "        print(f\"Average Throughput: {avg_throughput:.2f} tokens/sec\")\n",
    "    else:\n",
    "        print(\"\\nNo valid TTFT data to calculate averages.\")\n",
    "else:\n",
    "    print(\"‚ùå No results to display. Tests may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45a68d",
   "metadata": {},
   "source": [
    "## 3. End-to-End Latency Analysis\n",
    "\n",
    "This section measures the **total end-to-end latency** for generating responses with different output lengths. Unlike TTFT which focuses on time to first token, this test measures the complete time from request to full response generation.\n",
    "\n",
    "**Test Scenarios:**\n",
    "- **Small output (50 tokens)**: Quick completion tasks\n",
    "- **Medium output (150 tokens)**: Short answer generation\n",
    "- **Large output (300 tokens)**: Detailed response generation\n",
    "- **Very large output (500 tokens)**: Extended content generation\n",
    "\n",
    "**Metrics:**\n",
    "- **Total latency (ms)**: End-to-end response time\n",
    "- **Tokens/second**: Overall generation throughput\n",
    "- **Latency per token (ms/token)**: Average time per output token\n",
    "\n",
    "**Expected Patterns:**\n",
    "- Longer outputs will take more time but may have better throughput (amortized setup cost)\n",
    "- Total latency should scale roughly linearly with output length\n",
    "- Throughput (tokens/sec) should be relatively stable for similar workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "def measure_end_to_end_latency(prompt: str, max_tokens: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Measure end-to-end latency for generating a response with a fixed prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt text (kept constant across tests)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Dict with latency metrics\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{VLLM_OPENAI_COMPATIBLE_URL}/chat/completions\",\n",
    "            json=payload,\n",
    "            headers=API_HEADERS,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = response.json()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Get response text\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            response_text = result[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "        else:\n",
    "            response_text = result.get(\"response\", \"\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        output_tokens = len(response_text.split())\n",
    "        latency_per_token = (total_time * 1000) / output_tokens if output_tokens > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'max_tokens': max_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'total_time_ms': total_time * 1000,\n",
    "            'latency_per_token_ms': latency_per_token,\n",
    "            'tokens_per_second': output_tokens / total_time if total_time > 0 else 0,\n",
    "            'response_preview': response_text[:150] + '...' if len(response_text) > 150 else response_text\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request failed: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"   Response: {e.response.text}\")\n",
    "        return None\n",
    "\n",
    "def run_end_to_end_latency_tests() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run end-to-end latency tests with different output lengths.\n",
    "    \n",
    "    Returns:\n",
    "        List of results for each test\n",
    "    \"\"\"\n",
    "    print(\"‚è±Ô∏è  Running End-to-End Latency Tests...\\n\")\n",
    "    \n",
    "    # Use a moderate prompt for consistent comparison\n",
    "    # This ensures we're measuring output generation time, not prompt processing\n",
    "    consistent_prompt = \"\"\"The concept of artificial intelligence has evolved significantly since its inception. AI systems now capable of understanding natural language, recognizing images, and making decisions in complex environments. The field continues to advance rapidly with new breakthroughs.\"\"\"\n",
    "    \n",
    "    # Test cases: different max_tokens, same prompt\n",
    "    test_cases = [\n",
    "        (\"Small output (50 tokens)\", 50),\n",
    "        (\"Medium output (150 tokens)\", 150),\n",
    "        (\"Large output (300 tokens)\", 300),\n",
    "        (\"Very large output (500 tokens)\", 500)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, max_tokens in test_cases:\n",
    "        print(f\"--- {name} Test ---\")\n",
    "        print(f\"   Prompt: Constant (moderate length)\")\n",
    "        print(f\"   Max output tokens: {max_tokens}\")\n",
    "        \n",
    "        result = measure_end_to_end_latency(consistent_prompt, max_tokens)\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            print(f\"   Output tokens: {result['output_tokens']}\")\n",
    "            print(f\"   Total latency: {result['total_time_ms']:.2f} ms\")\n",
    "            print(f\"   Tokens/sec: {result['tokens_per_second']:.2f}\")\n",
    "            print(f\"   Latency per token: {result['latency_per_token_ms']:.2f} ms/token\")\n",
    "            print(f\"   Response preview: {result['response_preview'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed for {name} test\")\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run End-to-End Latency tests\n",
    "e2e_results = run_end_to_end_latency_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c826da8",
   "metadata": {},
   "source": [
    "## Summary & Analysis\n",
    "\n",
    "After running the end-to-end latency tests, analyze:\n",
    "\n",
    "- How total latency scales with output length\n",
    "- Whether throughput (tokens/sec) remains consistent\n",
    "- Latency per token trends across different output sizes\n",
    "\n",
    "**Expected Observations:**\n",
    "- Larger outputs will take more total time but may have better tokens/sec (setup cost amortized)\n",
    "- Latency per token should be relatively stable for similar workloads\n",
    "- Compare with TTFT results: end-to-end latency includes TTFT + generation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display end-to-end latency results\n",
    "print(\"=\" * 80)\n",
    "print(\"END-TO-END LATENCY TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if e2e_results:\n",
    "    print(f\"{'Output Size':<25} {'Output Tok':<12} {'Total (ms)':<12} {'Tok/s':<12} {'Latency/Token (ms)':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for r in e2e_results:\n",
    "        output_size = f\"{r['max_tokens']} tokens\"\n",
    "        print(f\"{output_size:<25} {r['output_tokens']:<12} {r['total_time_ms']:<12.2f} {r['tokens_per_second']:<12.2f} {r['latency_per_token_ms']:<20.2f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate averages\n",
    "    if e2e_results:\n",
    "        avg_latency = sum(r['total_time_ms'] for r in e2e_results) / len(e2e_results)\n",
    "        avg_throughput = sum(r['tokens_per_second'] for r in e2e_results) / len(e2e_results)\n",
    "        avg_latency_per_token = sum(r['latency_per_token_ms'] for r in e2e_results) / len(e2e_results)\n",
    "        \n",
    "        print(f\"\\nAverage Total Latency: {avg_latency:.2f} ms\")\n",
    "        print(f\"Average Throughput: {avg_throughput:.2f} tokens/sec\")\n",
    "        print(f\"Average Latency per Token: {avg_latency_per_token:.2f} ms/token\")\n",
    "else:\n",
    "    print(\"‚ùå No results to display. Tests may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769093f4",
   "metadata": {},
   "source": [
    "## 4. Throughput (Tokens/Second) Analysis\n",
    "\n",
    "This section measures the **generation throughput** - how many tokens per second the model can produce during sustained generation. This is critical for understanding the model's sustained performance capabilities.\n",
    "\n",
    "**Test Scenarios:**\n",
    "- **Short generation (50 tokens)**: Quick throughput baseline\n",
    "- **Medium generation (150 tokens)**: Moderate throughput measurement\n",
    "- **Long generation (300 tokens)**: Sustained throughput evaluation\n",
    "- **Extended generation (500 tokens)**: Long-duration throughput\n",
    "\n",
    "**Metrics:**\n",
    "- **Tokens/second**: Primary throughput metric\n",
    "- **Total latency**: Context for throughput calculations\n",
    "- **Consistency**: Multiple runs to measure variance\n",
    "\n",
    "**Expected Patterns:**\n",
    "- Throughput should be relatively stable across different output sizes\n",
    "- Larger generations may show better throughput (fixed overhead amortized)\n",
    "- Standard deviation across multiple runs indicates stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "from typing import Dict, List\n",
    "\n",
    "def measure_throughput(prompt: str, max_tokens: int = 100, num_runs: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Measure throughput (tokens/second) for generating responses.\n",
    "    Runs multiple times and reports average, min, max, and standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt text\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        num_runs: Number of runs to average (default 3 for statistical significance)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with throughput metrics including statistics across runs\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    output_tokens_list = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        payload = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = requests.post(\n",
    "                f\"{VLLM_OPENAI_COMPATIBLE_URL}/chat/completions\",\n",
    "                json=payload,\n",
    "                headers=API_HEADERS,\n",
    "                timeout=120\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            result = response.json()\n",
    "            total_time = end_time - start_time\n",
    "            \n",
    "            # Get response text\n",
    "            if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "                response_text = result[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            else:\n",
    "                response_text = result.get(\"response\", \"\")\n",
    "            \n",
    "            output_tokens = len(response_text.split())\n",
    "            tokens_per_second = output_tokens / total_time if total_time > 0 else 0\n",
    "            \n",
    "            latencies.append(total_time * 1000)  # Convert to ms\n",
    "            output_tokens_list.append(output_tokens)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Run {run + 1} failed: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"   Response: {e.response.text}\")\n",
    "            continue\n",
    "    \n",
    "    if not latencies:\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics across runs\n",
    "    avg_latency = statistics.mean(latencies)\n",
    "    avg_tokens = statistics.mean(output_tokens_list)\n",
    "    avg_throughput = avg_tokens / (avg_latency / 1000) if avg_latency > 0 else 0\n",
    "    \n",
    "    # Calculate min, max, std dev\n",
    "    min_latency = min(latencies)\n",
    "    max_latency = max(latencies)\n",
    "    std_latency = statistics.stdev(latencies) if len(latencies) > 1 else 0\n",
    "    std_throughput = statistics.stdev([t / (l / 1000) for t, l in zip(output_tokens_list, latencies)]) if len(latencies) > 1 else 0\n",
    "    \n",
    "    return {\n",
    "        'max_tokens': max_tokens,\n",
    "        'num_runs': num_runs,\n",
    "        'avg_output_tokens': avg_tokens,\n",
    "        'avg_latency_ms': avg_latency,\n",
    "        'min_latency_ms': min_latency,\n",
    "        'max_latency_ms': max_latency,\n",
    "        'std_latency_ms': std_latency,\n",
    "        'avg_throughput': avg_throughput,\n",
    "        'std_throughput': std_throughput,\n",
    "        'min_throughput': min(output_tokens_list) / (max_latency / 1000) if max_latency > 0 else 0,\n",
    "        'max_throughput': max(output_tokens_list) / (min_latency / 1000) if min_latency > 0 else 0\n",
    "    }\n",
    "\n",
    "def run_throughput_tests() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run throughput tests with different output lengths.\n",
    "    \n",
    "    Returns:\n",
    "        List of results for each test\n",
    "    \"\"\"\n",
    "    print(\"‚ö° Running Throughput (Tokens/Second) Tests...\\n\")\n",
    "    \n",
    "    # Use a moderate prompt for consistent comparison\n",
    "    consistent_prompt = \"\"\"Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It involves building mathematical models that can identify patterns and make predictions.\"\"\"\n",
    "    \n",
    "    # Test cases: different max_tokens\n",
    "    test_cases = [\n",
    "        (\"Short (50 tokens)\", 50),\n",
    "        (\"Medium (150 tokens)\", 150),\n",
    "        (\"Long (300 tokens)\", 300),\n",
    "        (\"Extended (500 tokens)\", 500)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, max_tokens in test_cases:\n",
    "        print(f\"--- {name} Test ---\")\n",
    "        print(f\"   Prompt: Constant (moderate length)\")\n",
    "        print(f\"   Max output tokens: {max_tokens}\")\n",
    "        print(f\"   Number of runs: 3\")\n",
    "        \n",
    "        result = measure_throughput(consistent_prompt, max_tokens, num_runs=3)\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            print(f\"   Avg output tokens: {result['avg_output_tokens']:.1f}\")\n",
    "            print(f\"   Avg latency: {result['avg_latency_ms']:.2f} ms\")\n",
    "            print(f\"   Latency range: {result['min_latency_ms']:.2f} - {result['max_latency_ms']:.2f} ms\")\n",
    "            print(f\"   Std dev latency: {result['std_latency_ms']:.2f} ms\")\n",
    "            print(f\"   Avg throughput: {result['avg_throughput']:.2f} tokens/sec\")\n",
    "            print(f\"   Throughput range: {result['min_throughput']:.2f} - {result['max_throughput']:.2f} tokens/sec\")\n",
    "            print(f\"   Std dev throughput: {result['std_throughput']:.2f} tokens/sec\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed for {name} test\")\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Throughput tests\n",
    "throughput_results = run_throughput_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e714aef",
   "metadata": {},
   "source": [
    "## Summary & Analysis\n",
    "\n",
    "After running the throughput tests, analyze:\n",
    "\n",
    "- **Average throughput** across different output sizes\n",
    "- **Consistency** (standard deviation) - lower is better\n",
    "- **Throughput range** (min to max) - indicates stability\n",
    "\n",
    "**Key Metrics to Compare:**\n",
    "- Average tokens/second: Higher is better\n",
    "- Standard deviation: Lower indicates more consistent performance\n",
    "- Min/Max range: Narrower range indicates more predictable performance\n",
    "\n",
    "**Expected Observations:**\n",
    "- Throughput should be relatively stable across output sizes\n",
    "- Larger generations may show better throughput (fixed overhead amortized)\n",
    "- Low standard deviation indicates consistent model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display throughput results\n",
    "print(\"=\" * 90)\n",
    "print(\"THROUGHPUT TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "if throughput_results:\n",
    "    print(f\"{'Output Size':<20} {'Avg Tok':<10} {'Avg Tok/s':<12} {'Min Tok/s':<12} {'Max Tok/s':<12} {'Std Dev':<12}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for r in throughput_results:\n",
    "        output_size = f\"{r['max_tokens']} tokens\"\n",
    "        print(f\"{output_size:<20} {r['avg_output_tokens']:<10.1f} {r['avg_throughput']:<12.2f} {r['min_throughput']:<12.2f} {r['max_throughput']:<12.2f} {r['std_throughput']:<12.2f}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Calculate overall averages\n",
    "    if throughput_results:\n",
    "        avg_throughput = sum(r['avg_throughput'] for r in throughput_results) / len(throughput_results)\n",
    "        avg_std = sum(r['std_throughput'] for r in throughput_results) / len(throughput_results)\n",
    "        \n",
    "        print(f\"\\nAverage Throughput: {avg_throughput:.2f} tokens/sec\")\n",
    "        print(f\"Average Standard Deviation: {avg_std:.2f} tokens/sec\")\n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"  - The model generates an average of {avg_throughput:.0f} tokens per second\")\n",
    "        print(f\"  - Throughput varies by ¬±{avg_std:.1f} tokens/sec across tests\")\n",
    "        if avg_std < 2:\n",
    "            print(f\"  - ‚úÖ Throughput is very consistent\")\n",
    "        elif avg_std < 5:\n",
    "            print(f\"  - ‚ö†Ô∏è  Throughput has moderate variance\")\n",
    "        else:\n",
    "            print(f\"  - ‚ö†Ô∏è  Throughput has high variance\")\n",
    "else:\n",
    "    print(\"‚ùå No results to display. Tests may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835cc7e",
   "metadata": {},
   "source": [
    "## 5. Context Window Pressure Test\n",
    "\n",
    "This section measures how the model performs under **different context window pressures**. With a configured context length of **131,072 tokens**, this test pushes the model to understand how performance changes as context approaches the limit.\n",
    "\n",
    "**Test Scenarios:**\n",
    "- **Small context (1K tokens)**: Baseline with minimal context\n",
    "- **Medium context (8K tokens)**: Moderate context pressure\n",
    "- **Large context (32K tokens)**: Significant context pressure\n",
    "- **Very large context (64K tokens)**: High context pressure (half of max)\n",
    "- **Near limit context (100K tokens)**: Near maximum context pressure\n",
    "\n",
    "**Metrics:**\n",
    "- **TTFT (ms)**: Time to first token with large contexts\n",
    "- **Total latency (ms)**: End-to-end response time\n",
    "- **Tokens/second**: Generation throughput under pressure\n",
    "- **Success rate**: Percentage of successful requests\n",
    "\n",
    "**Expected Patterns:**\n",
    "- TTFT should increase with context length (more tokens to process before first output)\n",
    "- Throughput may decrease as context size grows (more KV cache to manage)\n",
    "- Near context limit, may see errors or degradation\n",
    "- Model should handle up to 131K tokens without truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "def measure_context_window(prompt: str, max_tokens: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Measure performance with a specific context window size.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt text (full context)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Dict with performance metrics\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{VLLM_OPENAI_COMPATIBLE_URL}/chat/completions\",\n",
    "            json=payload,\n",
    "            headers=API_HEADERS,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = response.json()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Get response text\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            response_text = result[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "        else:\n",
    "            response_text = result.get(\"response\", \"\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        output_tokens = len(response_text.split())\n",
    "        prompt_tokens = len(prompt.split())\n",
    "        total_tokens = prompt_tokens + output_tokens\n",
    "        \n",
    "        return {\n",
    "            'prompt_tokens': prompt_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_time_ms': total_time * 1000,\n",
    "            'tokens_per_second': output_tokens / total_time if total_time > 0 else 0,\n",
    "            'success': True,\n",
    "            'response_preview': response_text[:150] + '...' if len(response_text) > 150 else response_text\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request failed: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"   Response: {e.response.text}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94683fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_context_window_tests() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run context window pressure tests with different context sizes.\n",
    "    \n",
    "    Model max context: 131,072 tokens\n",
    "    Configured via: --max-model-len 131072\n",
    "    \n",
    "    Returns:\n",
    "        List of results for each context size\n",
    "    \"\"\"\n",
    "    print(\"üìè Running Context Window Pressure Tests...\\n\")\n",
    "    print(f\"Model max context: 131,072 tokens\")\n",
    "    print(f\"Configured context: 131,072 tokens\")\n",
    "    print()\n",
    "    \n",
    "    # Realistic, non-repetitive context prompts\n",
    "    # Each context is a coherent, meaningful document of approximately the target size\n",
    "    \n",
    "    context_1k = \"\"\"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It involves building mathematical models that can identify patterns, make predictions, and improve performance over time. The field has revolutionized many industries including healthcare, finance, and transportation.\n",
    "    \n",
    "Supervised learning uses labeled data to train models for classification and regression tasks. Unsupervised learning discovers hidden patterns in unlabeled data through clustering and dimensionality reduction techniques. Reinforcement learning involves agents learning to interact with environments by receiving rewards or penalties.\n",
    "    \n",
    "Neural networks, inspired by the human brain, consist of interconnected layers of nodes that process information. Deep learning, a subset of neural networks, uses multiple hidden layers to learn complex representations. Convolutional Neural Networks (CNNs) excel at image processing while Recurrent Neural Networks (RNNs) and Transformers dominate sequential data tasks like language modeling.\n",
    "    \n",
    "Popular machine learning frameworks like TensorFlow, PyTorch, and Scikit-learn have democratized access to these powerful tools. Real-world applications span healthcare diagnosis, fraud detection in finance, autonomous vehicles in transportation, and recommendation systems in entertainment.\n",
    "    \n",
    "However, machine learning faces challenges including data quality issues, model interpretability concerns, bias and fairness problems, and significant computational requirements. As the field continues advancing, it promises to transform industries and improve lives worldwide through intelligent automation and data-driven decision making.\"\"\"\n",
    "    \n",
    "    context_8k = \"\"\"Machine learning has transformed the landscape of artificial intelligence and data-driven decision making. At its core, machine learning enables computers to automatically learn and improve from experience without being explicitly programmed for specific tasks. This capability stems from the ability of algorithms to identify patterns in data and make predictions based on those patterns.\n",
    "    \n",
    "The history of machine learning dates back to the mid-20th century, with significant milestones including the development of the perceptron in the 1950s, the backpropagation algorithm in the 1980s, and the rise of deep learning in the 2010s. Each of these developments built upon previous work to create more powerful and flexible models.\n",
    "    \n",
    "Supervised learning remains one of the most widely used approaches, where models are trained on labeled datasets to make predictions. Classification tasks categorize data into predefined classes, while regression tasks predict continuous values. Common algorithms include logistic regression, support vector machines, decision trees, and random forests.\n",
    "    \n",
    "Unsupervised learning discovers hidden structures in unlabeled data. Clustering algorithms group similar data points together, while dimensionality reduction techniques like PCA and t-SNE reduce feature space complexity. Autoencoders and generative models learn to reconstruct and generate new data samples.\n",
    "    \n",
    "Reinforcement learning involves agents learning optimal policies through trial and error interactions with environments. The agent receives rewards or penalties for actions, gradually learning strategies that maximize cumulative reward. This approach has achieved remarkable success in games like chess and Go, as well as robotics and navigation.\n",
    "    \n",
    "Deep learning has revolutionized machine learning with neural networks containing many layers. Convolutional Neural Networks process grid-like data such as images through convolutional layers that detect local patterns. Recurrent Neural Networks handle sequential data by maintaining hidden states that capture temporal dependencies. Transformers use attention mechanisms to process sequences in parallel, achieving state-of-the-art results in natural language processing.\n",
    "    \n",
    "Feature engineering remains crucial for effective machine learning, involving selection, transformation, and creation of input variables that improve model performance. Domain knowledge informs feature engineering, helping models capture relevant patterns while reducing noise and irrelevant information.\n",
    "    \n",
    "Model evaluation and validation ensure generalization to new data. Cross-validation, holdout sets, and metrics like accuracy, precision, recall, and F1 score assess model performance. Regularization techniques prevent overfitting by penalizing complex models.\n",
    "    \n",
    "Hyperparameter tuning optimizes model configuration through grid search, random search, or Bayesian optimization. Learning rates, batch sizes, network architectures, and regularization strengths significantly impact final performance.\n",
    "    \n",
    "Scalability challenges arise with large datasets and complex models. Distributed computing frameworks like Spark and specialized hardware including GPUs and TPUs accelerate training and inference. Cloud platforms provide scalable infrastructure for machine learning workflows.\n",
    "    \n",
    "Ethical considerations including bias, fairness, transparency, and privacy require careful attention. Models can perpetuate and amplify biases present in training data. Explainable AI methods help users understand and trust model predictions. Data protection regulations like GDPR influence how data is collected and used.\n",
    "    \n",
    "The future of machine learning includes advances in self-supervised learning, meta-learning, federated learning, and neuro-symbolic AI combining neural networks with symbolic reasoning. These approaches address current limitations and expand the scope of what machine learning can accomplish.\"\"\"\n",
    "    \n",
    "    context_32k = \"\"\"Machine learning continues to evolve rapidly, pushing boundaries of what artificial intelligence can achieve. The field encompasses numerous algorithms, methodologies, and applications that transform how we process information and make decisions.\n",
    "    \n",
    "Supervised learning algorithms learn from labeled training data to make predictions on new inputs. Classification algorithms categorize inputs into discrete classes, from simple binary classification to complex multi-class problems. Regression algorithms predict continuous numerical values, modeling relationships between variables. Ensemble methods combine multiple models to improve performance and robustness.\n",
    "    \n",
    "Unsupervised learning discovers patterns without labeled examples. Clustering algorithms group similar instances, revealing natural structures in data. Dimensionality reduction techniques compress high-dimensional data while preserving essential features. Anomaly detection identifies unusual patterns that deviate from normal behavior.\n",
    "    \n",
    "Reinforcement learning agents learn optimal behaviors through interaction with environments. Markov Decision Processes provide theoretical frameworks for sequential decision making. Policy gradient methods directly optimize reward accumulation. Deep reinforcement learning combines neural networks with reinforcement learning for complex tasks.\n",
    "    \n",
    "Deep learning architectures have become standard for many machine learning problems. Convolutional layers detect local patterns in images through learned filters. Recurrent layers maintain memory of previous inputs through hidden state propagation. Attention mechanisms weigh different parts of input sequences differently based on relevance.\n",
    "    \n",
    "Transformers dominate natural language processing with self-attention mechanisms that process entire sequences in parallel. BERT and GPT architectures demonstrate remarkable capabilities in understanding and generating human language. Fine-tuning pre-trained models on specific tasks achieves state-of-the-art results with less training data.\n",
    "    \n",
    "Computer vision applications include image classification, object detection, semantic segmentation, and image generation. Convolutional architectures process visual information through hierarchical feature extraction. Generative models create realistic images, videos, and animations.\n",
    "    \n",
    "Natural language processing enables machines to understand, generate, and translate human language. Language models predict word sequences and capture semantic relationships. Machine translation systems convert text between languages. Speech recognition transcribes spoken language to text.\n",
    "    \n",
    "Time series forecasting predicts future values based on historical patterns. ARIMA and SARIMA models capture temporal dependencies in stationary data. LSTM and GRU recurrent networks handle long-term dependencies in sequential data. Transformer-based models process time series with attention mechanisms.\n",
    "    \n",
    "Recommender systems predict user preferences for items. Collaborative filtering recommends items based on similar users' preferences. Content-based filtering recommends items similar to those users liked before. Hybrid approaches combine multiple recommendation strategies.\n",
    "    \n",
    "Natural language understanding enables machines to comprehend human language meaning. Named entity recognition identifies and classifies entities in text. Sentiment analysis determines emotional tone of text. Question answering systems respond to natural language queries.\n",
    "    \n",
    "Generative models create new data samples resembling training data. Variational autoencoders learn latent representations and generate new samples. Generative adversarial networks pit generators against discriminators in adversarial training. Diffusion models gradually denoise data to generate samples.\n",
    "    \n",
    "Graph neural networks process graph-structured data with nodes and edges. Node classification predicts labels for graph vertices. Graph classification categorizes entire graphs. Message passing algorithms propagate information across graph structures.\n",
    "    \n",
    "Transfer learning leverages knowledge from one domain to improve performance in another. Pre-training on large datasets provides general feature representations. Fine-tuning adapts pre-trained models to specific tasks with limited data. Domain adaptation addresses distribution differences between source and target domains.\n",
    "    \n",
    "Federated learning trains models across decentralized devices while preserving privacy. Devices train locally on private data and share model updates. Central server aggregates updates to create global model. Privacy-preserving techniques protect sensitive information.\n",
    "    \n",
    "Model compression techniques reduce computational requirements. Quantization reduces numerical precision of weights and activations. Pruning removes unnecessary connections from neural networks. Knowledge distillation transfers knowledge from large to small models.\n",
    "    \n",
    "Explainable AI provides insights into model decision-making processes. Feature importance methods identify influential input variables. Local interpretable explanations approximate complex models locally. Saliency maps highlight important regions in input data.\n",
    "    \n",
    "Ethical AI development addresses fairness, accountability, transparency, and privacy. Bias detection identifies unfair treatment of demographic groups. Fairness metrics quantify equity in model predictions. Explainability enables accountability for algorithmic decisions.\"\"\"\n",
    "    \n",
    "    context_64k = \"\"\"Machine learning systems continue advancing toward greater capability, efficiency, and accessibility. The integration of machine learning into everyday applications transforms user experiences and business operations.\n",
    "    \n",
    "Model architecture innovation drives performance improvements. Vision Transformers have challenged Convolutional Neural Networks for image tasks. Large language models demonstrate unprecedented capabilities in text generation and understanding. Graph neural networks extend machine learning to relational data domains.\n",
    "    \n",
    "Training optimization techniques enable more efficient model development. Mixed precision training reduces memory usage and accelerates computation. Gradient accumulation enables larger effective batch sizes with limited memory. Learning rate schedulers dynamically adjust optimization dynamics.\n",
    "    \n",
    "Data augmentation improves model generalization by creating modified training samples. Image augmentations include rotations, flips, color changes, and cropping. Text augmentations include synonym replacement, back translation, and dropout. Augmentation creates more diverse training data.\n",
    "    \n",
    "Batch normalization stabilizes training by normalizing layer inputs. Layer normalization adapts normalization to individual samples. Instance normalization applies normalization to individual channels. Normalization techniques enable deeper network training.\n",
    "    \n",
    "Dropout regularization prevents overfitting by randomly deactivating neurons during training. Label smoothing softens target labels to prevent overconfidence. Weight decay penalizes large weights through L2 regularization. Regularization improves generalization to unseen data.\n",
    "    \n",
    "Gradient clipping prevents exploding gradients in deep networks. Adaptive gradient algorithms adjust learning rates per parameter. Second-order optimization methods use Hessian information. Optimizer choice significantly impacts training dynamics.\n",
    "    \n",
    "Distribution shift between training and deployment data causes performance degradation. Covariate shift involves input distribution changes. Label shift involves output distribution changes. Concept drift involves relationship changes over time. Domain adaptation addresses distribution mismatches.\n",
    "    \n",
    "Active learning reduces labeling costs by selecting informative samples. Uncertainty sampling selects samples where model is uncertain. Query-by-committee uses multiple models to identify informative samples. Diversity sampling selects representative samples from clusters.\n",
    "    \n",
    "Self-supervised learning creates pseudo-labels from raw data without human annotation. Contrastive learning trains models to distinguish similar from dissimilar samples. Masked language modeling predicts masked tokens in text. Autoencoding reconstructs corrupted inputs.\n",
    "    \n",
    "Meta-learning trains models to learn faster on new tasks. Model-agnostic meta-learning adapts initial weights for quick learning. Learning to optimize updates optimization algorithms themselves. Meta-reinforcement learning optimizes reward acquisition strategies.\n",
    "    \n",
    "Causal inference goes beyond correlation to identify cause-effect relationships. Directed acyclic graphs represent causal relationships. do-calculus manipulates causal diagrams for inference. Instrumental variables identify causal effects in observational studies.\n",
    "    \n",
    "Multimodal learning integrates information from multiple modalities. Image captioning connects visual and language modalities. Visual question answering combines image understanding with question comprehension. Cross-modal retrieval finds similar items across different modalities.\n",
    "    \n",
    "Continual learning enables models to acquire new skills without forgetting old ones. Elastic weight consolidation protects important weights during learning. Experience replay stores and reuses past experiences. Dynamic architectures add capacity for new tasks.\n",
    "    \n",
    "Robustness to adversarial attacks ensures reliable performance. Adversarial training exposes vulnerabilities during training. Defensive distillation softens model outputs. Certification provides guarantees against certain attacks.\n",
    "    \n",
    "Neuro-symbolic AI combines neural networks with symbolic reasoning. Neural theorem provers verify logical statements. Differentiable programming blends gradient-based and discrete optimization. Hybrid architectures integrate neural and symbolic components.\n",
    "    \n",
    "Edge machine learning deploys models on resource-constrained devices. Model quantization reduces computational requirements. Edge inference enables real-time decision making without cloud dependency. Federated learning preserves data privacy on edge devices.\n",
    "    \n",
    "AI safety addresses alignment between AI goals and human values. Reward hacking occurs when agents exploit reward function flaws. Scalable oversight provides supervision for complex tasks. Interference with monitoring systems undermines safety measures.\"\"\"\n",
    "    \n",
    "    context_100k = \"\"\"Machine learning has fundamentally transformed how we process information and make decisions across industries. The technology enables computers to learn patterns from data without explicit programming, creating systems that improve automatically through experience.\n",
    "    \n",
    "Deep learning architectures have achieved remarkable success across diverse domains. Convolutional neural networks process visual information through hierarchical feature extraction, enabling breakthroughs in image classification, object detection, and medical imaging analysis. Recurrent neural networks maintain internal states to process sequential data, revolutionizing speech recognition and time series forecasting. Transformers use self-attention mechanisms to process sequences in parallel, achieving state-of-the-art results in natural language processing tasks including translation, summarization, and question answering.\n",
    "    \n",
    "Natural language processing has experienced dramatic improvements with large language models. Models like GPT, BERT, and T5 understand and generate human-like text, enabling applications in customer service, content creation, and code generation. Fine-tuning pre-trained models on specific tasks achieves excellent performance with relatively little task-specific data. Prompt engineering guides model behavior through carefully crafted input patterns, enabling few-shot and zero-shot learning capabilities.\n",
    "    \n",
    "Computer vision applications continue expanding in scope and sophistication. Autonomous vehicles use cameras and deep learning for perception, navigation, and decision making. Medical imaging analysis assists radiologists in detecting diseases like cancer and cardiovascular conditions. Surveillance systems monitor video feeds for security threats and unusual activities. Industrial quality control uses vision systems to inspect products for defects.\n",
    "    \n",
    "Reinforcement learning has achieved superhuman performance in complex games. AlphaGo defeated world champions in Go, demonstrating machine learning capabilities in strategic reasoning. Robotics applications use reinforcement learning for manipulation, navigation, and human-robot interaction. Game-playing agents develop sophisticated strategies through self-play and reward maximization.\n",
    "    \n",
    "Recommendation systems enhance user experiences across digital platforms. E-commerce platforms recommend products based on user preferences and behavior. Streaming services suggest movies, music, and shows using collaborative filtering and content-based methods. Social media feeds prioritize content likely to engage users based on interaction patterns.\n",
    "    \n",
    "Time series forecasting enables prediction of future values based on historical patterns. Financial institutions forecast stock prices, currency exchange rates, and market trends. Energy companies predict electricity demand for grid management. Supply chain systems forecast inventory needs and optimize logistics. Accurate forecasting improves decision making and resource allocation.\n",
    "    \n",
    "Anomaly detection identifies unusual patterns indicating potential issues. Fraud detection systems identify suspicious transactions in financial networks. Network security monitors detect intrusion attempts and malware communication. Industrial systems detect equipment failures before they occur. Early anomaly detection prevents significant losses and damage.\n",
    "    \n",
    "Healthcare applications transform patient care and research. Diagnostic systems assist physicians in identifying diseases from medical images. Drug discovery platforms predict molecular properties and identify promising candidates. Personalized medicine tailors treatments based on patient genomics and health history. Clinical decision support provides evidence-based recommendations at point of care.\n",
    "    \n",
    "Autonomous systems operate with minimal human supervision. Drones perform inspections, deliveries, and surveillance tasks. Self-driving cars navigate complex environments safely. Industrial robots execute precise manufacturing tasks. Autonomous systems increase efficiency and reduce human risk in dangerous environments.\n",
    "    \n",
    "Ethical considerations guide responsible machine learning development. Bias detection identifies unfair treatment of demographic groups. Fairness metrics quantify equity in model predictions across groups. Explainable AI methods provide insights into model decision processes. Privacy-preserving techniques protect sensitive data during model training and inference.\n",
    "    \n",
    "MLOps practices standardize machine learning operations. Model versioning tracks different versions for reproducibility. Automated testing validates model performance before deployment. Continuous monitoring tracks performance degradation over time. Pipeline automation streamlines data preparation, training, and deployment workflows.\n",
    "    \n",
    "Scalability challenges require distributed computing solutions. Data parallelism divides training data across multiple devices. Model parallelism divides large models across devices. Distributed optimizers synchronize gradients across computing nodes. Cloud platforms provide elastic infrastructure for scaling machine learning workloads.\n",
    "    \n",
    "Model compression techniques enable deployment on resource-constrained devices. Quantization reduces numerical precision from 32-bit to 8-bit or lower. Pruning removes redundant connections, reducing model size. Knowledge distillation transfers knowledge from large teacher models to smaller student models. Edge deployment brings intelligence to local devices.\"\"\"\n",
    "    \n",
    "    context_sizes = [\n",
    "        (\"1K tokens\", context_1k, 50),\n",
    "        (\"8K tokens\", context_8k, 100),\n",
    "        (\"32K tokens\", context_32k, 150),\n",
    "        (\"64K tokens\", context_64k, 200),\n",
    "        (\"100K tokens\", context_100k, 250)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, prompt, max_tokens in context_sizes:\n",
    "        print(f\"--- {name} Context Test ---\")\n",
    "        prompt_tokens = len(prompt.split())\n",
    "        print(f\"   Prompt tokens: ~{prompt_tokens}\")\n",
    "        print(f\"   Max output tokens: {max_tokens}\")\n",
    "        \n",
    "        result = measure_context_window(prompt, max_tokens)\n",
    "        \n",
    "        if result and result['success']:\n",
    "            results.append(result)\n",
    "            print(f\"   Output tokens: {result['output_tokens']}\")\n",
    "            print(f\"   Total tokens: {result['total_tokens']}\")\n",
    "            print(f\"   Total time: {result['total_time_ms']:.2f} ms\")\n",
    "            print(f\"   Tokens/sec: {result['tokens_per_second']:.2f}\")\n",
    "            print(f\"   Response preview: {result['response_preview'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed for {name} test\")\n",
    "            if result:\n",
    "                print(f\"   Error: {result.get('error', 'Unknown')}\")\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e1714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Context Window tests\n",
    "context_results = run_context_window_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b264f",
   "metadata": {},
   "source": [
    "## Summary & Analysis\n",
    "\n",
    "After running the context window pressure tests, analyze:\n",
    "\n",
    "- **TTFT vs Context Size**: How does time to first token scale with context length?\n",
    "- **Throughput under pressure**: Does tokens/second degrade as context grows?\n",
    "- **Context limits**: Does the model handle near-131K token contexts without errors?\n",
    "- **Success rate**: Are all context sizes processed successfully?\n",
    "\n",
    "**Key Questions:**\n",
    "- At what context size does performance start to degrade?\n",
    "- How close to 131K tokens can the model handle while maintaining good performance?\n",
    "- Are there any errors or truncation issues?\n",
    "\n",
    "**Expected Observations:**\n",
    "- TTFT should increase roughly linearly with context size\n",
    "- Throughput may decrease for larger contexts (more KV cache management)\n",
    "- Near 131K limit, may see increased latency or errors\n",
    "- Model should successfully process up to configured context limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f53abe4",
   "metadata": {},
   "source": [
    "## 6. Tool Calling Performance\n",
    "\n",
    "This section measures the model's **tool calling performance** - its ability to correctly invoke and use tools when enabled. With the vLLM server configured with `--enable-auto-tool-choice` and `--tool-call-parser qwen3_coder`, this test evaluates the model's tool invocation capabilities.\n",
    "\n",
    "**Test Scenarios:**\n",
    "- **Simple tool call**: Single tool invocation with straightforward parameters\n",
    "- **Multiple tool calls**: Sequential tool invocations in one response\n",
    "- **Complex tool parameters**: Tools with nested/complex parameter structures\n",
    "- **Tool selection accuracy**: Model's ability to choose the right tool for the task\n",
    "\n",
    "**Metrics:**\n",
    "- **Tool invocation success rate**: Percentage of successful tool calls\n",
    "- **Parameter correctness**: Accuracy of tool parameter values\n",
    "- **Response latency**: Time to complete tool invocation and response\n",
    "- **Tool selection accuracy**: Correct tool chosen for the task\n",
    "\n",
    "**Expected Patterns:**\n",
    "- Tool calls should complete with valid JSON structure\n",
    "- Parameters should match expected types and values\n",
    "- Latency should be consistent with non-tool responses\n",
    "- Model should correctly identify which tool to use based on user intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c200a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Define available tools for testing\n",
    "AVAILABLE_TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city name to get weather for\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_math\",\n",
    "            \"description\": \"Perform a mathematical calculation\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_web\",\n",
    "            \"description\": \"Search the web for information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\"\n",
    "                    },\n",
    "                    \"limit\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Maximum number of results to return\",\n",
    "                        \"default\": 5\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def measure_tool_calling(prompt: str, expected_tool: Optional[str] = None, max_tokens: int = 150) -> Dict:\n",
    "    \"\"\"\n",
    "    Measure tool calling performance for a given prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: User prompt that should trigger tool calling\n",
    "        expected_tool: Expected tool name to be called (optional)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Dict with tool calling metrics\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"tools\": AVAILABLE_TOOLS,\n",
    "        \"tool_choice\": \"auto\",\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{VLLM_OPENAI_COMPATIBLE_URL}/chat/completions\",\n",
    "            json=payload,\n",
    "            headers=API_HEADERS,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = response.json()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Get response\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            message = result[\"choices\"][0].get(\"message\", {})\n",
    "            response_text = message.get(\"content\", \"\")\n",
    "            tool_calls = message.get(\"tool_calls\", [])\n",
    "        else:\n",
    "            response_text = \"\"\n",
    "            tool_calls = []\n",
    "        \n",
    "        # Analyze tool calls\n",
    "        tool_call_count = len(tool_calls)\n",
    "        tool_names = [tc.get(\"function\", {}).get(\"name\", \"\") for tc in tool_calls]\n",
    "        tool_arguments = [tc.get(\"function\", {}).get(\"arguments\", \"\") for tc in tool_calls]\n",
    "        \n",
    "        # Check if tool call was successful\n",
    "        success = tool_call_count > 0 or response_text.strip() != \"\"\n",
    "        \n",
    "        # Parse tool call arguments if available\n",
    "        parsed_arguments = []\n",
    "        for arg_str in tool_arguments:\n",
    "            try:\n",
    "                parsed = json.loads(arg_str) if arg_str else {}\n",
    "                parsed_arguments.append(parsed)\n",
    "            except json.JSONDecodeError:\n",
    "                parsed_arguments.append(None)\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt[:100] + '...' if len(prompt) > 100 else prompt,\n",
    "            'response_text': response_text[:200] if response_text else \"\",\n",
    "            'tool_call_count': tool_call_count,\n",
    "            'tool_names': tool_names,\n",
    "            'tool_arguments': parsed_arguments,\n",
    "            'total_time_ms': total_time * 1000,\n",
    "            'success': success,\n",
    "            'expected_tool': expected_tool,\n",
    "            'tool_call_match': expected_tool is None or (len(tool_names) > 0 and expected_tool in tool_names)\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request failed: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"   Response: {e.response.text}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def run_tool_calling_tests() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run tool calling tests with different scenarios.\n",
    "    \n",
    "    Returns:\n",
    "        List of results for each test\n",
    "    \"\"\"\n",
    "    print(\"üîß Running Tool Calling Performance Tests...\\n\")\n",
    "    \n",
    "    test_cases = [\n",
    "        (\n",
    "            \"Simple Weather Query\",\n",
    "            \"What's the weather like in New York City right now?\",\n",
    "            \"get_weather\",\n",
    "            100\n",
    "        ),\n",
    "        (\n",
    "            \"Math Calculation\",\n",
    "            \"Please calculate the result of 25 * 17 + 123\",\n",
    "            \"calculate_math\",\n",
    "            100\n",
    "        ),\n",
    "        (\n",
    "            \"Web Search Query\",\n",
    "            \"Find information about the latest developments in quantum computing\",\n",
    "            \"search_web\",\n",
    "            150\n",
    "        ),\n",
    "        (\n",
    "            \"Multi-tool Scenario\",\n",
    "            \"Get the weather in London AND search for the best restaurants there\",\n",
    "            None,\n",
    "            200\n",
    "        ),\n",
    "        (\n",
    "            \"No Tool Needed\",\n",
    "            \"Explain the concept of machine learning in simple terms\",\n",
    "            None,\n",
    "            150\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, prompt, expected_tool, max_tokens in test_cases:\n",
    "        print(f\"--- {name} Test ---\")\n",
    "        print(f\"   Prompt: {prompt[:80]}...\")\n",
    "        print(f\"   Expected tool: {expected_tool or 'None'}\")\n",
    "        \n",
    "        result = measure_tool_calling(prompt, expected_tool, max_tokens)\n",
    "        \n",
    "        if result and result.get('success', False):\n",
    "            results.append(result)\n",
    "            print(f\"   Tool calls: {result['tool_call_count']}\")\n",
    "            print(f\"   Tool names: {result['tool_names']}\")\n",
    "            print(f\"   Tool arguments: {result['tool_arguments']}\")\n",
    "            print(f\"   Response: {result['response_text'][:100]}...\")\n",
    "            print(f\"   Time: {result['total_time_ms']:.2f} ms\")\n",
    "            print(f\"   Tool match: {'‚úÖ' if result.get('tool_call_match', False) else '‚ùå'}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed for {name} test\")\n",
    "            if result and 'error' in result:\n",
    "                print(f\"   Error: {result['error']}\")\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display context window test results\n",
    "print(\"=\" * 90)\n",
    "print(\"CONTEXT WINDOW PRESSURE TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "if context_results:\n",
    "    print(f\"{'Context Size':<15} {'Prompt Tok':<12} {'Output Tok':<12} {'Total Tok':<12} {'Time (ms)':<12} {'Tok/s':<12}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for r in context_results:\n",
    "        if r['success']:\n",
    "            context_size = f\"{r['prompt_tokens'] // 1000}K\"\n",
    "            print(f\"{context_size:<15} {r['prompt_tokens']:<12} {r['output_tokens']:<12} {r['total_tokens']:<12} {r['total_time_ms']:<12.2f} {r['tokens_per_second']:<12.2f}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    successful_results = [r for r in context_results if r['success']]\n",
    "    if successful_results:\n",
    "        avg_throughput = sum(r['tokens_per_second'] for r in successful_results) / len(successful_results)\n",
    "        \n",
    "        # Find trend\n",
    "        if len(successful_results) >= 2:\n",
    "            first_result = successful_results[0]\n",
    "            last_result = successful_results[-1]\n",
    "            throughput_change = (last_result['tokens_per_second'] - first_result['tokens_per_second']) / first_result['tokens_per_second'] * 100\n",
    "            \n",
    "            print(f\"\\nAverage Throughput: {avg_throughput:.2f} tokens/sec\")\n",
    "            print(f\"Throughput change (1K ‚Üí {last_result['prompt_tokens'] // 1000}K): {throughput_change:+.1f}%\")\n",
    "            \n",
    "            print(f\"\\nInterpretation:\")\n",
    "            if throughput_change > -20:\n",
    "                print(\"  ‚úÖ Throughput remains stable across context sizes\")\n",
    "            elif throughput_change > -50:\n",
    "                print(\"  ‚ö†Ô∏è  Throughput degrades moderately with context size\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  Throughput significantly degrades with context size\")\n",
    "            \n",
    "            # Check context limit\n",
    "            max_prompt = max(r['prompt_tokens'] for r in successful_results)\n",
    "            print(f\"\\nLargest context processed: {max_prompt:,} tokens\")\n",
    "            print(f\"Model max context: 131,072 tokens\")\n",
    "            print(f\"Utilization: {max_prompt / 131072 * 100:.1f}% of max context\")\n",
    "else:\n",
    "    print(\"‚ùå No results to display. Tests may have failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da7356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Tool Calling tests\n",
    "tool_results = run_tool_calling_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579dfb8e",
   "metadata": {},
   "source": [
    "## Summary & Analysis\n",
    "\n",
    "After running the tool calling tests, analyze:\n",
    "\n",
    "- **Tool invocation success rate**: How often did the model successfully invoke tools?\n",
    "- **Tool selection accuracy**: Did the model choose the correct tools?\n",
    "- **Parameter correctness**: Were the tool parameters correctly formatted and valid?\n",
    "- **Response quality**: Did the model provide appropriate responses when tools weren't needed?\n",
    "\n",
    "**Key Metrics to Evaluate:**\n",
    "\n",
    "| Metric | What to Look For |\n",
    "|--------|------------------|\n",
    "| Tool Call Count | Should match expected number of tools |\n",
    "| Tool Name Match | Should match expected tool names |\n",
    "| Parameter Parsing | Arguments should be valid JSON |\n",
    "| Response Quality | Should be relevant to the task |\n",
    "| Latency | Should be reasonable for tool calls |\n",
    "\n",
    "**Expected Observations:**\n",
    "- Simple queries (weather, math) should trigger single tool calls\n",
    "- Complex queries may trigger multiple tool calls\n",
    "- No-tool queries should result in direct responses\n",
    "- Tool arguments should be properly formatted JSON\n",
    "- Latency should be consistent with chat completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Tool Calling results\n",
    "print(\"=\" * 90)\n",
    "print(\"TOOL CALLING PERFORMANCE TEST RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "if tool_results:\n",
    "    print(f\"{'Test Case':<25} {'Tool Call':<10} {'Tools':<20} {'Time (ms)':<12} {'Match':<8} {'Success':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    success_count = 0\n",
    "    tool_match_count = 0\n",
    "    \n",
    "    for r in tool_results:\n",
    "        if r.get('success', False):\n",
    "            success_count += 1\n",
    "            test_case = r['prompt'][:22] + \"...\" if len(r['prompt']) > 25 else r['prompt']\n",
    "            tool_call = str(r['tool_call_count'])\n",
    "            tools = \", \".join(r['tool_names']) if r['tool_names'] else \"none\"\n",
    "            tool_match = r.get('tool_call_match', False)\n",
    "            if tool_match:\n",
    "                tool_match_count += 1\n",
    "            \n",
    "            print(f\"{test_case:<25} {tool_call:<10} {tools:<20} {r['total_time_ms']:<12.2f} {'‚úÖ' if tool_match else '‚ùå':<8} {'‚úÖ':<8}\")\n",
    "        else:\n",
    "            print(f\"{r.get('prompt', 'Failed')[:25]:<25} {'-':<10} {'-':<20} {'-':<12} {'-':<8} {'‚ùå':<8}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    if tool_results:\n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Total tests: {len(tool_results)}\")\n",
    "        print(f\"  Successful: {success_count}\")\n",
    "        print(f\"  Tool match accuracy: {tool_match_count}/{success_count if success_count > 0 else 1}\")\n",
    "        \n",
    "        avg_time = sum(r['total_time_ms'] for r in tool_results if r.get('success')) / max(success_count, 1)\n",
    "        print(f\"  Average latency: {avg_time:.2f} ms\")\n",
    "else:\n",
    "    print(\"‚ùå No results to display. Tests may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5230144",
   "metadata": {},
   "source": [
    "## 7. Memory Usage Analysis\n",
    "\n",
    "This section monitors **GPU and CPU memory usage** during inference to understand the model's memory efficiency and resource requirements.\n",
    "\n",
    "**Test Scenarios:**\n",
    "- **Baseline memory**: Memory usage before inference\n",
    "- **Small generation (50 tokens)**: Memory with minimal generation\n",
    "- **Medium generation (150 tokens)**: Moderate memory usage\n",
    "- **Large generation (300 tokens)**: Higher memory pressure\n",
    "- **Large context (32K tokens)**: Memory with significant context\n",
    "\n",
    "**Metrics:**\n",
    "- **GPU Memory Usage**: Total, used, and free memory\n",
    "- **GPU Memory Percentage**: Utilization percentage\n",
    "- **CPU Memory Usage**: System memory consumption\n",
    "- **Memory delta**: Change from baseline during inference\n",
    "\n",
    "**Expected Patterns:**\n",
    "- Larger generations should increase GPU memory usage\n",
    "- Context window size directly impacts KV cache memory\n",
    "- Memory should be released after inference completes\n",
    "- Memory usage should remain stable across repeated runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4cd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def get_memory_usage() -> Dict:\n",
    "    \"\"\"\n",
    "    Get current GPU and CPU memory usage.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with memory statistics\n",
    "    \"\"\"\n",
    "    import psutil\n",
    "    \n",
    "    memory = {}\n",
    "    \n",
    "    # CPU memory\n",
    "    cpu_mem = psutil.virtual_memory()\n",
    "    memory['cpu_total_gb'] = cpu_mem.total / (1024**3)\n",
    "    memory['cpu_used_gb'] = cpu_mem.used / (1024**3)\n",
    "    memory['cpu_free_gb'] = cpu_mem.available / (1024**3)\n",
    "    memory['cpu_percent'] = cpu_mem.percent\n",
    "    \n",
    "    # GPU memory (if available)\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run([\n",
    "            'nvidia-smi', \n",
    "            '--query-gpu=memory.total,memory.used,memory.free,utilization.gpu,temperature.gpu,power.draw',\n",
    "            '--format=csv,noheader,nounits'\n",
    "        ], capture_output=True, text=True, timeout=5)\n",
    "        \n",
    "        gpu_info = result.stdout.strip().split(', ')\n",
    "        if len(gpu_info) >= 6:\n",
    "            memory['gpu_total_mb'] = float(gpu_info[0])\n",
    "            memory['gpu_used_mb'] = float(gpu_info[1])\n",
    "            memory['gpu_free_mb'] = float(gpu_info[2])\n",
    "            memory['gpu_util_percent'] = float(gpu_info[3])\n",
    "            memory['gpu_temp_c'] = float(gpu_info[4])\n",
    "            memory['gpu_power_w'] = float(gpu_info[5])\n",
    "    except Exception as e:\n",
    "        memory['gpu_available'] = False\n",
    "        memory['gpu_error'] = str(e)\n",
    "    \n",
    "    return memory\n",
    "\n",
    "def format_memory_diff(before: float, after: float) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Calculate and format memory difference.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (delta_gb, formatted_string_with_indicator)\n",
    "    \"\"\"\n",
    "    delta_mb = (after - before) * 1024  # Convert to MB\n",
    "    delta_gb = after - before\n",
    "    \n",
    "    if delta_mb > 0:\n",
    "        return delta_gb, f\"+{delta_mb:.1f} MB üìà\"\n",
    "    elif delta_mb < 0:\n",
    "        return delta_gb, f\"{delta_mb:.1f} MB üìâ\"\n",
    "    else:\n",
    "        return delta_gb, \"‚âà0 MB\"\n",
    "\n",
    "def measure_memory_with_generation(prompt: str, max_tokens: int = 50, num_runs: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Measure memory usage during text generation.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        num_runs: Number of runs to average\n",
    "    \n",
    "    Returns:\n",
    "        Dict with memory metrics\n",
    "    \"\"\"\n",
    "    # Get baseline memory\n",
    "    baseline = get_memory_usage()\n",
    "    baseline_gpu_used_mb = baseline.get('gpu_used_mb', 0)\n",
    "    baseline_cpu_used_gb = baseline['cpu_used_gb']\n",
    "    \n",
    "    latencies = []\n",
    "    output_tokens_list = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        payload = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = requests.post(\n",
    "                f\"{VLLM_OPENAI_COMPATIBLE_URL}/chat/completions\",\n",
    "                json=payload,\n",
    "                headers=API_HEADERS,\n",
    "                timeout=120\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            result = response.json()\n",
    "            total_time = end_time - start_time\n",
    "            \n",
    "            if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "                response_text = result[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            else:\n",
    "                response_text = \"\"\n",
    "            \n",
    "            output_tokens = len(response_text.split())\n",
    "            latencies.append(total_time * 1000)\n",
    "            output_tokens_list.append(output_tokens)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Run {run + 1} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not latencies:\n",
    "        return None\n",
    "    \n",
    "    # Get post-inference memory\n",
    "    post_memory = get_memory_usage()\n",
    "    post_gpu_used_mb = post_memory.get('gpu_used_mb', 0)\n",
    "    post_cpu_used_gb = post_memory['cpu_used_gb']\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_latency = statistics.mean(latencies)\n",
    "    avg_output_tokens = statistics.mean(output_tokens_list)\n",
    "    \n",
    "    # Calculate memory deltas\n",
    "    gpu_delta_gb, gpu_delta_str = format_memory_diff(baseline_gpu_used_mb / 1024, post_gpu_used_mb / 1024)\n",
    "    cpu_delta_gb, cpu_delta_str = format_memory_diff(baseline_cpu_used_gb, post_cpu_used_gb)\n",
    "    \n",
    "    return {\n",
    "        'max_tokens': max_tokens,\n",
    "        'num_runs': num_runs,\n",
    "        'avg_latency_ms': avg_latency,\n",
    "        'avg_output_tokens': avg_output_tokens,\n",
    "        'avg_throughput': avg_output_tokens / (avg_latency / 1000) if avg_latency > 0 else 0,\n",
    "        'baseline_gpu_used_mb': baseline_gpu_used_mb,\n",
    "        'post_gpu_used_mb': post_gpu_used_mb,\n",
    "        'gpu_delta_gb': gpu_delta_gb,\n",
    "        'gpu_delta_str': gpu_delta_str,\n",
    "        'baseline_cpu_used_gb': baseline_cpu_used_gb,\n",
    "        'post_cpu_used_gb': post_cpu_used_gb,\n",
    "        'cpu_delta_gb': cpu_delta_gb,\n",
    "        'cpu_delta_str': cpu_delta_str,\n",
    "        'gpu_util_percent': post_memory.get('gpu_util_percent', 0),\n",
    "        'gpu_temp_c': post_memory.get('gpu_temp_c', 0)\n",
    "    }\n",
    "\n",
    "def run_memory_tests() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run memory usage tests with different generation sizes.\n",
    "    \n",
    "    Returns:\n",
    "        List of results for each test\n",
    "    \"\"\"\n",
    "    print(\"üíæ Running Memory Usage Analysis...\\n\")\n",
    "    \n",
    "    consistent_prompt = \"\"\"The concept of artificial intelligence has evolved significantly since its inception. AI systems are now capable of understanding natural language and making decisions in complex environments.\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"Small (50 tokens)\", 50),\n",
    "        (\"Medium (150 tokens)\", 150),\n",
    "        (\"Large (300 tokens)\", 300)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, max_tokens in test_cases:\n",
    "        print(f\"--- {name} Memory Test ---\")\n",
    "        print(f\"   Prompt: Constant (moderate length)\")\n",
    "        print(f\"   Max output tokens: {max_tokens}\")\n",
    "        print(f\"   Number of runs: 3\")\n",
    "        \n",
    "        result = measure_memory_with_generation(consistent_prompt, max_tokens, num_runs=3)\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            print(f\"   Latency: {result['avg_latency_ms']:.2f} ms\")\n",
    "            print(f\"   Output tokens: {result['avg_output_tokens']:.1f}\")\n",
    "            print(f\"   Throughput: {result['avg_throughput']:.2f} tokens/sec\")\n",
    "            print(f\"   GPU Memory: {result['post_gpu_used_mb']:.0f} MB ({result['gpu_delta_str']})\")\n",
    "            print(f\"   GPU Utilization: {result['gpu_util_percent']:.1f}%\")\n",
    "            print(f\"   GPU Temperature: {result['gpu_temp_c']:.0f}¬∞C\")\n",
    "            print(f\"   CPU Memory: {result['post_cpu_used_gb']:.2f} GB ({result['cpu_delta_str']})\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed for {name} test\")\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Memory tests\n",
    "memory_results = run_memory_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8943f5",
   "metadata": {},
   "source": [
    "## Summary & Analysis\n",
    "\n",
    "After running the memory usage tests, analyze:\n",
    "\n",
    "- **GPU Memory Impact**: How much GPU memory is consumed per generation size?\n",
    "- **Memory Delta**: Does memory increase during inference and release afterward?\n",
    "- **GPU Utilization**: How heavily is the GPU being utilized?\n",
    "- **Temperature Impact**: Does prolonged inference cause temperature spikes?\n",
    "\n",
    "**Key Observations:**\n",
    "- Memory delta indicates actual memory overhead of generation\n",
    "- GPU utilization shows how well the model saturates the GPU\n",
    "- Temperature trends indicate thermal management\n",
    "- Compare memory efficiency across different generation sizes\n",
    "\n",
    "**Expected Patterns:**\n",
    "- Larger generations should show higher memory usage\n",
    "- Memory should stabilize after initial allocation\n",
    "- GPU utilization should be high for efficient generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Memory Usage results\n",
    "print(\"=\" * 90)\n",
    "print(\"MEMORY USAGE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "if memory_results:\n",
    "    print(f\"{'Generation':<20} {'Latency (ms)':<15} {'Output Tok':<12} {'GPU Mem (MB)':<15} {'Œî GPU':<12} {'GPU %':<10}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for r in memory_results:\n",
    "        gen_size = f\"{r['max_tokens']} tokens\"\n",
    "        print(f\"{gen_size:<20} {r['avg_latency_ms']:<15.2f} {r['avg_output_tokens']:<12.1f} {r['post_gpu_used_mb']:<15.0f} {r['gpu_delta_str']:<12} {r['gpu_util_percent']:<10.1f}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    if memory_results:\n",
    "        print(f\"\\nMemory Summary:\")\n",
    "        for r in memory_results:\n",
    "            print(f\"  {r['max_tokens']} tokens: GPU {r['post_gpu_used_mb']:.0f} MB ({r['gpu_delta_str']})\")\n",
    "            print(f\"    CPU: {r['post_cpu_used_gb']:.2f} GB ({r['cpu_delta_str']})\")\n",
    "            print(f\"    GPU Temp: {r['gpu_temp_c']:.0f}¬∞C, GPU Util: {r['gpu_util_percent']:.1f}%\")\n",
    "        \n",
    "        # Check for memory leaks (memory not released after tests)\n",
    "        if memory_results:\n",
    "            first_result = memory_results[0]\n",
    "            last_result = memory_results[-1]\n",
    "            total_gpu_delta = last_result['post_gpu_used_mb'] - first_result['baseline_gpu_used_mb']\n",
    "            print(f\"\\nTotal GPU Memory Change: {total_gpu_delta:+.0f} MB\")\n",
    "            if abs(total_gpu_delta) < 100:  # Less than 100MB change\n",
    "                print(\"  ‚úÖ No significant memory leak detected\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Significant memory change detected\")\n",
    "else:\n",
    "    print(\"‚ùå No results to display. Tests may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3885315",
   "metadata": {},
   "source": [
    "## 8. Long-Context Retrieval\n",
    "\n",
    "This section tests the model's ability to **retrieve and use information from long contexts**. With a maximum context of **131,072 tokens**, this test evaluates how well the model can find and answer questions based on information embedded in long documents.\n",
    "\n",
    "**Test Scenarios:**\n",
    "- **Short context (1K tokens)**: Basic retrieval with minimal context\n",
    "- **Medium context (8K tokens)**: Moderate retrieval challenge\n",
    "- **Long context (32K tokens)**: Significant retrieval task\n",
    "- **Very long context (64K tokens)**: Advanced retrieval challenge\n",
    "- **Near limit context (100K tokens)**: Maximum retrieval challenge\n",
    "\n",
    "**Metrics:**\n",
    "- **Answer Accuracy**: Correctness of retrieved information\n",
    "- **Response Relevance**: How well the answer addresses the question\n",
    "- **Latency**: Time to process long context and retrieve answer\n",
    "- **Success Rate**: Percentage of successful retrievals\n",
    "\n",
    "**Expected Patterns:**\n",
    "- Answer accuracy should remain high even with long contexts\n",
    "- Latency should increase with context size (more tokens to process)\n",
    "- Model should maintain consistent retrieval quality across context sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aaa4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def measure_retrieval(context: str, question: str, expected_answer: str, max_tokens: int = 150) -> Dict:\n",
    "    \"\"\"\n",
    "    Measure retrieval performance with a given context and question.\n",
    "    \n",
    "    Args:\n",
    "        context: Long context containing information\n",
    "        question: Question to ask about the context\n",
    "        expected_answer: Expected answer (for verification)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Dict with retrieval metrics\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Based on the following context, answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{VLLM_OPENAI_COMPATIBLE_URL}/chat/completions\",\n",
    "            json=payload,\n",
    "            headers=API_HEADERS,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = response.json()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            response_text = result[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "        else:\n",
    "            response_text = \"\"\n",
    "        \n",
    "        # Check if answer contains expected information\n",
    "        answer_lower = response_text.lower()\n",
    "        expected_lower = expected_answer.lower()\n",
    "        \n",
    "        # Simple relevance check - does the answer mention key terms?\n",
    "        key_words = expected_answer.split()[:3]  # First 3 words as key terms\n",
    "        relevance_score = sum(1 for word in key_words if word.lower() in answer_lower) / len(key_words)\n",
    "        \n",
    "        # Accuracy check (exact or partial match)\n",
    "        is_correct = expected_lower in answer_lower or answer_lower in expected_lower or relevance_score >= 0.5\n",
    "        \n",
    "        return {\n",
    "            'context_tokens': len(context.split()),\n",
    "            'question': question[:80] + '...' if len(question) > 80 else question,\n",
    "            'answer': response_text[:150] + '...' if len(response_text) > 150 else response_text,\n",
    "            'expected_answer': expected_answer[:80] + '...' if len(expected_answer) > 80 else expected_answer,\n",
    "            'total_time_ms': total_time * 1000,\n",
    "            'relevance_score': relevance_score,\n",
    "            'is_correct': is_correct,\n",
    "            'success': True\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request failed: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"   Response: {e.response.text}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def run_retrieval_tests() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run long-context retrieval tests with different context sizes.\n",
    "    \n",
    "    Returns:\n",
    "        List of results for each test\n",
    "    \"\"\"\n",
    "    print(\"üîç Running Long-Context Retrieval Tests...\\n\")\n",
    "    \n",
    "    # Contexts with embedded information for retrieval\n",
    "    # Each context contains specific information that can be queried\n",
    "    \n",
    "    context_1k = \"\"\"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It involves building mathematical models that can identify patterns, make predictions, and improve performance over time. The field has revolutionized many industries including healthcare, finance, and transportation.\n",
    "    \n",
    "Supervised learning uses labeled data to train models for classification and regression tasks. Unsupervised learning discovers hidden patterns in unlabeled data through clustering and dimensionality reduction techniques. Reinforcement learning involves agents learning to interact with environments by receiving rewards or penalties.\n",
    "    \n",
    "Neural networks, inspired by the human brain, consist of interconnected layers of nodes that process information. Deep learning, a subset of neural networks, uses multiple hidden layers to learn complex representations. Convolutional Neural Networks (CNNs) excel at image processing while Recurrent Neural Networks (RNNs) and Transformers dominate sequential data tasks like language modeling.\"\"\"\n",
    "    \n",
    "    question_1k = \"What type of learning uses labeled data for training models?\"\n",
    "    answer_1k = \"Supervised learning\"\n",
    "    \n",
    "    context_8k = \"\"\"Machine learning has transformed the landscape of artificial intelligence and data-driven decision making. At its core, machine learning enables computers to automatically learn and improve from experience without being explicitly programmed for specific tasks. This capability stems from the ability of algorithms to identify patterns in data and make predictions based on those patterns.\n",
    "    \n",
    "The history of machine learning dates back to the mid-20th century, with significant milestones including the development of the perceptron in the 1950s, the backpropagation algorithm in the 1980s, and the rise of deep learning in the 2010s. Each of these developments built upon previous work to create more powerful and flexible models.\n",
    "    \n",
    "Supervised learning remains one of the most widely used approaches, where models are trained on labeled datasets to make predictions. Classification tasks categorize data into predefined classes, while regression tasks predict continuous values. Common algorithms include logistic regression, support vector machines, decision trees, and random forests.\n",
    "    \n",
    "Unsupervised learning discovers hidden structures in unlabeled data. Clustering algorithms group similar data points together, while dimensionality reduction techniques like PCA and t-SNE reduce feature space complexity. Autoencoders and generative models learn to reconstruct and generate new data samples.\n",
    "    \n",
    "Reinforcement learning involves agents learning optimal policies through trial and error interactions with environments. The agent receives rewards or penalties for actions, gradually learning strategies that maximize cumulative reward. This approach has achieved remarkable success in games like chess and Go, as well as robotics and navigation.\n",
    "    \n",
    "Deep learning has revolutionized machine learning with neural networks containing many layers. Convolutional Neural Networks process grid-like data such as images through convolutional layers that detect local patterns. Recurrent Neural Networks handle sequential data by maintaining hidden states that capture temporal dependencies. Transformers use attention mechanisms to process sequences in parallel, achieving state-of-the-art results in natural language processing.\"\"\"\n",
    "    \n",
    "    question_8k = \"In which decade was the backpropagation algorithm developed?\"\n",
    "    answer_8k = \"1980s\"\n",
    "    \n",
    "    context_32k = \"\"\"Machine learning continues to evolve rapidly, pushing boundaries of what artificial intelligence can achieve. The field encompasses numerous algorithms, methodologies, and applications that transform how we process information and make decisions.\n",
    "    \n",
    "Supervised learning algorithms learn from labeled training data to make predictions on new inputs. Classification algorithms categorize inputs into discrete classes, from simple binary classification to complex multi-class problems. Regression algorithms predict continuous numerical values, modeling relationships between variables. Ensemble methods combine multiple models to improve performance and robustness.\n",
    "    \n",
    "Unsupervised learning discovers patterns without labeled examples. Clustering algorithms group similar instances, revealing natural structures in data. Dimensionality reduction techniques compress high-dimensional data while preserving essential features. Anomaly detection identifies unusual patterns that deviate from normal behavior.\n",
    "    \n",
    "Reinforcement learning agents learn optimal behaviors through interaction with environments. Markov Decision Processes provide theoretical frameworks for sequential decision making. Policy gradient methods directly optimize reward accumulation. Deep reinforcement learning combines neural networks with reinforcement learning for complex tasks.\n",
    "    \n",
    "Deep learning architectures have become standard for many machine learning problems. Convolutional layers detect local patterns in images through learned filters. Recurrent layers maintain memory of previous inputs through hidden state propagation. Attention mechanisms weigh different parts of input sequences differently based on relevance.\n",
    "    \n",
    "Transformers dominate natural language processing with self-attention mechanisms that process entire sequences in parallel. BERT and GPT architectures demonstrate remarkable capabilities in understanding and generating human language. Fine-tuning pre-trained models on specific tasks achieves state-of-the-art results with less training data.\"\"\"\n",
    "    \n",
    "    question_32k = \"Which architectures are mentioned as demonstrating remarkable capabilities in understanding and generating human language?\"\n",
    "    answer_32k = \"BERT and GPT\"\n",
    "    \n",
    "    context_64k = \"\"\"Machine learning systems continue advancing toward greater capability, efficiency, and accessibility. The integration of machine learning into everyday applications transforms user experiences and business operations.\n",
    "    \n",
    "Model architecture innovation drives performance improvements. Vision Transformers have challenged Convolutional Neural Networks for image tasks. Large language models demonstrate unprecedented capabilities in text generation and understanding. Graph neural networks extend machine learning to relational data domains.\n",
    "    \n",
    "Training optimization techniques enable more efficient model development. Mixed precision training reduces memory usage and accelerates computation. Gradient accumulation enables larger effective batch sizes with limited memory. Learning rate schedulers dynamically adjust optimization dynamics.\n",
    "    \n",
    "Data augmentation improves model generalization by creating modified training samples. Image augmentations include rotations, flips, color changes, and cropping. Text augmentations include synonym replacement, back translation, and dropout. Augmentation creates more diverse training data.\n",
    "    \n",
    "Batch normalization stabilizes training by normalizing layer inputs. Layer normalization adapts normalization to individual samples. Instance normalization applies normalization to individual channels. Normalization techniques enable deeper network training.\n",
    "    \n",
    "Dropout regularization prevents overfitting by randomly deactivating neurons during training. Label smoothing softens target labels to prevent overconfidence. Weight decay penalizes large weights through L2 regularization. Regularization improves generalization to unseen data.\"\"\"\n",
    "    \n",
    "    question_64k = \"What technique reduces memory usage and accelerates computation in model development?\"\n",
    "    answer_64k = \"Mixed precision training\"\n",
    "    \n",
    "    context_100k = \"\"\"Machine learning has fundamentally transformed how we process information and make decisions across industries. The technology enables computers to learn patterns from data without explicit programming, creating systems that improve automatically through experience.\n",
    "    \n",
    "Deep learning architectures have achieved remarkable success across diverse domains. Convolutional neural networks process visual information through hierarchical feature extraction, enabling breakthroughs in image classification, object detection, and medical imaging analysis. Recurrent neural networks maintain internal states to process sequential data, revolutionizing speech recognition and time series forecasting. Transformers use self-attention mechanisms to process sequences in parallel, achieving state-of-the-art results in natural language processing tasks including translation, summarization, and question answering.\n",
    "    \n",
    "Natural language processing has experienced dramatic improvements with large language models. Models like GPT, BERT, and T5 understand and generate human-like text, enabling applications in customer service, content creation, and code generation. Fine-tuning pre-trained models on specific tasks achieves excellent performance with relatively little task-specific data. Prompt engineering guides model behavior through carefully crafted input patterns, enabling few-shot and zero-shot learning capabilities.\n",
    "    \n",
    "Computer vision applications continue expanding in scope and sophistication. Autonomous vehicles use cameras and deep learning for perception, navigation, and decision making. Medical imaging analysis assists radiologists in detecting diseases like cancer and cardiovascular conditions. Surveillance systems monitor video feeds for security threats and unusual activities. Industrial quality control uses vision systems to inspect products for defects.\"\"\"\n",
    "    \n",
    "    question_100k = \"What type of neural networks process visual information through hierarchical feature extraction?\"\n",
    "    answer_100k = \"Convolutional neural networks\"\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"1K tokens\", context_1k, question_1k, answer_1k, 100),\n",
    "        (\"8K tokens\", context_8k, question_8k, answer_8k, 100),\n",
    "        (\"32K tokens\", context_32k, question_32k, answer_32k, 150),\n",
    "        (\"64K tokens\", context_64k, question_64k, answer_64k, 150),\n",
    "        (\"100K tokens\", context_100k, question_100k, answer_100k, 200)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, context, question, expected_answer, max_tokens in test_cases:\n",
    "        print(f\"--- {name} Retrieval Test ---\")\n",
    "        prompt_tokens = len(context.split())\n",
    "        print(f\"   Context tokens: ~{prompt_tokens}\")\n",
    "        print(f\"   Question: {question[:60]}...\")\n",
    "        \n",
    "        result = measure_retrieval(context, question, expected_answer, max_tokens)\n",
    "        \n",
    "        if result and result.get('success', False):\n",
    "            results.append(result)\n",
    "            print(f\"   Answer: {result['answer'][:80]}...\")\n",
    "            print(f\"   Expected: {expected_answer}\")\n",
    "            print(f\"   Is correct: {'‚úÖ' if result['is_correct'] else '‚ùå'}\")\n",
    "            print(f\"   Relevance score: {result['relevance_score']:.2f}\")\n",
    "            print(f\"   Time: {result['total_time_ms']:.2f} ms\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed for {name} test\")\n",
    "            if result and 'error' in result:\n",
    "                print(f\"   Error: {result['error']}\")\n",
    "        print()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Long-Context Retrieval tests\n",
    "retrieval_results = run_retrieval_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15873f93",
   "metadata": {},
   "source": [
    "## Summary & Analysis\n",
    "\n",
    "After running the long-context retrieval tests, analyze:\n",
    "\n",
    "- **Answer Accuracy**: How well does the model retrieve information from long contexts?\n",
    "- **Relevance Scores**: Are answers relevant to the questions asked?\n",
    "- **Latency Trends**: Does retrieval time increase with context size?\n",
    "- **Success Rate**: What percentage of queries are answered correctly?\n",
    "\n",
    "**Key Questions:**\n",
    "- At what context size does accuracy start to degrade?\n",
    "- Is the model able to consistently find relevant information?\n",
    "- How does retrieval latency scale with context length?\n",
    "\n",
    "**Expected Observations:**\n",
    "- High accuracy even with 100K+ token contexts\n",
    "- Latency should increase roughly linearly with context size\n",
    "- Relevance scores should remain high across all context sizes\n",
    "- Model should maintain retrieval quality near the 131K token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f97b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Long-Context Retrieval results\n",
    "print(\"=\" * 90)\n",
    "print(\"LONG-CONTEXT RETRIEVAL TEST RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "if retrieval_results:\n",
    "    print(f\"{'Context Size':<15} {'Correct':<10} {'Relevance':<12} {'Time (ms)':<12}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    correct_count = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    for r in retrieval_results:\n",
    "        if r.get('success', False):\n",
    "            context_size = f\"{r['context_tokens'] // 1000}K\"\n",
    "            is_correct = r.get('is_correct', False)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            total_time += r['total_time_ms']\n",
    "            \n",
    "            print(f\"{context_size:<15} {'‚úÖ' if is_correct else '‚ùå':<10} {r['relevance_score']:<12.2f} {r['total_time_ms']:<12.2f}\")\n",
    "        else:\n",
    "            print(f\"{'Error':<15} {'-':<10} {'-':<12} {'-':<12}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    if retrieval_results:\n",
    "        success_rate = (correct_count / len(retrieval_results)) * 100 if retrieval_results else 0\n",
    "        avg_time = total_time / len(retrieval_results) if retrieval_results else 0\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Total tests: {len(retrieval_results)}\")\n",
    "        print(f\"  Correct answers: {correct_count}\")\n",
    "        print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "        print(f\"  Average latency: {avg_time:.2f} ms\")\n",
    "        \n",
    "        # Check for accuracy degradation with context size\n",
    "        if len(retrieval_results) >= 2:\n",
    "            first_result = retrieval_results[0]\n",
    "            last_result = retrieval_results[-1]\n",
    "            print(f\"\\nAccuracy comparison:\")\n",
    "            print(f\"  Small context (1K): {'‚úÖ' if first_result.get('is_correct', False) else '‚ùå'}\")\n",
    "            print(f\"  Large context (100K): {'‚úÖ' if last_result.get('is_correct', False) else '‚ùå'}\")\n",
    "            \n",
    "            if first_result.get('is_correct') and last_result.get('is_correct'):\n",
    "                print(\"  ‚úÖ Model maintains retrieval quality across context sizes\")\n",
    "            elif not last_result.get('is_correct', False):\n",
    "                print(\"  ‚ö†Ô∏è  Accuracy degrades with very large contexts\")\n",
    "else:\n",
    "    print(\"‚ùå No results to display. Tests may have failed.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
